{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIh/U42sGLrqYuBdADOy0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_mnist512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox_FE_xeDz6F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svakog osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1, za cifar 3\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQCkD03W_sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a78683-3bb0-423f-fcee-20833d705bc1"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "best_acc = 0  \n",
        "start_epoch = 0  \n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    #transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),# ovo je za mnist\n",
        "])\n",
        "\n",
        "\n",
        "#trainset = torchvision.datasets.CIFAR10(\n",
        " #   root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=512, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        " #         'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4',\n",
        "           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "net = net.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "start_time = time.time()\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "     \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "      train(epoch)\n",
        "      scheduler.step()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "Time elapsed: 0.00 min\n",
            "0 118 Loss: 2.353 | Acc: 11.133% (57/512)\n",
            "1 118 Loss: 2.314 | Acc: 12.793% (131/1024)\n",
            "2 118 Loss: 2.284 | Acc: 15.820% (243/1536)\n",
            "3 118 Loss: 2.253 | Acc: 17.432% (357/2048)\n",
            "4 118 Loss: 2.222 | Acc: 18.555% (475/2560)\n",
            "5 118 Loss: 2.206 | Acc: 19.108% (587/3072)\n",
            "6 118 Loss: 2.168 | Acc: 20.424% (732/3584)\n",
            "7 118 Loss: 2.139 | Acc: 21.606% (885/4096)\n",
            "8 118 Loss: 2.105 | Acc: 22.982% (1059/4608)\n",
            "9 118 Loss: 2.076 | Acc: 23.906% (1224/5120)\n",
            "10 118 Loss: 2.049 | Acc: 25.444% (1433/5632)\n",
            "11 118 Loss: 2.027 | Acc: 26.497% (1628/6144)\n",
            "12 118 Loss: 2.001 | Acc: 28.050% (1867/6656)\n",
            "13 118 Loss: 1.973 | Acc: 29.771% (2134/7168)\n",
            "14 118 Loss: 1.943 | Acc: 31.289% (2403/7680)\n",
            "15 118 Loss: 1.913 | Acc: 32.849% (2691/8192)\n",
            "16 118 Loss: 1.881 | Acc: 34.639% (3015/8704)\n",
            "17 118 Loss: 1.847 | Acc: 36.382% (3353/9216)\n",
            "18 118 Loss: 1.811 | Acc: 38.106% (3707/9728)\n",
            "19 118 Loss: 1.775 | Acc: 39.482% (4043/10240)\n",
            "20 118 Loss: 1.737 | Acc: 40.951% (4403/10752)\n",
            "21 118 Loss: 1.703 | Acc: 42.205% (4754/11264)\n",
            "22 118 Loss: 1.664 | Acc: 43.775% (5155/11776)\n",
            "23 118 Loss: 1.627 | Acc: 45.158% (5549/12288)\n",
            "24 118 Loss: 1.590 | Acc: 46.547% (5958/12800)\n",
            "25 118 Loss: 1.556 | Acc: 47.724% (6353/13312)\n",
            "26 118 Loss: 1.523 | Acc: 48.980% (6771/13824)\n",
            "27 118 Loss: 1.491 | Acc: 50.119% (7185/14336)\n",
            "28 118 Loss: 1.461 | Acc: 51.098% (7587/14848)\n",
            "29 118 Loss: 1.432 | Acc: 52.096% (8002/15360)\n",
            "30 118 Loss: 1.401 | Acc: 53.125% (8432/15872)\n",
            "31 118 Loss: 1.374 | Acc: 53.979% (8844/16384)\n",
            "32 118 Loss: 1.347 | Acc: 54.901% (9276/16896)\n",
            "33 118 Loss: 1.320 | Acc: 55.865% (9725/17408)\n",
            "34 118 Loss: 1.294 | Acc: 56.730% (10166/17920)\n",
            "35 118 Loss: 1.270 | Acc: 57.541% (10606/18432)\n",
            "36 118 Loss: 1.247 | Acc: 58.361% (11056/18944)\n",
            "37 118 Loss: 1.224 | Acc: 59.123% (11503/19456)\n",
            "38 118 Loss: 1.201 | Acc: 59.926% (11966/19968)\n",
            "39 118 Loss: 1.180 | Acc: 60.708% (12433/20480)\n",
            "40 118 Loss: 1.161 | Acc: 61.342% (12877/20992)\n",
            "41 118 Loss: 1.142 | Acc: 62.044% (13342/21504)\n",
            "42 118 Loss: 1.124 | Acc: 62.673% (13798/22016)\n",
            "43 118 Loss: 1.105 | Acc: 63.299% (14260/22528)\n",
            "44 118 Loss: 1.088 | Acc: 63.889% (14720/23040)\n",
            "45 118 Loss: 1.071 | Acc: 64.470% (15184/23552)\n",
            "46 118 Loss: 1.054 | Acc: 65.039% (15651/24064)\n",
            "47 118 Loss: 1.040 | Acc: 65.540% (16107/24576)\n",
            "48 118 Loss: 1.025 | Acc: 66.032% (16566/25088)\n",
            "49 118 Loss: 1.011 | Acc: 66.512% (17027/25600)\n",
            "50 118 Loss: 0.996 | Acc: 67.027% (17502/26112)\n",
            "51 118 Loss: 0.983 | Acc: 67.488% (17968/26624)\n",
            "52 118 Loss: 0.968 | Acc: 67.991% (18450/27136)\n",
            "53 118 Loss: 0.956 | Acc: 68.403% (18912/27648)\n",
            "54 118 Loss: 0.944 | Acc: 68.810% (19377/28160)\n",
            "55 118 Loss: 0.933 | Acc: 69.203% (19842/28672)\n",
            "56 118 Loss: 0.921 | Acc: 69.617% (20317/29184)\n",
            "57 118 Loss: 0.909 | Acc: 70.016% (20792/29696)\n",
            "58 118 Loss: 0.899 | Acc: 70.362% (21255/30208)\n",
            "59 118 Loss: 0.889 | Acc: 70.687% (21715/30720)\n",
            "60 118 Loss: 0.878 | Acc: 71.049% (22190/31232)\n",
            "61 118 Loss: 0.867 | Acc: 71.409% (22668/31744)\n",
            "62 118 Loss: 0.858 | Acc: 71.735% (23139/32256)\n",
            "63 118 Loss: 0.847 | Acc: 72.098% (23625/32768)\n",
            "64 118 Loss: 0.837 | Acc: 72.419% (24101/33280)\n",
            "65 118 Loss: 0.828 | Acc: 72.736% (24579/33792)\n",
            "66 118 Loss: 0.819 | Acc: 73.021% (25049/34304)\n",
            "67 118 Loss: 0.811 | Acc: 73.331% (25531/34816)\n",
            "68 118 Loss: 0.802 | Acc: 73.641% (26016/35328)\n",
            "69 118 Loss: 0.794 | Acc: 73.912% (26490/35840)\n",
            "70 118 Loss: 0.785 | Acc: 74.205% (26975/36352)\n",
            "71 118 Loss: 0.777 | Acc: 74.468% (27452/36864)\n",
            "72 118 Loss: 0.769 | Acc: 74.735% (27933/37376)\n",
            "73 118 Loss: 0.762 | Acc: 75.008% (28419/37888)\n",
            "74 118 Loss: 0.753 | Acc: 75.284% (28909/38400)\n",
            "75 118 Loss: 0.746 | Acc: 75.545% (29396/38912)\n",
            "76 118 Loss: 0.739 | Acc: 75.771% (29872/39424)\n",
            "77 118 Loss: 0.732 | Acc: 76.009% (30355/39936)\n",
            "78 118 Loss: 0.725 | Acc: 76.216% (30828/40448)\n",
            "79 118 Loss: 0.718 | Acc: 76.458% (31317/40960)\n",
            "80 118 Loss: 0.711 | Acc: 76.673% (31798/41472)\n",
            "81 118 Loss: 0.705 | Acc: 76.882% (32278/41984)\n",
            "82 118 Loss: 0.699 | Acc: 77.085% (32758/42496)\n",
            "83 118 Loss: 0.693 | Acc: 77.283% (33238/43008)\n",
            "84 118 Loss: 0.686 | Acc: 77.500% (33728/43520)\n",
            "85 118 Loss: 0.680 | Acc: 77.712% (34218/44032)\n",
            "86 118 Loss: 0.674 | Acc: 77.923% (34710/44544)\n",
            "87 118 Loss: 0.668 | Acc: 78.129% (35202/45056)\n",
            "88 118 Loss: 0.662 | Acc: 78.325% (35691/45568)\n",
            "89 118 Loss: 0.656 | Acc: 78.518% (36181/46080)\n",
            "90 118 Loss: 0.651 | Acc: 78.707% (36671/46592)\n",
            "91 118 Loss: 0.646 | Acc: 78.872% (37152/47104)\n",
            "92 118 Loss: 0.641 | Acc: 79.028% (37630/47616)\n",
            "93 118 Loss: 0.635 | Acc: 79.203% (38119/48128)\n",
            "94 118 Loss: 0.631 | Acc: 79.365% (38603/48640)\n",
            "95 118 Loss: 0.626 | Acc: 79.517% (39084/49152)\n",
            "96 118 Loss: 0.621 | Acc: 79.683% (39574/49664)\n",
            "97 118 Loss: 0.616 | Acc: 79.857% (40069/50176)\n",
            "98 118 Loss: 0.611 | Acc: 80.017% (40559/50688)\n",
            "99 118 Loss: 0.607 | Acc: 80.170% (41047/51200)\n",
            "100 118 Loss: 0.602 | Acc: 80.328% (41539/51712)\n",
            "101 118 Loss: 0.597 | Acc: 80.473% (42026/52224)\n",
            "102 118 Loss: 0.593 | Acc: 80.624% (42518/52736)\n",
            "103 118 Loss: 0.589 | Acc: 80.754% (43000/53248)\n",
            "104 118 Loss: 0.584 | Acc: 80.891% (43487/53760)\n",
            "105 118 Loss: 0.580 | Acc: 81.025% (43974/54272)\n",
            "106 118 Loss: 0.576 | Acc: 81.166% (44466/54784)\n",
            "107 118 Loss: 0.572 | Acc: 81.288% (44949/55296)\n",
            "108 118 Loss: 0.568 | Acc: 81.422% (45440/55808)\n",
            "109 118 Loss: 0.564 | Acc: 81.552% (45930/56320)\n",
            "110 118 Loss: 0.561 | Acc: 81.671% (46415/56832)\n",
            "111 118 Loss: 0.557 | Acc: 81.780% (46896/57344)\n",
            "112 118 Loss: 0.554 | Acc: 81.902% (47385/57856)\n",
            "113 118 Loss: 0.550 | Acc: 82.035% (47882/58368)\n",
            "114 118 Loss: 0.546 | Acc: 82.164% (48378/58880)\n",
            "115 118 Loss: 0.543 | Acc: 82.277% (48866/59392)\n",
            "116 118 Loss: 0.539 | Acc: 82.405% (49364/59904)\n",
            "117 118 Loss: 0.536 | Acc: 82.427% (49456/60000)\n",
            "\n",
            "Epoch: 1\n",
            "Time elapsed: 1.77 min\n",
            "0 118 Loss: 0.095 | Acc: 96.875% (496/512)\n",
            "1 118 Loss: 0.118 | Acc: 95.996% (983/1024)\n",
            "2 118 Loss: 0.113 | Acc: 96.289% (1479/1536)\n",
            "3 118 Loss: 0.108 | Acc: 96.436% (1975/2048)\n",
            "4 118 Loss: 0.118 | Acc: 96.289% (2465/2560)\n",
            "5 118 Loss: 0.122 | Acc: 96.257% (2957/3072)\n",
            "6 118 Loss: 0.118 | Acc: 96.401% (3455/3584)\n",
            "7 118 Loss: 0.119 | Acc: 96.362% (3947/4096)\n",
            "8 118 Loss: 0.119 | Acc: 96.311% (4438/4608)\n",
            "9 118 Loss: 0.119 | Acc: 96.328% (4932/5120)\n",
            "10 118 Loss: 0.116 | Acc: 96.502% (5435/5632)\n",
            "11 118 Loss: 0.119 | Acc: 96.419% (5924/6144)\n",
            "12 118 Loss: 0.119 | Acc: 96.439% (6419/6656)\n",
            "13 118 Loss: 0.121 | Acc: 96.456% (6914/7168)\n",
            "14 118 Loss: 0.119 | Acc: 96.458% (7408/7680)\n",
            "15 118 Loss: 0.118 | Acc: 96.436% (7900/8192)\n",
            "16 118 Loss: 0.116 | Acc: 96.484% (8398/8704)\n",
            "17 118 Loss: 0.117 | Acc: 96.419% (8886/9216)\n",
            "18 118 Loss: 0.116 | Acc: 96.423% (9380/9728)\n",
            "19 118 Loss: 0.115 | Acc: 96.445% (9876/10240)\n",
            "20 118 Loss: 0.117 | Acc: 96.363% (10361/10752)\n",
            "21 118 Loss: 0.118 | Acc: 96.351% (10853/11264)\n",
            "22 118 Loss: 0.118 | Acc: 96.349% (11346/11776)\n",
            "23 118 Loss: 0.119 | Acc: 96.281% (11831/12288)\n",
            "24 118 Loss: 0.118 | Acc: 96.305% (12327/12800)\n",
            "25 118 Loss: 0.117 | Acc: 96.349% (12826/13312)\n",
            "26 118 Loss: 0.116 | Acc: 96.390% (13325/13824)\n",
            "27 118 Loss: 0.115 | Acc: 96.408% (13821/14336)\n",
            "28 118 Loss: 0.116 | Acc: 96.410% (14315/14848)\n",
            "29 118 Loss: 0.117 | Acc: 96.393% (14806/15360)\n",
            "30 118 Loss: 0.116 | Acc: 96.434% (15306/15872)\n",
            "31 118 Loss: 0.117 | Acc: 96.417% (15797/16384)\n",
            "32 118 Loss: 0.116 | Acc: 96.443% (16295/16896)\n",
            "33 118 Loss: 0.116 | Acc: 96.456% (16791/17408)\n",
            "34 118 Loss: 0.116 | Acc: 96.462% (17286/17920)\n",
            "35 118 Loss: 0.115 | Acc: 96.468% (17781/18432)\n",
            "36 118 Loss: 0.115 | Acc: 96.469% (18275/18944)\n",
            "37 118 Loss: 0.114 | Acc: 96.464% (18768/19456)\n",
            "38 118 Loss: 0.114 | Acc: 96.474% (19264/19968)\n",
            "39 118 Loss: 0.113 | Acc: 96.504% (19764/20480)\n",
            "40 118 Loss: 0.114 | Acc: 96.484% (20254/20992)\n",
            "41 118 Loss: 0.113 | Acc: 96.503% (20752/21504)\n",
            "42 118 Loss: 0.113 | Acc: 96.521% (21250/22016)\n",
            "43 118 Loss: 0.113 | Acc: 96.524% (21745/22528)\n",
            "44 118 Loss: 0.113 | Acc: 96.489% (22231/23040)\n",
            "45 118 Loss: 0.113 | Acc: 96.497% (22727/23552)\n",
            "46 118 Loss: 0.114 | Acc: 96.472% (23215/24064)\n",
            "47 118 Loss: 0.113 | Acc: 96.497% (23715/24576)\n",
            "48 118 Loss: 0.114 | Acc: 96.480% (24205/25088)\n",
            "49 118 Loss: 0.113 | Acc: 96.504% (24705/25600)\n",
            "50 118 Loss: 0.113 | Acc: 96.511% (25201/26112)\n",
            "51 118 Loss: 0.112 | Acc: 96.518% (25697/26624)\n",
            "52 118 Loss: 0.112 | Acc: 96.554% (26201/27136)\n",
            "53 118 Loss: 0.112 | Acc: 96.539% (26691/27648)\n",
            "54 118 Loss: 0.111 | Acc: 96.552% (27189/28160)\n",
            "55 118 Loss: 0.111 | Acc: 96.572% (27689/28672)\n",
            "56 118 Loss: 0.110 | Acc: 96.591% (28189/29184)\n",
            "57 118 Loss: 0.109 | Acc: 96.606% (28688/29696)\n",
            "58 118 Loss: 0.109 | Acc: 96.620% (29187/30208)\n",
            "59 118 Loss: 0.108 | Acc: 96.650% (29691/30720)\n",
            "60 118 Loss: 0.108 | Acc: 96.654% (30187/31232)\n",
            "61 118 Loss: 0.108 | Acc: 96.670% (30687/31744)\n",
            "62 118 Loss: 0.108 | Acc: 96.667% (31181/32256)\n",
            "63 118 Loss: 0.108 | Acc: 96.680% (31680/32768)\n",
            "64 118 Loss: 0.107 | Acc: 96.695% (32180/33280)\n",
            "65 118 Loss: 0.107 | Acc: 96.700% (32677/33792)\n",
            "66 118 Loss: 0.107 | Acc: 96.700% (33172/34304)\n",
            "67 118 Loss: 0.106 | Acc: 96.720% (33674/34816)\n",
            "68 118 Loss: 0.107 | Acc: 96.714% (34167/35328)\n",
            "69 118 Loss: 0.106 | Acc: 96.722% (34665/35840)\n",
            "70 118 Loss: 0.107 | Acc: 96.704% (35154/36352)\n",
            "71 118 Loss: 0.107 | Acc: 96.710% (35651/36864)\n",
            "72 118 Loss: 0.106 | Acc: 96.733% (36155/37376)\n",
            "73 118 Loss: 0.105 | Acc: 96.738% (36652/37888)\n",
            "74 118 Loss: 0.105 | Acc: 96.732% (37145/38400)\n",
            "75 118 Loss: 0.105 | Acc: 96.736% (37642/38912)\n",
            "76 118 Loss: 0.105 | Acc: 96.730% (38135/39424)\n",
            "77 118 Loss: 0.104 | Acc: 96.752% (38639/39936)\n",
            "78 118 Loss: 0.104 | Acc: 96.746% (39132/40448)\n",
            "79 118 Loss: 0.105 | Acc: 96.748% (39628/40960)\n",
            "80 118 Loss: 0.105 | Acc: 96.752% (40125/41472)\n",
            "81 118 Loss: 0.104 | Acc: 96.749% (40619/41984)\n",
            "82 118 Loss: 0.105 | Acc: 96.743% (41112/42496)\n",
            "83 118 Loss: 0.105 | Acc: 96.747% (41609/43008)\n",
            "84 118 Loss: 0.104 | Acc: 96.749% (42105/43520)\n",
            "85 118 Loss: 0.104 | Acc: 96.757% (42604/44032)\n",
            "86 118 Loss: 0.105 | Acc: 96.758% (43100/44544)\n",
            "87 118 Loss: 0.104 | Acc: 96.773% (43602/45056)\n",
            "88 118 Loss: 0.104 | Acc: 96.781% (44101/45568)\n",
            "89 118 Loss: 0.104 | Acc: 96.784% (44598/46080)\n",
            "90 118 Loss: 0.104 | Acc: 96.791% (45097/46592)\n",
            "91 118 Loss: 0.103 | Acc: 96.790% (45592/47104)\n",
            "92 118 Loss: 0.104 | Acc: 96.774% (46080/47616)\n",
            "93 118 Loss: 0.103 | Acc: 96.790% (46583/48128)\n",
            "94 118 Loss: 0.103 | Acc: 96.801% (47084/48640)\n",
            "95 118 Loss: 0.102 | Acc: 96.808% (47583/49152)\n",
            "96 118 Loss: 0.102 | Acc: 96.811% (48080/49664)\n",
            "97 118 Loss: 0.102 | Acc: 96.823% (48582/50176)\n",
            "98 118 Loss: 0.101 | Acc: 96.826% (49079/50688)\n",
            "99 118 Loss: 0.101 | Acc: 96.836% (49580/51200)\n",
            "100 118 Loss: 0.101 | Acc: 96.850% (50083/51712)\n",
            "101 118 Loss: 0.101 | Acc: 96.858% (50583/52224)\n",
            "102 118 Loss: 0.100 | Acc: 96.871% (51086/52736)\n",
            "103 118 Loss: 0.100 | Acc: 96.875% (51584/53248)\n",
            "104 118 Loss: 0.100 | Acc: 96.877% (52081/53760)\n",
            "105 118 Loss: 0.100 | Acc: 96.881% (52579/54272)\n",
            "106 118 Loss: 0.099 | Acc: 96.891% (53081/54784)\n",
            "107 118 Loss: 0.099 | Acc: 96.899% (53581/55296)\n",
            "108 118 Loss: 0.099 | Acc: 96.911% (54084/55808)\n",
            "109 118 Loss: 0.099 | Acc: 96.921% (54586/56320)\n",
            "110 118 Loss: 0.098 | Acc: 96.921% (55082/56832)\n",
            "111 118 Loss: 0.098 | Acc: 96.931% (55584/57344)\n",
            "112 118 Loss: 0.098 | Acc: 96.934% (56082/57856)\n",
            "113 118 Loss: 0.098 | Acc: 96.938% (56581/58368)\n",
            "114 118 Loss: 0.097 | Acc: 96.950% (57084/58880)\n",
            "115 118 Loss: 0.097 | Acc: 96.959% (57586/59392)\n",
            "116 118 Loss: 0.096 | Acc: 96.967% (58087/59904)\n",
            "117 118 Loss: 0.096 | Acc: 96.968% (58181/60000)\n",
            "\n",
            "Epoch: 2\n",
            "Time elapsed: 3.52 min\n",
            "0 118 Loss: 0.083 | Acc: 97.070% (497/512)\n",
            "1 118 Loss: 0.072 | Acc: 97.656% (1000/1024)\n",
            "2 118 Loss: 0.075 | Acc: 97.721% (1501/1536)\n",
            "3 118 Loss: 0.070 | Acc: 97.852% (2004/2048)\n",
            "4 118 Loss: 0.067 | Acc: 98.047% (2510/2560)\n",
            "5 118 Loss: 0.068 | Acc: 98.014% (3011/3072)\n",
            "6 118 Loss: 0.070 | Acc: 97.935% (3510/3584)\n",
            "7 118 Loss: 0.071 | Acc: 97.876% (4009/4096)\n",
            "8 118 Loss: 0.073 | Acc: 97.873% (4510/4608)\n",
            "9 118 Loss: 0.077 | Acc: 97.754% (5005/5120)\n",
            "10 118 Loss: 0.079 | Acc: 97.638% (5499/5632)\n",
            "11 118 Loss: 0.077 | Acc: 97.705% (6003/6144)\n",
            "12 118 Loss: 0.077 | Acc: 97.671% (6501/6656)\n",
            "13 118 Loss: 0.076 | Acc: 97.740% (7006/7168)\n",
            "14 118 Loss: 0.078 | Acc: 97.656% (7500/7680)\n",
            "15 118 Loss: 0.079 | Acc: 97.632% (7998/8192)\n",
            "16 118 Loss: 0.077 | Acc: 97.691% (8503/8704)\n",
            "17 118 Loss: 0.077 | Acc: 97.721% (9006/9216)\n",
            "18 118 Loss: 0.075 | Acc: 97.759% (9510/9728)\n",
            "19 118 Loss: 0.076 | Acc: 97.734% (10008/10240)\n",
            "20 118 Loss: 0.077 | Acc: 97.693% (10504/10752)\n",
            "21 118 Loss: 0.076 | Acc: 97.710% (11006/11264)\n",
            "22 118 Loss: 0.077 | Acc: 97.699% (11505/11776)\n",
            "23 118 Loss: 0.076 | Acc: 97.746% (12011/12288)\n",
            "24 118 Loss: 0.076 | Acc: 97.766% (12514/12800)\n",
            "25 118 Loss: 0.077 | Acc: 97.716% (13008/13312)\n",
            "26 118 Loss: 0.077 | Acc: 97.685% (13504/13824)\n",
            "27 118 Loss: 0.076 | Acc: 97.719% (14009/14336)\n",
            "28 118 Loss: 0.075 | Acc: 97.757% (14515/14848)\n",
            "29 118 Loss: 0.075 | Acc: 97.754% (15015/15360)\n",
            "30 118 Loss: 0.074 | Acc: 97.776% (15519/15872)\n",
            "31 118 Loss: 0.073 | Acc: 97.791% (16022/16384)\n",
            "32 118 Loss: 0.073 | Acc: 97.786% (16522/16896)\n",
            "33 118 Loss: 0.072 | Acc: 97.806% (17026/17408)\n",
            "34 118 Loss: 0.072 | Acc: 97.801% (17526/17920)\n",
            "35 118 Loss: 0.071 | Acc: 97.819% (18030/18432)\n",
            "36 118 Loss: 0.072 | Acc: 97.799% (18527/18944)\n",
            "37 118 Loss: 0.072 | Acc: 97.795% (19027/19456)\n",
            "38 118 Loss: 0.071 | Acc: 97.801% (19529/19968)\n",
            "39 118 Loss: 0.071 | Acc: 97.798% (20029/20480)\n",
            "40 118 Loss: 0.071 | Acc: 97.809% (20532/20992)\n",
            "41 118 Loss: 0.071 | Acc: 97.814% (21034/21504)\n",
            "42 118 Loss: 0.071 | Acc: 97.820% (21536/22016)\n",
            "43 118 Loss: 0.071 | Acc: 97.834% (22040/22528)\n",
            "44 118 Loss: 0.071 | Acc: 97.826% (22539/23040)\n",
            "45 118 Loss: 0.071 | Acc: 97.826% (23040/23552)\n",
            "46 118 Loss: 0.071 | Acc: 97.827% (23541/24064)\n",
            "47 118 Loss: 0.071 | Acc: 97.839% (24045/24576)\n",
            "48 118 Loss: 0.071 | Acc: 97.844% (24547/25088)\n",
            "49 118 Loss: 0.070 | Acc: 97.836% (25046/25600)\n",
            "50 118 Loss: 0.071 | Acc: 97.836% (25547/26112)\n",
            "51 118 Loss: 0.071 | Acc: 97.822% (26044/26624)\n",
            "52 118 Loss: 0.071 | Acc: 97.818% (26544/27136)\n",
            "53 118 Loss: 0.071 | Acc: 97.808% (27042/27648)\n",
            "54 118 Loss: 0.071 | Acc: 97.816% (27545/28160)\n",
            "55 118 Loss: 0.071 | Acc: 97.820% (28047/28672)\n",
            "56 118 Loss: 0.071 | Acc: 97.807% (28544/29184)\n",
            "57 118 Loss: 0.070 | Acc: 97.808% (29045/29696)\n",
            "58 118 Loss: 0.071 | Acc: 97.792% (29541/30208)\n",
            "59 118 Loss: 0.071 | Acc: 97.793% (30042/30720)\n",
            "60 118 Loss: 0.071 | Acc: 97.804% (30546/31232)\n",
            "61 118 Loss: 0.070 | Acc: 97.820% (31052/31744)\n",
            "62 118 Loss: 0.071 | Acc: 97.802% (31547/32256)\n",
            "63 118 Loss: 0.071 | Acc: 97.806% (32049/32768)\n",
            "64 118 Loss: 0.071 | Acc: 97.800% (32548/33280)\n",
            "65 118 Loss: 0.070 | Acc: 97.801% (33049/33792)\n",
            "66 118 Loss: 0.070 | Acc: 97.814% (33554/34304)\n",
            "67 118 Loss: 0.070 | Acc: 97.817% (34056/34816)\n",
            "68 118 Loss: 0.070 | Acc: 97.806% (34553/35328)\n",
            "69 118 Loss: 0.070 | Acc: 97.804% (35053/35840)\n",
            "70 118 Loss: 0.071 | Acc: 97.805% (35554/36352)\n",
            "71 118 Loss: 0.070 | Acc: 97.819% (36060/36864)\n",
            "72 118 Loss: 0.070 | Acc: 97.825% (36563/37376)\n",
            "73 118 Loss: 0.070 | Acc: 97.828% (37065/37888)\n",
            "74 118 Loss: 0.070 | Acc: 97.833% (37568/38400)\n",
            "75 118 Loss: 0.070 | Acc: 97.834% (38069/38912)\n",
            "76 118 Loss: 0.070 | Acc: 97.836% (38571/39424)\n",
            "77 118 Loss: 0.070 | Acc: 97.842% (39074/39936)\n",
            "78 118 Loss: 0.071 | Acc: 97.832% (39571/40448)\n",
            "79 118 Loss: 0.071 | Acc: 97.830% (40071/40960)\n",
            "80 118 Loss: 0.071 | Acc: 97.835% (40574/41472)\n",
            "81 118 Loss: 0.071 | Acc: 97.837% (41076/41984)\n",
            "82 118 Loss: 0.071 | Acc: 97.849% (41582/42496)\n",
            "83 118 Loss: 0.071 | Acc: 97.840% (42079/43008)\n",
            "84 118 Loss: 0.071 | Acc: 97.822% (42572/43520)\n",
            "85 118 Loss: 0.072 | Acc: 97.804% (43065/44032)\n",
            "86 118 Loss: 0.072 | Acc: 97.807% (43567/44544)\n",
            "87 118 Loss: 0.071 | Acc: 97.823% (44075/45056)\n",
            "88 118 Loss: 0.071 | Acc: 97.825% (44577/45568)\n",
            "89 118 Loss: 0.071 | Acc: 97.826% (45078/46080)\n",
            "90 118 Loss: 0.072 | Acc: 97.817% (45575/46592)\n",
            "91 118 Loss: 0.072 | Acc: 97.813% (46074/47104)\n",
            "92 118 Loss: 0.072 | Acc: 97.814% (46575/47616)\n",
            "93 118 Loss: 0.071 | Acc: 97.829% (47083/48128)\n",
            "94 118 Loss: 0.071 | Acc: 97.835% (47587/48640)\n",
            "95 118 Loss: 0.071 | Acc: 97.841% (48091/49152)\n",
            "96 118 Loss: 0.071 | Acc: 97.844% (48593/49664)\n",
            "97 118 Loss: 0.070 | Acc: 97.846% (49095/50176)\n",
            "98 118 Loss: 0.070 | Acc: 97.842% (49594/50688)\n",
            "99 118 Loss: 0.070 | Acc: 97.852% (50100/51200)\n",
            "100 118 Loss: 0.070 | Acc: 97.861% (50606/51712)\n",
            "101 118 Loss: 0.070 | Acc: 97.869% (51111/52224)\n",
            "102 118 Loss: 0.070 | Acc: 97.859% (51607/52736)\n",
            "103 118 Loss: 0.070 | Acc: 97.872% (52115/53248)\n",
            "104 118 Loss: 0.070 | Acc: 97.878% (52619/53760)\n",
            "105 118 Loss: 0.070 | Acc: 97.877% (53120/54272)\n",
            "106 118 Loss: 0.070 | Acc: 97.873% (53619/54784)\n",
            "107 118 Loss: 0.069 | Acc: 97.882% (54125/55296)\n",
            "108 118 Loss: 0.069 | Acc: 97.878% (54624/55808)\n",
            "109 118 Loss: 0.069 | Acc: 97.884% (55128/56320)\n",
            "110 118 Loss: 0.069 | Acc: 97.883% (55629/56832)\n",
            "111 118 Loss: 0.069 | Acc: 97.893% (56136/57344)\n",
            "112 118 Loss: 0.069 | Acc: 97.898% (56640/57856)\n",
            "113 118 Loss: 0.069 | Acc: 97.905% (57145/58368)\n",
            "114 118 Loss: 0.069 | Acc: 97.904% (57646/58880)\n",
            "115 118 Loss: 0.069 | Acc: 97.912% (58152/59392)\n",
            "116 118 Loss: 0.069 | Acc: 97.915% (58655/59904)\n",
            "117 118 Loss: 0.068 | Acc: 97.915% (58749/60000)\n",
            "\n",
            "Epoch: 3\n",
            "Time elapsed: 5.28 min\n",
            "0 118 Loss: 0.063 | Acc: 97.852% (501/512)\n",
            "1 118 Loss: 0.068 | Acc: 98.047% (1004/1024)\n",
            "2 118 Loss: 0.073 | Acc: 97.917% (1504/1536)\n",
            "3 118 Loss: 0.074 | Acc: 97.656% (2000/2048)\n",
            "4 118 Loss: 0.070 | Acc: 97.695% (2501/2560)\n",
            "5 118 Loss: 0.066 | Acc: 97.786% (3004/3072)\n",
            "6 118 Loss: 0.063 | Acc: 97.907% (3509/3584)\n",
            "7 118 Loss: 0.061 | Acc: 97.925% (4011/4096)\n",
            "8 118 Loss: 0.058 | Acc: 97.960% (4514/4608)\n",
            "9 118 Loss: 0.058 | Acc: 98.008% (5018/5120)\n",
            "10 118 Loss: 0.058 | Acc: 98.029% (5521/5632)\n",
            "11 118 Loss: 0.056 | Acc: 98.096% (6027/6144)\n",
            "12 118 Loss: 0.055 | Acc: 98.077% (6528/6656)\n",
            "13 118 Loss: 0.056 | Acc: 98.019% (7026/7168)\n",
            "14 118 Loss: 0.058 | Acc: 97.982% (7525/7680)\n",
            "15 118 Loss: 0.058 | Acc: 98.022% (8030/8192)\n",
            "16 118 Loss: 0.059 | Acc: 98.012% (8531/8704)\n",
            "17 118 Loss: 0.058 | Acc: 98.058% (9037/9216)\n",
            "18 118 Loss: 0.058 | Acc: 98.088% (9542/9728)\n",
            "19 118 Loss: 0.060 | Acc: 98.037% (10039/10240)\n",
            "20 118 Loss: 0.060 | Acc: 98.010% (10538/10752)\n",
            "21 118 Loss: 0.059 | Acc: 98.038% (11043/11264)\n",
            "22 118 Loss: 0.059 | Acc: 98.021% (11543/11776)\n",
            "23 118 Loss: 0.061 | Acc: 97.982% (12040/12288)\n",
            "24 118 Loss: 0.060 | Acc: 98.008% (12545/12800)\n",
            "25 118 Loss: 0.059 | Acc: 98.032% (13050/13312)\n",
            "26 118 Loss: 0.059 | Acc: 98.011% (13549/13824)\n",
            "27 118 Loss: 0.059 | Acc: 98.047% (14056/14336)\n",
            "28 118 Loss: 0.059 | Acc: 98.027% (14555/14848)\n",
            "29 118 Loss: 0.059 | Acc: 98.034% (15058/15360)\n",
            "30 118 Loss: 0.060 | Acc: 98.003% (15555/15872)\n",
            "31 118 Loss: 0.060 | Acc: 98.022% (16060/16384)\n",
            "32 118 Loss: 0.059 | Acc: 98.059% (16568/16896)\n",
            "33 118 Loss: 0.059 | Acc: 98.058% (17070/17408)\n",
            "34 118 Loss: 0.059 | Acc: 98.075% (17575/17920)\n",
            "35 118 Loss: 0.058 | Acc: 98.101% (18082/18432)\n",
            "36 118 Loss: 0.058 | Acc: 98.084% (18581/18944)\n",
            "37 118 Loss: 0.058 | Acc: 98.078% (19082/19456)\n",
            "38 118 Loss: 0.058 | Acc: 98.092% (19587/19968)\n",
            "39 118 Loss: 0.058 | Acc: 98.086% (20088/20480)\n",
            "40 118 Loss: 0.058 | Acc: 98.090% (20591/20992)\n",
            "41 118 Loss: 0.058 | Acc: 98.098% (21095/21504)\n",
            "42 118 Loss: 0.058 | Acc: 98.088% (21595/22016)\n",
            "43 118 Loss: 0.058 | Acc: 98.105% (22101/22528)\n",
            "44 118 Loss: 0.058 | Acc: 98.112% (22605/23040)\n",
            "45 118 Loss: 0.058 | Acc: 98.106% (23106/23552)\n",
            "46 118 Loss: 0.058 | Acc: 98.105% (23608/24064)\n",
            "47 118 Loss: 0.058 | Acc: 98.092% (24107/24576)\n",
            "48 118 Loss: 0.058 | Acc: 98.099% (24611/25088)\n",
            "49 118 Loss: 0.058 | Acc: 98.113% (25117/25600)\n",
            "50 118 Loss: 0.057 | Acc: 98.131% (25624/26112)\n",
            "51 118 Loss: 0.058 | Acc: 98.122% (26124/26624)\n",
            "52 118 Loss: 0.057 | Acc: 98.139% (26631/27136)\n",
            "53 118 Loss: 0.057 | Acc: 98.141% (27134/27648)\n",
            "54 118 Loss: 0.057 | Acc: 98.157% (27641/28160)\n",
            "55 118 Loss: 0.057 | Acc: 98.152% (28142/28672)\n",
            "56 118 Loss: 0.057 | Acc: 98.160% (28647/29184)\n",
            "57 118 Loss: 0.057 | Acc: 98.175% (29154/29696)\n",
            "58 118 Loss: 0.056 | Acc: 98.179% (29658/30208)\n",
            "59 118 Loss: 0.056 | Acc: 98.184% (30162/30720)\n",
            "60 118 Loss: 0.056 | Acc: 98.175% (30662/31232)\n",
            "61 118 Loss: 0.056 | Acc: 98.179% (31166/31744)\n",
            "62 118 Loss: 0.056 | Acc: 98.180% (31669/32256)\n",
            "63 118 Loss: 0.057 | Acc: 98.172% (32169/32768)\n",
            "64 118 Loss: 0.056 | Acc: 98.188% (32677/33280)\n",
            "65 118 Loss: 0.056 | Acc: 98.192% (33181/33792)\n",
            "66 118 Loss: 0.056 | Acc: 98.190% (33683/34304)\n",
            "67 118 Loss: 0.056 | Acc: 98.188% (34185/34816)\n",
            "68 118 Loss: 0.056 | Acc: 98.191% (34689/35328)\n",
            "69 118 Loss: 0.057 | Acc: 98.189% (35191/35840)\n",
            "70 118 Loss: 0.057 | Acc: 98.187% (35693/36352)\n",
            "71 118 Loss: 0.056 | Acc: 98.185% (36195/36864)\n",
            "72 118 Loss: 0.056 | Acc: 98.205% (36705/37376)\n",
            "73 118 Loss: 0.056 | Acc: 98.213% (37211/37888)\n",
            "74 118 Loss: 0.056 | Acc: 98.214% (37714/38400)\n",
            "75 118 Loss: 0.056 | Acc: 98.214% (38217/38912)\n",
            "76 118 Loss: 0.056 | Acc: 98.219% (38722/39424)\n",
            "77 118 Loss: 0.056 | Acc: 98.217% (39224/39936)\n",
            "78 118 Loss: 0.056 | Acc: 98.220% (39728/40448)\n",
            "79 118 Loss: 0.056 | Acc: 98.220% (40231/40960)\n",
            "80 118 Loss: 0.056 | Acc: 98.211% (40730/41472)\n",
            "81 118 Loss: 0.056 | Acc: 98.223% (41238/41984)\n",
            "82 118 Loss: 0.056 | Acc: 98.230% (41744/42496)\n",
            "83 118 Loss: 0.056 | Acc: 98.224% (42244/43008)\n",
            "84 118 Loss: 0.056 | Acc: 98.226% (42748/43520)\n",
            "85 118 Loss: 0.056 | Acc: 98.217% (43247/44032)\n",
            "86 118 Loss: 0.056 | Acc: 98.224% (43753/44544)\n",
            "87 118 Loss: 0.056 | Acc: 98.224% (44256/45056)\n",
            "88 118 Loss: 0.055 | Acc: 98.238% (44765/45568)\n",
            "89 118 Loss: 0.056 | Acc: 98.236% (45267/46080)\n",
            "90 118 Loss: 0.056 | Acc: 98.223% (45764/46592)\n",
            "91 118 Loss: 0.056 | Acc: 98.225% (46268/47104)\n",
            "92 118 Loss: 0.056 | Acc: 98.240% (46778/47616)\n",
            "93 118 Loss: 0.055 | Acc: 98.240% (47281/48128)\n",
            "94 118 Loss: 0.055 | Acc: 98.242% (47785/48640)\n",
            "95 118 Loss: 0.055 | Acc: 98.240% (48287/49152)\n",
            "96 118 Loss: 0.055 | Acc: 98.242% (48791/49664)\n",
            "97 118 Loss: 0.056 | Acc: 98.240% (49293/50176)\n",
            "98 118 Loss: 0.055 | Acc: 98.242% (49797/50688)\n",
            "99 118 Loss: 0.056 | Acc: 98.242% (50300/51200)\n",
            "100 118 Loss: 0.056 | Acc: 98.244% (50804/51712)\n",
            "101 118 Loss: 0.056 | Acc: 98.248% (51309/52224)\n",
            "102 118 Loss: 0.056 | Acc: 98.244% (51810/52736)\n",
            "103 118 Loss: 0.056 | Acc: 98.244% (52313/53248)\n",
            "104 118 Loss: 0.056 | Acc: 98.238% (52813/53760)\n",
            "105 118 Loss: 0.056 | Acc: 98.235% (53314/54272)\n",
            "106 118 Loss: 0.056 | Acc: 98.233% (53816/54784)\n",
            "107 118 Loss: 0.056 | Acc: 98.235% (54320/55296)\n",
            "108 118 Loss: 0.056 | Acc: 98.226% (54818/55808)\n",
            "109 118 Loss: 0.056 | Acc: 98.224% (55320/56320)\n",
            "110 118 Loss: 0.056 | Acc: 98.218% (55819/56832)\n",
            "111 118 Loss: 0.056 | Acc: 98.211% (56318/57344)\n",
            "112 118 Loss: 0.056 | Acc: 98.208% (56819/57856)\n",
            "113 118 Loss: 0.056 | Acc: 98.206% (57321/58368)\n",
            "114 118 Loss: 0.056 | Acc: 98.207% (57824/58880)\n",
            "115 118 Loss: 0.056 | Acc: 98.207% (58327/59392)\n",
            "116 118 Loss: 0.056 | Acc: 98.210% (58832/59904)\n",
            "117 118 Loss: 0.056 | Acc: 98.213% (58928/60000)\n",
            "\n",
            "Epoch: 4\n",
            "Time elapsed: 7.03 min\n",
            "0 118 Loss: 0.043 | Acc: 98.828% (506/512)\n",
            "1 118 Loss: 0.053 | Acc: 98.145% (1005/1024)\n",
            "2 118 Loss: 0.055 | Acc: 97.982% (1505/1536)\n",
            "3 118 Loss: 0.060 | Acc: 97.949% (2006/2048)\n",
            "4 118 Loss: 0.053 | Acc: 98.164% (2513/2560)\n",
            "5 118 Loss: 0.051 | Acc: 98.372% (3022/3072)\n",
            "6 118 Loss: 0.053 | Acc: 98.186% (3519/3584)\n",
            "7 118 Loss: 0.053 | Acc: 98.169% (4021/4096)\n",
            "8 118 Loss: 0.055 | Acc: 98.199% (4525/4608)\n",
            "9 118 Loss: 0.058 | Acc: 98.105% (5023/5120)\n",
            "10 118 Loss: 0.057 | Acc: 98.136% (5527/5632)\n",
            "11 118 Loss: 0.056 | Acc: 98.177% (6032/6144)\n",
            "12 118 Loss: 0.056 | Acc: 98.182% (6535/6656)\n",
            "13 118 Loss: 0.054 | Acc: 98.256% (7043/7168)\n",
            "14 118 Loss: 0.054 | Acc: 98.255% (7546/7680)\n",
            "15 118 Loss: 0.055 | Acc: 98.230% (8047/8192)\n",
            "16 118 Loss: 0.054 | Acc: 98.265% (8553/8704)\n",
            "17 118 Loss: 0.053 | Acc: 98.286% (9058/9216)\n",
            "18 118 Loss: 0.053 | Acc: 98.314% (9564/9728)\n",
            "19 118 Loss: 0.053 | Acc: 98.291% (10065/10240)\n",
            "20 118 Loss: 0.052 | Acc: 98.279% (10567/10752)\n",
            "21 118 Loss: 0.053 | Acc: 98.269% (11069/11264)\n",
            "22 118 Loss: 0.053 | Acc: 98.276% (11573/11776)\n",
            "23 118 Loss: 0.053 | Acc: 98.275% (12076/12288)\n",
            "24 118 Loss: 0.054 | Acc: 98.281% (12580/12800)\n",
            "25 118 Loss: 0.055 | Acc: 98.272% (13082/13312)\n",
            "26 118 Loss: 0.054 | Acc: 98.286% (13587/13824)\n",
            "27 118 Loss: 0.054 | Acc: 98.277% (14089/14336)\n",
            "28 118 Loss: 0.055 | Acc: 98.276% (14592/14848)\n",
            "29 118 Loss: 0.055 | Acc: 98.262% (15093/15360)\n",
            "30 118 Loss: 0.056 | Acc: 98.255% (15595/15872)\n",
            "31 118 Loss: 0.055 | Acc: 98.254% (16098/16384)\n",
            "32 118 Loss: 0.056 | Acc: 98.236% (16598/16896)\n",
            "33 118 Loss: 0.055 | Acc: 98.242% (17102/17408)\n",
            "34 118 Loss: 0.055 | Acc: 98.237% (17604/17920)\n",
            "35 118 Loss: 0.055 | Acc: 98.242% (18108/18432)\n",
            "36 118 Loss: 0.055 | Acc: 98.242% (18611/18944)\n",
            "37 118 Loss: 0.054 | Acc: 98.263% (19118/19456)\n",
            "38 118 Loss: 0.054 | Acc: 98.267% (19622/19968)\n",
            "39 118 Loss: 0.054 | Acc: 98.262% (20124/20480)\n",
            "40 118 Loss: 0.055 | Acc: 98.237% (20622/20992)\n",
            "41 118 Loss: 0.054 | Acc: 98.256% (21129/21504)\n",
            "42 118 Loss: 0.054 | Acc: 98.269% (21635/22016)\n",
            "43 118 Loss: 0.053 | Acc: 98.282% (22141/22528)\n",
            "44 118 Loss: 0.053 | Acc: 98.281% (22644/23040)\n",
            "45 118 Loss: 0.053 | Acc: 98.280% (23147/23552)\n",
            "46 118 Loss: 0.053 | Acc: 98.271% (23648/24064)\n",
            "47 118 Loss: 0.052 | Acc: 98.295% (24157/24576)\n",
            "48 118 Loss: 0.053 | Acc: 98.290% (24659/25088)\n",
            "49 118 Loss: 0.052 | Acc: 98.305% (25166/25600)\n",
            "50 118 Loss: 0.052 | Acc: 98.315% (25672/26112)\n",
            "51 118 Loss: 0.052 | Acc: 98.321% (26177/26624)\n",
            "52 118 Loss: 0.052 | Acc: 98.316% (26679/27136)\n",
            "53 118 Loss: 0.052 | Acc: 98.296% (27177/27648)\n",
            "54 118 Loss: 0.052 | Acc: 98.303% (27682/28160)\n",
            "55 118 Loss: 0.052 | Acc: 98.308% (28187/28672)\n",
            "56 118 Loss: 0.052 | Acc: 98.314% (28692/29184)\n",
            "57 118 Loss: 0.052 | Acc: 98.316% (29196/29696)\n",
            "58 118 Loss: 0.051 | Acc: 98.322% (29701/30208)\n",
            "59 118 Loss: 0.051 | Acc: 98.317% (30203/30720)\n",
            "60 118 Loss: 0.051 | Acc: 98.329% (30710/31232)\n",
            "61 118 Loss: 0.051 | Acc: 98.330% (31214/31744)\n",
            "62 118 Loss: 0.051 | Acc: 98.320% (31714/32256)\n",
            "63 118 Loss: 0.051 | Acc: 98.309% (32214/32768)\n",
            "64 118 Loss: 0.051 | Acc: 98.311% (32718/33280)\n",
            "65 118 Loss: 0.051 | Acc: 98.319% (33224/33792)\n",
            "66 118 Loss: 0.051 | Acc: 98.303% (33722/34304)\n",
            "67 118 Loss: 0.052 | Acc: 98.300% (34224/34816)\n",
            "68 118 Loss: 0.052 | Acc: 98.290% (34724/35328)\n",
            "69 118 Loss: 0.052 | Acc: 98.290% (35227/35840)\n",
            "70 118 Loss: 0.052 | Acc: 98.300% (35734/36352)\n",
            "71 118 Loss: 0.052 | Acc: 98.305% (36239/36864)\n",
            "72 118 Loss: 0.051 | Acc: 98.314% (36746/37376)\n",
            "73 118 Loss: 0.051 | Acc: 98.316% (37250/37888)\n",
            "74 118 Loss: 0.051 | Acc: 98.315% (37753/38400)\n",
            "75 118 Loss: 0.051 | Acc: 98.322% (38259/38912)\n",
            "76 118 Loss: 0.051 | Acc: 98.328% (38765/39424)\n",
            "77 118 Loss: 0.051 | Acc: 98.327% (39268/39936)\n",
            "78 118 Loss: 0.051 | Acc: 98.331% (39773/40448)\n",
            "79 118 Loss: 0.051 | Acc: 98.342% (40281/40960)\n",
            "80 118 Loss: 0.051 | Acc: 98.336% (40782/41472)\n",
            "81 118 Loss: 0.051 | Acc: 98.342% (41288/41984)\n",
            "82 118 Loss: 0.051 | Acc: 98.346% (41793/42496)\n",
            "83 118 Loss: 0.051 | Acc: 98.351% (42299/43008)\n",
            "84 118 Loss: 0.050 | Acc: 98.362% (42807/43520)\n",
            "85 118 Loss: 0.050 | Acc: 98.374% (43316/44032)\n",
            "86 118 Loss: 0.050 | Acc: 98.377% (43821/44544)\n",
            "87 118 Loss: 0.050 | Acc: 98.389% (44330/45056)\n",
            "88 118 Loss: 0.050 | Acc: 98.396% (44837/45568)\n",
            "89 118 Loss: 0.049 | Acc: 98.405% (45345/46080)\n",
            "90 118 Loss: 0.049 | Acc: 98.412% (45852/46592)\n",
            "91 118 Loss: 0.049 | Acc: 98.412% (46356/47104)\n",
            "92 118 Loss: 0.049 | Acc: 98.414% (46861/47616)\n",
            "93 118 Loss: 0.049 | Acc: 98.421% (47368/48128)\n",
            "94 118 Loss: 0.049 | Acc: 98.415% (47869/48640)\n",
            "95 118 Loss: 0.049 | Acc: 98.417% (48374/49152)\n",
            "96 118 Loss: 0.049 | Acc: 98.411% (48875/49664)\n",
            "97 118 Loss: 0.049 | Acc: 98.416% (49381/50176)\n",
            "98 118 Loss: 0.049 | Acc: 98.418% (49886/50688)\n",
            "99 118 Loss: 0.049 | Acc: 98.426% (50394/51200)\n",
            "100 118 Loss: 0.049 | Acc: 98.412% (50891/51712)\n",
            "101 118 Loss: 0.049 | Acc: 98.401% (51389/52224)\n",
            "102 118 Loss: 0.049 | Acc: 98.403% (51894/52736)\n",
            "103 118 Loss: 0.049 | Acc: 98.409% (52401/53248)\n",
            "104 118 Loss: 0.049 | Acc: 98.408% (52904/53760)\n",
            "105 118 Loss: 0.049 | Acc: 98.410% (53409/54272)\n",
            "106 118 Loss: 0.049 | Acc: 98.406% (53911/54784)\n",
            "107 118 Loss: 0.049 | Acc: 98.401% (54412/55296)\n",
            "108 118 Loss: 0.049 | Acc: 98.398% (54914/55808)\n",
            "109 118 Loss: 0.050 | Acc: 98.388% (55412/56320)\n",
            "110 118 Loss: 0.050 | Acc: 98.385% (55914/56832)\n",
            "111 118 Loss: 0.050 | Acc: 98.385% (56418/57344)\n",
            "112 118 Loss: 0.050 | Acc: 98.384% (56921/57856)\n",
            "113 118 Loss: 0.050 | Acc: 98.388% (57427/58368)\n",
            "114 118 Loss: 0.050 | Acc: 98.395% (57935/58880)\n",
            "115 118 Loss: 0.050 | Acc: 98.394% (58438/59392)\n",
            "116 118 Loss: 0.050 | Acc: 98.382% (58935/59904)\n",
            "117 118 Loss: 0.050 | Acc: 98.382% (59029/60000)\n",
            "\n",
            "Epoch: 5\n",
            "Time elapsed: 8.80 min\n",
            "0 118 Loss: 0.028 | Acc: 99.414% (509/512)\n",
            "1 118 Loss: 0.036 | Acc: 99.121% (1015/1024)\n",
            "2 118 Loss: 0.039 | Acc: 98.958% (1520/1536)\n",
            "3 118 Loss: 0.044 | Acc: 98.779% (2023/2048)\n",
            "4 118 Loss: 0.041 | Acc: 98.789% (2529/2560)\n",
            "5 118 Loss: 0.043 | Acc: 98.633% (3030/3072)\n",
            "6 118 Loss: 0.045 | Acc: 98.549% (3532/3584)\n",
            "7 118 Loss: 0.046 | Acc: 98.511% (4035/4096)\n",
            "8 118 Loss: 0.047 | Acc: 98.481% (4538/4608)\n",
            "9 118 Loss: 0.047 | Acc: 98.496% (5043/5120)\n",
            "10 118 Loss: 0.050 | Acc: 98.331% (5538/5632)\n",
            "11 118 Loss: 0.050 | Acc: 98.324% (6041/6144)\n",
            "12 118 Loss: 0.050 | Acc: 98.362% (6547/6656)\n",
            "13 118 Loss: 0.050 | Acc: 98.410% (7054/7168)\n",
            "14 118 Loss: 0.050 | Acc: 98.424% (7559/7680)\n",
            "15 118 Loss: 0.050 | Acc: 98.413% (8062/8192)\n",
            "16 118 Loss: 0.050 | Acc: 98.357% (8561/8704)\n",
            "17 118 Loss: 0.050 | Acc: 98.351% (9064/9216)\n",
            "18 118 Loss: 0.049 | Acc: 98.376% (9570/9728)\n",
            "19 118 Loss: 0.049 | Acc: 98.389% (10075/10240)\n",
            "20 118 Loss: 0.048 | Acc: 98.438% (10584/10752)\n",
            "21 118 Loss: 0.048 | Acc: 98.438% (11088/11264)\n",
            "22 118 Loss: 0.049 | Acc: 98.395% (11587/11776)\n",
            "23 118 Loss: 0.049 | Acc: 98.364% (12087/12288)\n",
            "24 118 Loss: 0.049 | Acc: 98.383% (12593/12800)\n",
            "25 118 Loss: 0.049 | Acc: 98.400% (13099/13312)\n",
            "26 118 Loss: 0.048 | Acc: 98.430% (13607/13824)\n",
            "27 118 Loss: 0.048 | Acc: 98.417% (14109/14336)\n",
            "28 118 Loss: 0.048 | Acc: 98.417% (14613/14848)\n",
            "29 118 Loss: 0.050 | Acc: 98.405% (15115/15360)\n",
            "30 118 Loss: 0.049 | Acc: 98.438% (15624/15872)\n",
            "31 118 Loss: 0.049 | Acc: 98.431% (16127/16384)\n",
            "32 118 Loss: 0.050 | Acc: 98.426% (16630/16896)\n",
            "33 118 Loss: 0.050 | Acc: 98.420% (17133/17408)\n",
            "34 118 Loss: 0.050 | Acc: 98.432% (17639/17920)\n",
            "35 118 Loss: 0.049 | Acc: 98.465% (18149/18432)\n",
            "36 118 Loss: 0.050 | Acc: 98.438% (18648/18944)\n",
            "37 118 Loss: 0.049 | Acc: 98.432% (19151/19456)\n",
            "38 118 Loss: 0.049 | Acc: 98.427% (19654/19968)\n",
            "39 118 Loss: 0.049 | Acc: 98.428% (20158/20480)\n",
            "40 118 Loss: 0.049 | Acc: 98.433% (20663/20992)\n",
            "41 118 Loss: 0.049 | Acc: 98.442% (21169/21504)\n",
            "42 118 Loss: 0.048 | Acc: 98.451% (21675/22016)\n",
            "43 118 Loss: 0.048 | Acc: 98.469% (22183/22528)\n",
            "44 118 Loss: 0.047 | Acc: 98.494% (22693/23040)\n",
            "45 118 Loss: 0.047 | Acc: 98.501% (23199/23552)\n",
            "46 118 Loss: 0.047 | Acc: 98.516% (23707/24064)\n",
            "47 118 Loss: 0.047 | Acc: 98.515% (24211/24576)\n",
            "48 118 Loss: 0.047 | Acc: 98.521% (24717/25088)\n",
            "49 118 Loss: 0.047 | Acc: 98.527% (25223/25600)\n",
            "50 118 Loss: 0.046 | Acc: 98.537% (25730/26112)\n",
            "51 118 Loss: 0.046 | Acc: 98.535% (26234/26624)\n",
            "52 118 Loss: 0.046 | Acc: 98.537% (26739/27136)\n",
            "53 118 Loss: 0.046 | Acc: 98.550% (27247/27648)\n",
            "54 118 Loss: 0.047 | Acc: 98.526% (27745/28160)\n",
            "55 118 Loss: 0.047 | Acc: 98.521% (28248/28672)\n",
            "56 118 Loss: 0.046 | Acc: 98.537% (28757/29184)\n",
            "57 118 Loss: 0.046 | Acc: 98.542% (29263/29696)\n",
            "58 118 Loss: 0.046 | Acc: 98.537% (29766/30208)\n",
            "59 118 Loss: 0.046 | Acc: 98.542% (30272/30720)\n",
            "60 118 Loss: 0.046 | Acc: 98.550% (30779/31232)\n",
            "61 118 Loss: 0.047 | Acc: 98.541% (31281/31744)\n",
            "62 118 Loss: 0.047 | Acc: 98.534% (31783/32256)\n",
            "63 118 Loss: 0.047 | Acc: 98.535% (32288/32768)\n",
            "64 118 Loss: 0.047 | Acc: 98.540% (32794/33280)\n",
            "65 118 Loss: 0.047 | Acc: 98.538% (33298/33792)\n",
            "66 118 Loss: 0.047 | Acc: 98.537% (33802/34304)\n",
            "67 118 Loss: 0.047 | Acc: 98.532% (34305/34816)\n",
            "68 118 Loss: 0.046 | Acc: 98.539% (34812/35328)\n",
            "69 118 Loss: 0.046 | Acc: 98.549% (35320/35840)\n",
            "70 118 Loss: 0.046 | Acc: 98.556% (35827/36352)\n",
            "71 118 Loss: 0.046 | Acc: 98.560% (36333/36864)\n",
            "72 118 Loss: 0.046 | Acc: 98.561% (36838/37376)\n",
            "73 118 Loss: 0.046 | Acc: 98.554% (37340/37888)\n",
            "74 118 Loss: 0.046 | Acc: 98.547% (37842/38400)\n",
            "75 118 Loss: 0.046 | Acc: 98.548% (38347/38912)\n",
            "76 118 Loss: 0.046 | Acc: 98.549% (38852/39424)\n",
            "77 118 Loss: 0.046 | Acc: 98.558% (39360/39936)\n",
            "78 118 Loss: 0.045 | Acc: 98.566% (39868/40448)\n",
            "79 118 Loss: 0.046 | Acc: 98.552% (40367/40960)\n",
            "80 118 Loss: 0.045 | Acc: 98.553% (40872/41472)\n",
            "81 118 Loss: 0.045 | Acc: 98.559% (41379/41984)\n",
            "82 118 Loss: 0.045 | Acc: 98.558% (41883/42496)\n",
            "83 118 Loss: 0.045 | Acc: 98.563% (42390/43008)\n",
            "84 118 Loss: 0.045 | Acc: 98.568% (42897/43520)\n",
            "85 118 Loss: 0.045 | Acc: 98.567% (43401/44032)\n",
            "86 118 Loss: 0.045 | Acc: 98.577% (43910/44544)\n",
            "87 118 Loss: 0.045 | Acc: 98.580% (44416/45056)\n",
            "88 118 Loss: 0.045 | Acc: 98.589% (44925/45568)\n",
            "89 118 Loss: 0.045 | Acc: 98.594% (45432/46080)\n",
            "90 118 Loss: 0.045 | Acc: 98.590% (45935/46592)\n",
            "91 118 Loss: 0.045 | Acc: 98.595% (46442/47104)\n",
            "92 118 Loss: 0.045 | Acc: 98.589% (46944/47616)\n",
            "93 118 Loss: 0.045 | Acc: 98.587% (47448/48128)\n",
            "94 118 Loss: 0.045 | Acc: 98.577% (47948/48640)\n",
            "95 118 Loss: 0.045 | Acc: 98.586% (48457/49152)\n",
            "96 118 Loss: 0.045 | Acc: 98.593% (48965/49664)\n",
            "97 118 Loss: 0.045 | Acc: 98.595% (49471/50176)\n",
            "98 118 Loss: 0.044 | Acc: 98.597% (49977/50688)\n",
            "99 118 Loss: 0.044 | Acc: 98.594% (50480/51200)\n",
            "100 118 Loss: 0.044 | Acc: 98.598% (50987/51712)\n",
            "101 118 Loss: 0.044 | Acc: 98.606% (51496/52224)\n",
            "102 118 Loss: 0.044 | Acc: 98.608% (52002/52736)\n",
            "103 118 Loss: 0.044 | Acc: 98.612% (52509/53248)\n",
            "104 118 Loss: 0.044 | Acc: 98.601% (53008/53760)\n",
            "105 118 Loss: 0.044 | Acc: 98.609% (53517/54272)\n",
            "106 118 Loss: 0.044 | Acc: 98.611% (54023/54784)\n",
            "107 118 Loss: 0.044 | Acc: 98.613% (54529/55296)\n",
            "108 118 Loss: 0.044 | Acc: 98.615% (55035/55808)\n",
            "109 118 Loss: 0.045 | Acc: 98.606% (55535/56320)\n",
            "110 118 Loss: 0.044 | Acc: 98.608% (56041/56832)\n",
            "111 118 Loss: 0.044 | Acc: 98.608% (56546/57344)\n",
            "112 118 Loss: 0.044 | Acc: 98.600% (57046/57856)\n",
            "113 118 Loss: 0.045 | Acc: 98.590% (57545/58368)\n",
            "114 118 Loss: 0.045 | Acc: 98.595% (58053/58880)\n",
            "115 118 Loss: 0.045 | Acc: 98.592% (58556/59392)\n",
            "116 118 Loss: 0.045 | Acc: 98.589% (59059/59904)\n",
            "117 118 Loss: 0.045 | Acc: 98.590% (59154/60000)\n",
            "\n",
            "Epoch: 6\n",
            "Time elapsed: 10.56 min\n",
            "0 118 Loss: 0.050 | Acc: 98.633% (505/512)\n",
            "1 118 Loss: 0.044 | Acc: 98.535% (1009/1024)\n",
            "2 118 Loss: 0.051 | Acc: 98.307% (1510/1536)\n",
            "3 118 Loss: 0.058 | Acc: 97.998% (2007/2048)\n",
            "4 118 Loss: 0.050 | Acc: 98.320% (2517/2560)\n",
            "5 118 Loss: 0.049 | Acc: 98.340% (3021/3072)\n",
            "6 118 Loss: 0.046 | Acc: 98.438% (3528/3584)\n",
            "7 118 Loss: 0.045 | Acc: 98.438% (4032/4096)\n",
            "8 118 Loss: 0.044 | Acc: 98.481% (4538/4608)\n",
            "9 118 Loss: 0.044 | Acc: 98.516% (5044/5120)\n",
            "10 118 Loss: 0.042 | Acc: 98.615% (5554/5632)\n",
            "11 118 Loss: 0.041 | Acc: 98.617% (6059/6144)\n",
            "12 118 Loss: 0.042 | Acc: 98.603% (6563/6656)\n",
            "13 118 Loss: 0.044 | Acc: 98.591% (7067/7168)\n",
            "14 118 Loss: 0.043 | Acc: 98.594% (7572/7680)\n",
            "15 118 Loss: 0.044 | Acc: 98.511% (8070/8192)\n",
            "16 118 Loss: 0.044 | Acc: 98.564% (8579/8704)\n",
            "17 118 Loss: 0.044 | Acc: 98.568% (9084/9216)\n",
            "18 118 Loss: 0.044 | Acc: 98.561% (9588/9728)\n",
            "19 118 Loss: 0.045 | Acc: 98.564% (10093/10240)\n",
            "20 118 Loss: 0.045 | Acc: 98.586% (10600/10752)\n",
            "21 118 Loss: 0.044 | Acc: 98.588% (11105/11264)\n",
            "22 118 Loss: 0.044 | Acc: 98.599% (11611/11776)\n",
            "23 118 Loss: 0.044 | Acc: 98.600% (12116/12288)\n",
            "24 118 Loss: 0.044 | Acc: 98.609% (12622/12800)\n",
            "25 118 Loss: 0.043 | Acc: 98.610% (13127/13312)\n",
            "26 118 Loss: 0.044 | Acc: 98.597% (13630/13824)\n",
            "27 118 Loss: 0.044 | Acc: 98.577% (14132/14336)\n",
            "28 118 Loss: 0.044 | Acc: 98.572% (14636/14848)\n",
            "29 118 Loss: 0.043 | Acc: 98.587% (15143/15360)\n",
            "30 118 Loss: 0.043 | Acc: 98.608% (15651/15872)\n",
            "31 118 Loss: 0.043 | Acc: 98.608% (16156/16384)\n",
            "32 118 Loss: 0.044 | Acc: 98.585% (16657/16896)\n",
            "33 118 Loss: 0.044 | Acc: 98.558% (17157/17408)\n",
            "34 118 Loss: 0.044 | Acc: 98.571% (17664/17920)\n",
            "35 118 Loss: 0.044 | Acc: 98.546% (18164/18432)\n",
            "36 118 Loss: 0.044 | Acc: 98.548% (18669/18944)\n",
            "37 118 Loss: 0.045 | Acc: 98.540% (19172/19456)\n",
            "38 118 Loss: 0.045 | Acc: 98.533% (19675/19968)\n",
            "39 118 Loss: 0.045 | Acc: 98.550% (20183/20480)\n",
            "40 118 Loss: 0.045 | Acc: 98.542% (20686/20992)\n",
            "41 118 Loss: 0.045 | Acc: 98.544% (21191/21504)\n",
            "42 118 Loss: 0.045 | Acc: 98.551% (21697/22016)\n",
            "43 118 Loss: 0.044 | Acc: 98.566% (22205/22528)\n",
            "44 118 Loss: 0.044 | Acc: 98.572% (22711/23040)\n",
            "45 118 Loss: 0.045 | Acc: 98.556% (23212/23552)\n",
            "46 118 Loss: 0.044 | Acc: 98.570% (23720/24064)\n",
            "47 118 Loss: 0.045 | Acc: 98.560% (24222/24576)\n",
            "48 118 Loss: 0.045 | Acc: 98.557% (24726/25088)\n",
            "49 118 Loss: 0.045 | Acc: 98.562% (25232/25600)\n",
            "50 118 Loss: 0.044 | Acc: 98.575% (25740/26112)\n",
            "51 118 Loss: 0.044 | Acc: 98.595% (26250/26624)\n",
            "52 118 Loss: 0.043 | Acc: 98.607% (26758/27136)\n",
            "53 118 Loss: 0.043 | Acc: 98.629% (27269/27648)\n",
            "54 118 Loss: 0.042 | Acc: 98.647% (27779/28160)\n",
            "55 118 Loss: 0.042 | Acc: 98.654% (28286/28672)\n",
            "56 118 Loss: 0.042 | Acc: 98.667% (28795/29184)\n",
            "57 118 Loss: 0.042 | Acc: 98.666% (29300/29696)\n",
            "58 118 Loss: 0.041 | Acc: 98.666% (29805/30208)\n",
            "59 118 Loss: 0.041 | Acc: 98.659% (30308/30720)\n",
            "60 118 Loss: 0.041 | Acc: 98.662% (30814/31232)\n",
            "61 118 Loss: 0.041 | Acc: 98.658% (31318/31744)\n",
            "62 118 Loss: 0.041 | Acc: 98.664% (31825/32256)\n",
            "63 118 Loss: 0.041 | Acc: 98.669% (32332/32768)\n",
            "64 118 Loss: 0.041 | Acc: 98.684% (32842/33280)\n",
            "65 118 Loss: 0.041 | Acc: 98.677% (33345/33792)\n",
            "66 118 Loss: 0.040 | Acc: 98.685% (33853/34304)\n",
            "67 118 Loss: 0.040 | Acc: 98.687% (34359/34816)\n",
            "68 118 Loss: 0.040 | Acc: 98.701% (34869/35328)\n",
            "69 118 Loss: 0.040 | Acc: 98.700% (35374/35840)\n",
            "70 118 Loss: 0.040 | Acc: 98.699% (35879/36352)\n",
            "71 118 Loss: 0.041 | Acc: 98.684% (36379/36864)\n",
            "72 118 Loss: 0.041 | Acc: 98.678% (36882/37376)\n",
            "73 118 Loss: 0.041 | Acc: 98.675% (37386/37888)\n",
            "74 118 Loss: 0.041 | Acc: 98.674% (37891/38400)\n",
            "75 118 Loss: 0.041 | Acc: 98.674% (38396/38912)\n",
            "76 118 Loss: 0.041 | Acc: 98.668% (38899/39424)\n",
            "77 118 Loss: 0.041 | Acc: 98.673% (39406/39936)\n",
            "78 118 Loss: 0.041 | Acc: 98.667% (39909/40448)\n",
            "79 118 Loss: 0.041 | Acc: 98.660% (40411/40960)\n",
            "80 118 Loss: 0.041 | Acc: 98.652% (40913/41472)\n",
            "81 118 Loss: 0.041 | Acc: 98.652% (41418/41984)\n",
            "82 118 Loss: 0.041 | Acc: 98.661% (41927/42496)\n",
            "83 118 Loss: 0.041 | Acc: 98.656% (42430/43008)\n",
            "84 118 Loss: 0.041 | Acc: 98.663% (42938/43520)\n",
            "85 118 Loss: 0.042 | Acc: 98.656% (43440/44032)\n",
            "86 118 Loss: 0.041 | Acc: 98.653% (43944/44544)\n",
            "87 118 Loss: 0.042 | Acc: 98.648% (44447/45056)\n",
            "88 118 Loss: 0.041 | Acc: 98.653% (44954/45568)\n",
            "89 118 Loss: 0.041 | Acc: 98.655% (45460/46080)\n",
            "90 118 Loss: 0.041 | Acc: 98.652% (45964/46592)\n",
            "91 118 Loss: 0.041 | Acc: 98.663% (46474/47104)\n",
            "92 118 Loss: 0.041 | Acc: 98.658% (46977/47616)\n",
            "93 118 Loss: 0.041 | Acc: 98.656% (47481/48128)\n",
            "94 118 Loss: 0.041 | Acc: 98.657% (47987/48640)\n",
            "95 118 Loss: 0.041 | Acc: 98.659% (48493/49152)\n",
            "96 118 Loss: 0.041 | Acc: 98.653% (48995/49664)\n",
            "97 118 Loss: 0.041 | Acc: 98.653% (49500/50176)\n",
            "98 118 Loss: 0.041 | Acc: 98.649% (50003/50688)\n",
            "99 118 Loss: 0.041 | Acc: 98.650% (50509/51200)\n",
            "100 118 Loss: 0.041 | Acc: 98.654% (51016/51712)\n",
            "101 118 Loss: 0.041 | Acc: 98.658% (51523/52224)\n",
            "102 118 Loss: 0.041 | Acc: 98.661% (52030/52736)\n",
            "103 118 Loss: 0.041 | Acc: 98.670% (52540/53248)\n",
            "104 118 Loss: 0.041 | Acc: 98.676% (53048/53760)\n",
            "105 118 Loss: 0.040 | Acc: 98.681% (53556/54272)\n",
            "106 118 Loss: 0.040 | Acc: 98.684% (54063/54784)\n",
            "107 118 Loss: 0.040 | Acc: 98.682% (54567/55296)\n",
            "108 118 Loss: 0.040 | Acc: 98.676% (55069/55808)\n",
            "109 118 Loss: 0.040 | Acc: 98.674% (55573/56320)\n",
            "110 118 Loss: 0.040 | Acc: 98.675% (56079/56832)\n",
            "111 118 Loss: 0.040 | Acc: 98.675% (56584/57344)\n",
            "112 118 Loss: 0.040 | Acc: 98.679% (57092/57856)\n",
            "113 118 Loss: 0.040 | Acc: 98.677% (57596/58368)\n",
            "114 118 Loss: 0.040 | Acc: 98.670% (58097/58880)\n",
            "115 118 Loss: 0.040 | Acc: 98.668% (58601/59392)\n",
            "116 118 Loss: 0.041 | Acc: 98.653% (59097/59904)\n",
            "117 118 Loss: 0.041 | Acc: 98.652% (59191/60000)\n",
            "\n",
            "Epoch: 7\n",
            "Time elapsed: 12.33 min\n",
            "0 118 Loss: 0.037 | Acc: 98.438% (504/512)\n",
            "1 118 Loss: 0.043 | Acc: 98.535% (1009/1024)\n",
            "2 118 Loss: 0.048 | Acc: 98.242% (1509/1536)\n",
            "3 118 Loss: 0.052 | Acc: 98.047% (2008/2048)\n",
            "4 118 Loss: 0.050 | Acc: 98.164% (2513/2560)\n",
            "5 118 Loss: 0.051 | Acc: 98.145% (3015/3072)\n",
            "6 118 Loss: 0.054 | Acc: 98.075% (3515/3584)\n",
            "7 118 Loss: 0.055 | Acc: 98.071% (4017/4096)\n",
            "8 118 Loss: 0.055 | Acc: 98.134% (4522/4608)\n",
            "9 118 Loss: 0.054 | Acc: 98.184% (5027/5120)\n",
            "10 118 Loss: 0.056 | Acc: 98.153% (5528/5632)\n",
            "11 118 Loss: 0.057 | Acc: 98.145% (6030/6144)\n",
            "12 118 Loss: 0.057 | Acc: 98.137% (6532/6656)\n",
            "13 118 Loss: 0.054 | Acc: 98.214% (7040/7168)\n",
            "14 118 Loss: 0.053 | Acc: 98.281% (7548/7680)\n",
            "15 118 Loss: 0.051 | Acc: 98.328% (8055/8192)\n",
            "16 118 Loss: 0.050 | Acc: 98.346% (8560/8704)\n",
            "17 118 Loss: 0.050 | Acc: 98.362% (9065/9216)\n",
            "18 118 Loss: 0.050 | Acc: 98.355% (9568/9728)\n",
            "19 118 Loss: 0.049 | Acc: 98.389% (10075/10240)\n",
            "20 118 Loss: 0.049 | Acc: 98.400% (10580/10752)\n",
            "21 118 Loss: 0.050 | Acc: 98.384% (11082/11264)\n",
            "22 118 Loss: 0.050 | Acc: 98.404% (11588/11776)\n",
            "23 118 Loss: 0.049 | Acc: 98.429% (12095/12288)\n",
            "24 118 Loss: 0.048 | Acc: 98.453% (12602/12800)\n",
            "25 118 Loss: 0.048 | Acc: 98.445% (13105/13312)\n",
            "26 118 Loss: 0.047 | Acc: 98.481% (13614/13824)\n",
            "27 118 Loss: 0.046 | Acc: 98.500% (14121/14336)\n",
            "28 118 Loss: 0.047 | Acc: 98.491% (14624/14848)\n",
            "29 118 Loss: 0.046 | Acc: 98.503% (15130/15360)\n",
            "30 118 Loss: 0.047 | Acc: 98.488% (15632/15872)\n",
            "31 118 Loss: 0.046 | Acc: 98.511% (16140/16384)\n",
            "32 118 Loss: 0.046 | Acc: 98.497% (16642/16896)\n",
            "33 118 Loss: 0.046 | Acc: 98.512% (17149/17408)\n",
            "34 118 Loss: 0.046 | Acc: 98.521% (17655/17920)\n",
            "35 118 Loss: 0.046 | Acc: 98.519% (18159/18432)\n",
            "36 118 Loss: 0.045 | Acc: 98.522% (18664/18944)\n",
            "37 118 Loss: 0.046 | Acc: 98.499% (19164/19456)\n",
            "38 118 Loss: 0.046 | Acc: 98.513% (19671/19968)\n",
            "39 118 Loss: 0.046 | Acc: 98.525% (20178/20480)\n",
            "40 118 Loss: 0.045 | Acc: 98.528% (20683/20992)\n",
            "41 118 Loss: 0.045 | Acc: 98.540% (21190/21504)\n",
            "42 118 Loss: 0.045 | Acc: 98.542% (21695/22016)\n",
            "43 118 Loss: 0.045 | Acc: 98.544% (22200/22528)\n",
            "44 118 Loss: 0.045 | Acc: 98.537% (22703/23040)\n",
            "45 118 Loss: 0.045 | Acc: 98.514% (23202/23552)\n",
            "46 118 Loss: 0.045 | Acc: 98.529% (23710/24064)\n",
            "47 118 Loss: 0.045 | Acc: 98.531% (24215/24576)\n",
            "48 118 Loss: 0.045 | Acc: 98.521% (24717/25088)\n",
            "49 118 Loss: 0.045 | Acc: 98.531% (25224/25600)\n",
            "50 118 Loss: 0.045 | Acc: 98.526% (25727/26112)\n",
            "51 118 Loss: 0.045 | Acc: 98.535% (26234/26624)\n",
            "52 118 Loss: 0.044 | Acc: 98.537% (26739/27136)\n",
            "53 118 Loss: 0.044 | Acc: 98.539% (27244/27648)\n",
            "54 118 Loss: 0.044 | Acc: 98.548% (27751/28160)\n",
            "55 118 Loss: 0.044 | Acc: 98.535% (28252/28672)\n",
            "56 118 Loss: 0.044 | Acc: 98.527% (28754/29184)\n",
            "57 118 Loss: 0.044 | Acc: 98.535% (29261/29696)\n",
            "58 118 Loss: 0.044 | Acc: 98.517% (29760/30208)\n",
            "59 118 Loss: 0.044 | Acc: 98.522% (30266/30720)\n",
            "60 118 Loss: 0.044 | Acc: 98.543% (30777/31232)\n",
            "61 118 Loss: 0.044 | Acc: 98.551% (31284/31744)\n",
            "62 118 Loss: 0.043 | Acc: 98.562% (31792/32256)\n",
            "63 118 Loss: 0.043 | Acc: 98.572% (32300/32768)\n",
            "64 118 Loss: 0.043 | Acc: 98.588% (32810/33280)\n",
            "65 118 Loss: 0.043 | Acc: 98.594% (33317/33792)\n",
            "66 118 Loss: 0.042 | Acc: 98.601% (33824/34304)\n",
            "67 118 Loss: 0.042 | Acc: 98.607% (34331/34816)\n",
            "68 118 Loss: 0.042 | Acc: 98.616% (34839/35328)\n",
            "69 118 Loss: 0.042 | Acc: 98.624% (35347/35840)\n",
            "70 118 Loss: 0.042 | Acc: 98.630% (35854/36352)\n",
            "71 118 Loss: 0.042 | Acc: 98.636% (36361/36864)\n",
            "72 118 Loss: 0.042 | Acc: 98.627% (36863/37376)\n",
            "73 118 Loss: 0.042 | Acc: 98.633% (37370/37888)\n",
            "74 118 Loss: 0.042 | Acc: 98.633% (37875/38400)\n",
            "75 118 Loss: 0.042 | Acc: 98.638% (38382/38912)\n",
            "76 118 Loss: 0.042 | Acc: 98.645% (38890/39424)\n",
            "77 118 Loss: 0.041 | Acc: 98.645% (39395/39936)\n",
            "78 118 Loss: 0.041 | Acc: 98.645% (39900/40448)\n",
            "79 118 Loss: 0.041 | Acc: 98.638% (40402/40960)\n",
            "80 118 Loss: 0.041 | Acc: 98.647% (40911/41472)\n",
            "81 118 Loss: 0.041 | Acc: 98.647% (41416/41984)\n",
            "82 118 Loss: 0.041 | Acc: 98.649% (41922/42496)\n",
            "83 118 Loss: 0.041 | Acc: 98.651% (42428/43008)\n",
            "84 118 Loss: 0.041 | Acc: 98.658% (42936/43520)\n",
            "85 118 Loss: 0.041 | Acc: 98.658% (43441/44032)\n",
            "86 118 Loss: 0.041 | Acc: 98.660% (43947/44544)\n",
            "87 118 Loss: 0.041 | Acc: 98.659% (44452/45056)\n",
            "88 118 Loss: 0.041 | Acc: 98.659% (44957/45568)\n",
            "89 118 Loss: 0.041 | Acc: 98.668% (45466/46080)\n",
            "90 118 Loss: 0.041 | Acc: 98.669% (45972/46592)\n",
            "91 118 Loss: 0.041 | Acc: 98.673% (46479/47104)\n",
            "92 118 Loss: 0.041 | Acc: 98.677% (46986/47616)\n",
            "93 118 Loss: 0.041 | Acc: 98.679% (47492/48128)\n",
            "94 118 Loss: 0.041 | Acc: 98.684% (48000/48640)\n",
            "95 118 Loss: 0.041 | Acc: 98.686% (48506/49152)\n",
            "96 118 Loss: 0.041 | Acc: 98.681% (49009/49664)\n",
            "97 118 Loss: 0.041 | Acc: 98.681% (49514/50176)\n",
            "98 118 Loss: 0.041 | Acc: 98.676% (50017/50688)\n",
            "99 118 Loss: 0.041 | Acc: 98.678% (50523/51200)\n",
            "100 118 Loss: 0.041 | Acc: 98.683% (51031/51712)\n",
            "101 118 Loss: 0.041 | Acc: 98.686% (51538/52224)\n",
            "102 118 Loss: 0.041 | Acc: 98.686% (52043/52736)\n",
            "103 118 Loss: 0.041 | Acc: 98.684% (52547/53248)\n",
            "104 118 Loss: 0.041 | Acc: 98.681% (53051/53760)\n",
            "105 118 Loss: 0.041 | Acc: 98.688% (53560/54272)\n",
            "106 118 Loss: 0.041 | Acc: 98.684% (54063/54784)\n",
            "107 118 Loss: 0.041 | Acc: 98.669% (54560/55296)\n",
            "108 118 Loss: 0.041 | Acc: 98.676% (55069/55808)\n",
            "109 118 Loss: 0.041 | Acc: 98.668% (55570/56320)\n",
            "110 118 Loss: 0.041 | Acc: 98.670% (56076/56832)\n",
            "111 118 Loss: 0.041 | Acc: 98.671% (56582/57344)\n",
            "112 118 Loss: 0.041 | Acc: 98.676% (57090/57856)\n",
            "113 118 Loss: 0.041 | Acc: 98.679% (57597/58368)\n",
            "114 118 Loss: 0.041 | Acc: 98.677% (58101/58880)\n",
            "115 118 Loss: 0.041 | Acc: 98.680% (58608/59392)\n",
            "116 118 Loss: 0.041 | Acc: 98.685% (59116/59904)\n",
            "117 118 Loss: 0.040 | Acc: 98.687% (59212/60000)\n",
            "\n",
            "Epoch: 8\n",
            "Time elapsed: 14.09 min\n",
            "0 118 Loss: 0.047 | Acc: 99.023% (507/512)\n",
            "1 118 Loss: 0.047 | Acc: 98.828% (1012/1024)\n",
            "2 118 Loss: 0.045 | Acc: 98.698% (1516/1536)\n",
            "3 118 Loss: 0.045 | Acc: 98.682% (2021/2048)\n",
            "4 118 Loss: 0.045 | Acc: 98.711% (2527/2560)\n",
            "5 118 Loss: 0.041 | Acc: 98.861% (3037/3072)\n",
            "6 118 Loss: 0.039 | Acc: 98.884% (3544/3584)\n",
            "7 118 Loss: 0.039 | Acc: 98.853% (4049/4096)\n",
            "8 118 Loss: 0.038 | Acc: 98.850% (4555/4608)\n",
            "9 118 Loss: 0.038 | Acc: 98.828% (5060/5120)\n",
            "10 118 Loss: 0.042 | Acc: 98.739% (5561/5632)\n",
            "11 118 Loss: 0.040 | Acc: 98.779% (6069/6144)\n",
            "12 118 Loss: 0.043 | Acc: 98.723% (6571/6656)\n",
            "13 118 Loss: 0.041 | Acc: 98.772% (7080/7168)\n",
            "14 118 Loss: 0.040 | Acc: 98.802% (7588/7680)\n",
            "15 118 Loss: 0.041 | Acc: 98.779% (8092/8192)\n",
            "16 118 Loss: 0.042 | Acc: 98.667% (8588/8704)\n",
            "17 118 Loss: 0.042 | Acc: 98.676% (9094/9216)\n",
            "18 118 Loss: 0.041 | Acc: 98.725% (9604/9728)\n",
            "19 118 Loss: 0.041 | Acc: 98.711% (10108/10240)\n",
            "20 118 Loss: 0.040 | Acc: 98.735% (10616/10752)\n",
            "21 118 Loss: 0.040 | Acc: 98.730% (11121/11264)\n",
            "22 118 Loss: 0.040 | Acc: 98.675% (11620/11776)\n",
            "23 118 Loss: 0.040 | Acc: 98.698% (12128/12288)\n",
            "24 118 Loss: 0.039 | Acc: 98.680% (12631/12800)\n",
            "25 118 Loss: 0.039 | Acc: 98.700% (13139/13312)\n",
            "26 118 Loss: 0.039 | Acc: 98.712% (13646/13824)\n",
            "27 118 Loss: 0.039 | Acc: 98.710% (14151/14336)\n",
            "28 118 Loss: 0.038 | Acc: 98.754% (14663/14848)\n",
            "29 118 Loss: 0.039 | Acc: 98.737% (15166/15360)\n",
            "30 118 Loss: 0.039 | Acc: 98.727% (15670/15872)\n",
            "31 118 Loss: 0.038 | Acc: 98.761% (16181/16384)\n",
            "32 118 Loss: 0.039 | Acc: 98.733% (16682/16896)\n",
            "33 118 Loss: 0.039 | Acc: 98.736% (17188/17408)\n",
            "34 118 Loss: 0.039 | Acc: 98.733% (17693/17920)\n",
            "35 118 Loss: 0.039 | Acc: 98.730% (18198/18432)\n",
            "36 118 Loss: 0.039 | Acc: 98.738% (18705/18944)\n",
            "37 118 Loss: 0.039 | Acc: 98.751% (19213/19456)\n",
            "38 118 Loss: 0.038 | Acc: 98.758% (19720/19968)\n",
            "39 118 Loss: 0.038 | Acc: 98.765% (20227/20480)\n",
            "40 118 Loss: 0.038 | Acc: 98.776% (20735/20992)\n",
            "41 118 Loss: 0.038 | Acc: 98.786% (21243/21504)\n",
            "42 118 Loss: 0.038 | Acc: 98.796% (21751/22016)\n",
            "43 118 Loss: 0.038 | Acc: 98.806% (22259/22528)\n",
            "44 118 Loss: 0.037 | Acc: 98.806% (22765/23040)\n",
            "45 118 Loss: 0.037 | Acc: 98.811% (23272/23552)\n",
            "46 118 Loss: 0.037 | Acc: 98.807% (23777/24064)\n",
            "47 118 Loss: 0.037 | Acc: 98.787% (24278/24576)\n",
            "48 118 Loss: 0.037 | Acc: 98.788% (24784/25088)\n",
            "49 118 Loss: 0.037 | Acc: 98.801% (25293/25600)\n",
            "50 118 Loss: 0.037 | Acc: 98.801% (25799/26112)\n",
            "51 118 Loss: 0.037 | Acc: 98.794% (26303/26624)\n",
            "52 118 Loss: 0.037 | Acc: 98.802% (26811/27136)\n",
            "53 118 Loss: 0.037 | Acc: 98.799% (27316/27648)\n",
            "54 118 Loss: 0.037 | Acc: 98.800% (27822/28160)\n",
            "55 118 Loss: 0.037 | Acc: 98.793% (28326/28672)\n",
            "56 118 Loss: 0.037 | Acc: 98.794% (28832/29184)\n",
            "57 118 Loss: 0.037 | Acc: 98.798% (29339/29696)\n",
            "58 118 Loss: 0.037 | Acc: 98.808% (29848/30208)\n",
            "59 118 Loss: 0.036 | Acc: 98.812% (30355/30720)\n",
            "60 118 Loss: 0.037 | Acc: 98.803% (30858/31232)\n",
            "61 118 Loss: 0.037 | Acc: 98.809% (31366/31744)\n",
            "62 118 Loss: 0.036 | Acc: 98.816% (31874/32256)\n",
            "63 118 Loss: 0.036 | Acc: 98.819% (32381/32768)\n",
            "64 118 Loss: 0.036 | Acc: 98.816% (32886/33280)\n",
            "65 118 Loss: 0.036 | Acc: 98.825% (33395/33792)\n",
            "66 118 Loss: 0.036 | Acc: 98.825% (33901/34304)\n",
            "67 118 Loss: 0.036 | Acc: 98.828% (34408/34816)\n",
            "68 118 Loss: 0.036 | Acc: 98.820% (34911/35328)\n",
            "69 118 Loss: 0.036 | Acc: 98.825% (35419/35840)\n",
            "70 118 Loss: 0.036 | Acc: 98.828% (35926/36352)\n",
            "71 118 Loss: 0.037 | Acc: 98.823% (36430/36864)\n",
            "72 118 Loss: 0.037 | Acc: 98.831% (36939/37376)\n",
            "73 118 Loss: 0.037 | Acc: 98.825% (37443/37888)\n",
            "74 118 Loss: 0.037 | Acc: 98.823% (37948/38400)\n",
            "75 118 Loss: 0.036 | Acc: 98.831% (38457/38912)\n",
            "76 118 Loss: 0.036 | Acc: 98.836% (38965/39424)\n",
            "77 118 Loss: 0.036 | Acc: 98.836% (39471/39936)\n",
            "78 118 Loss: 0.036 | Acc: 98.840% (39979/40448)\n",
            "79 118 Loss: 0.036 | Acc: 98.840% (40485/40960)\n",
            "80 118 Loss: 0.036 | Acc: 98.838% (40990/41472)\n",
            "81 118 Loss: 0.036 | Acc: 98.847% (41500/41984)\n",
            "82 118 Loss: 0.036 | Acc: 98.852% (42008/42496)\n",
            "83 118 Loss: 0.036 | Acc: 98.851% (42514/43008)\n",
            "84 118 Loss: 0.036 | Acc: 98.849% (43019/43520)\n",
            "85 118 Loss: 0.036 | Acc: 98.855% (43528/44032)\n",
            "86 118 Loss: 0.036 | Acc: 98.853% (44033/44544)\n",
            "87 118 Loss: 0.036 | Acc: 98.859% (44542/45056)\n",
            "88 118 Loss: 0.036 | Acc: 98.861% (45049/45568)\n",
            "89 118 Loss: 0.036 | Acc: 98.861% (45555/46080)\n",
            "90 118 Loss: 0.036 | Acc: 98.862% (46062/46592)\n",
            "91 118 Loss: 0.036 | Acc: 98.862% (46568/47104)\n",
            "92 118 Loss: 0.036 | Acc: 98.855% (47071/47616)\n",
            "93 118 Loss: 0.036 | Acc: 98.853% (47576/48128)\n",
            "94 118 Loss: 0.036 | Acc: 98.855% (48083/48640)\n",
            "95 118 Loss: 0.036 | Acc: 98.857% (48590/49152)\n",
            "96 118 Loss: 0.036 | Acc: 98.860% (49098/49664)\n",
            "97 118 Loss: 0.036 | Acc: 98.860% (49604/50176)\n",
            "98 118 Loss: 0.036 | Acc: 98.858% (50109/50688)\n",
            "99 118 Loss: 0.036 | Acc: 98.852% (50612/51200)\n",
            "100 118 Loss: 0.036 | Acc: 98.851% (51118/51712)\n",
            "101 118 Loss: 0.036 | Acc: 98.847% (51622/52224)\n",
            "102 118 Loss: 0.036 | Acc: 98.843% (52126/52736)\n",
            "103 118 Loss: 0.036 | Acc: 98.841% (52631/53248)\n",
            "104 118 Loss: 0.036 | Acc: 98.847% (53140/53760)\n",
            "105 118 Loss: 0.036 | Acc: 98.847% (53646/54272)\n",
            "106 118 Loss: 0.036 | Acc: 98.848% (54153/54784)\n",
            "107 118 Loss: 0.036 | Acc: 98.850% (54660/55296)\n",
            "108 118 Loss: 0.036 | Acc: 98.851% (55167/55808)\n",
            "109 118 Loss: 0.036 | Acc: 98.849% (55672/56320)\n",
            "110 118 Loss: 0.036 | Acc: 98.855% (56181/56832)\n",
            "111 118 Loss: 0.036 | Acc: 98.846% (56682/57344)\n",
            "112 118 Loss: 0.036 | Acc: 98.847% (57189/57856)\n",
            "113 118 Loss: 0.036 | Acc: 98.854% (57699/58368)\n",
            "114 118 Loss: 0.036 | Acc: 98.852% (58204/58880)\n",
            "115 118 Loss: 0.036 | Acc: 98.855% (58712/59392)\n",
            "116 118 Loss: 0.036 | Acc: 98.851% (59216/59904)\n",
            "117 118 Loss: 0.036 | Acc: 98.852% (59311/60000)\n",
            "\n",
            "Epoch: 9\n",
            "Time elapsed: 15.85 min\n",
            "0 118 Loss: 0.028 | Acc: 98.828% (506/512)\n",
            "1 118 Loss: 0.025 | Acc: 99.023% (1014/1024)\n",
            "2 118 Loss: 0.021 | Acc: 99.284% (1525/1536)\n",
            "3 118 Loss: 0.027 | Acc: 99.023% (2028/2048)\n",
            "4 118 Loss: 0.024 | Acc: 99.141% (2538/2560)\n",
            "5 118 Loss: 0.025 | Acc: 99.154% (3046/3072)\n",
            "6 118 Loss: 0.024 | Acc: 99.191% (3555/3584)\n",
            "7 118 Loss: 0.025 | Acc: 99.170% (4062/4096)\n",
            "8 118 Loss: 0.028 | Acc: 99.110% (4567/4608)\n",
            "9 118 Loss: 0.030 | Acc: 99.004% (5069/5120)\n",
            "10 118 Loss: 0.033 | Acc: 98.935% (5572/5632)\n",
            "11 118 Loss: 0.033 | Acc: 98.910% (6077/6144)\n",
            "12 118 Loss: 0.033 | Acc: 98.933% (6585/6656)\n",
            "13 118 Loss: 0.033 | Acc: 98.926% (7091/7168)\n",
            "14 118 Loss: 0.033 | Acc: 98.932% (7598/7680)\n",
            "15 118 Loss: 0.033 | Acc: 98.914% (8103/8192)\n",
            "16 118 Loss: 0.032 | Acc: 98.932% (8611/8704)\n",
            "17 118 Loss: 0.031 | Acc: 98.980% (9122/9216)\n",
            "18 118 Loss: 0.031 | Acc: 98.982% (9629/9728)\n",
            "19 118 Loss: 0.032 | Acc: 98.965% (10134/10240)\n",
            "20 118 Loss: 0.032 | Acc: 98.940% (10638/10752)\n",
            "21 118 Loss: 0.032 | Acc: 98.970% (11148/11264)\n",
            "22 118 Loss: 0.032 | Acc: 98.964% (11654/11776)\n",
            "23 118 Loss: 0.032 | Acc: 98.950% (12159/12288)\n",
            "24 118 Loss: 0.032 | Acc: 98.961% (12667/12800)\n",
            "25 118 Loss: 0.032 | Acc: 98.948% (13172/13312)\n",
            "26 118 Loss: 0.031 | Acc: 98.980% (13683/13824)\n",
            "27 118 Loss: 0.032 | Acc: 98.982% (14190/14336)\n",
            "28 118 Loss: 0.031 | Acc: 98.990% (14698/14848)\n",
            "29 118 Loss: 0.031 | Acc: 98.997% (15206/15360)\n",
            "30 118 Loss: 0.032 | Acc: 98.967% (15708/15872)\n",
            "31 118 Loss: 0.032 | Acc: 98.981% (16217/16384)\n",
            "32 118 Loss: 0.032 | Acc: 98.976% (16723/16896)\n",
            "33 118 Loss: 0.032 | Acc: 98.983% (17231/17408)\n",
            "34 118 Loss: 0.032 | Acc: 98.984% (17738/17920)\n",
            "35 118 Loss: 0.032 | Acc: 98.991% (18246/18432)\n",
            "36 118 Loss: 0.032 | Acc: 98.997% (18754/18944)\n",
            "37 118 Loss: 0.032 | Acc: 98.993% (19260/19456)\n",
            "38 118 Loss: 0.032 | Acc: 98.978% (19764/19968)\n",
            "39 118 Loss: 0.033 | Acc: 98.989% (20273/20480)\n",
            "40 118 Loss: 0.032 | Acc: 98.995% (20781/20992)\n",
            "41 118 Loss: 0.032 | Acc: 99.005% (21290/21504)\n",
            "42 118 Loss: 0.032 | Acc: 99.005% (21797/22016)\n",
            "43 118 Loss: 0.033 | Acc: 98.979% (22298/22528)\n",
            "44 118 Loss: 0.032 | Acc: 98.989% (22807/23040)\n",
            "45 118 Loss: 0.032 | Acc: 98.985% (23313/23552)\n",
            "46 118 Loss: 0.033 | Acc: 98.969% (23816/24064)\n",
            "47 118 Loss: 0.032 | Acc: 98.979% (24325/24576)\n",
            "48 118 Loss: 0.032 | Acc: 98.968% (24829/25088)\n",
            "49 118 Loss: 0.032 | Acc: 98.957% (25333/25600)\n",
            "50 118 Loss: 0.032 | Acc: 98.962% (25841/26112)\n",
            "51 118 Loss: 0.032 | Acc: 98.960% (26347/26624)\n",
            "52 118 Loss: 0.033 | Acc: 98.953% (26852/27136)\n",
            "53 118 Loss: 0.033 | Acc: 98.958% (27360/27648)\n",
            "54 118 Loss: 0.033 | Acc: 98.942% (27862/28160)\n",
            "55 118 Loss: 0.033 | Acc: 98.940% (28368/28672)\n",
            "56 118 Loss: 0.033 | Acc: 98.931% (28872/29184)\n",
            "57 118 Loss: 0.033 | Acc: 98.936% (29380/29696)\n",
            "58 118 Loss: 0.033 | Acc: 98.947% (29890/30208)\n",
            "59 118 Loss: 0.033 | Acc: 98.955% (30399/30720)\n",
            "60 118 Loss: 0.032 | Acc: 98.963% (30908/31232)\n",
            "61 118 Loss: 0.033 | Acc: 98.957% (31413/31744)\n",
            "62 118 Loss: 0.032 | Acc: 98.961% (31921/32256)\n",
            "63 118 Loss: 0.032 | Acc: 98.969% (32430/32768)\n",
            "64 118 Loss: 0.032 | Acc: 98.975% (32939/33280)\n",
            "65 118 Loss: 0.032 | Acc: 98.970% (33444/33792)\n",
            "66 118 Loss: 0.032 | Acc: 98.971% (33951/34304)\n",
            "67 118 Loss: 0.033 | Acc: 98.960% (34454/34816)\n",
            "68 118 Loss: 0.033 | Acc: 98.950% (34957/35328)\n",
            "69 118 Loss: 0.033 | Acc: 98.954% (35465/35840)\n",
            "70 118 Loss: 0.032 | Acc: 98.955% (35972/36352)\n",
            "71 118 Loss: 0.032 | Acc: 98.953% (36478/36864)\n",
            "72 118 Loss: 0.032 | Acc: 98.962% (36988/37376)\n",
            "73 118 Loss: 0.032 | Acc: 98.963% (37495/37888)\n",
            "74 118 Loss: 0.032 | Acc: 98.969% (38004/38400)\n",
            "75 118 Loss: 0.032 | Acc: 98.964% (38509/38912)\n",
            "76 118 Loss: 0.032 | Acc: 98.970% (39018/39424)\n",
            "77 118 Loss: 0.033 | Acc: 98.968% (39524/39936)\n",
            "78 118 Loss: 0.032 | Acc: 98.974% (40033/40448)\n",
            "79 118 Loss: 0.033 | Acc: 98.972% (40539/40960)\n",
            "80 118 Loss: 0.032 | Acc: 98.982% (41050/41472)\n",
            "81 118 Loss: 0.032 | Acc: 98.978% (41555/41984)\n",
            "82 118 Loss: 0.032 | Acc: 98.974% (42060/42496)\n",
            "83 118 Loss: 0.032 | Acc: 98.979% (42569/43008)\n",
            "84 118 Loss: 0.033 | Acc: 98.968% (43071/43520)\n",
            "85 118 Loss: 0.033 | Acc: 98.971% (43579/44032)\n",
            "86 118 Loss: 0.033 | Acc: 98.970% (44085/44544)\n",
            "87 118 Loss: 0.033 | Acc: 98.968% (44591/45056)\n",
            "88 118 Loss: 0.033 | Acc: 98.975% (45101/45568)\n",
            "89 118 Loss: 0.032 | Acc: 98.971% (45606/46080)\n",
            "90 118 Loss: 0.032 | Acc: 98.970% (46112/46592)\n",
            "91 118 Loss: 0.032 | Acc: 98.975% (46621/47104)\n",
            "92 118 Loss: 0.032 | Acc: 98.971% (47126/47616)\n",
            "93 118 Loss: 0.032 | Acc: 98.974% (47634/48128)\n",
            "94 118 Loss: 0.032 | Acc: 98.972% (48140/48640)\n",
            "95 118 Loss: 0.032 | Acc: 98.969% (48645/49152)\n",
            "96 118 Loss: 0.032 | Acc: 98.973% (49154/49664)\n",
            "97 118 Loss: 0.032 | Acc: 98.978% (49663/50176)\n",
            "98 118 Loss: 0.032 | Acc: 98.978% (50170/50688)\n",
            "99 118 Loss: 0.032 | Acc: 98.975% (50675/51200)\n",
            "100 118 Loss: 0.032 | Acc: 98.971% (51180/51712)\n",
            "101 118 Loss: 0.032 | Acc: 98.964% (51683/52224)\n",
            "102 118 Loss: 0.032 | Acc: 98.961% (52188/52736)\n",
            "103 118 Loss: 0.032 | Acc: 98.958% (52693/53248)\n",
            "104 118 Loss: 0.033 | Acc: 98.953% (53197/53760)\n",
            "105 118 Loss: 0.033 | Acc: 98.953% (53704/54272)\n",
            "106 118 Loss: 0.033 | Acc: 98.958% (54213/54784)\n",
            "107 118 Loss: 0.033 | Acc: 98.957% (54719/55296)\n",
            "108 118 Loss: 0.033 | Acc: 98.946% (55220/55808)\n",
            "109 118 Loss: 0.033 | Acc: 98.949% (55728/56320)\n",
            "110 118 Loss: 0.033 | Acc: 98.942% (56231/56832)\n",
            "111 118 Loss: 0.033 | Acc: 98.941% (56737/57344)\n",
            "112 118 Loss: 0.033 | Acc: 98.940% (57243/57856)\n",
            "113 118 Loss: 0.033 | Acc: 98.934% (57746/58368)\n",
            "114 118 Loss: 0.033 | Acc: 98.937% (58254/58880)\n",
            "115 118 Loss: 0.033 | Acc: 98.936% (58760/59392)\n",
            "116 118 Loss: 0.033 | Acc: 98.938% (59268/59904)\n",
            "117 118 Loss: 0.033 | Acc: 98.938% (59363/60000)\n"
          ]
        }
      ]
    }
  ]
}