{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMySwSXjY4Cfafqc47aR/wY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29bb8c57463b44fea12ee092119410c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_984309ac50ff4cbd8b88b4dfd0c7e98b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6318382e0faf4665982bff1976482d4c",
              "IPY_MODEL_450a6ea2c9254ee58158f311cf3588bc",
              "IPY_MODEL_7ad6bccf3ada4612a4e0c6a3f4c3a5b2"
            ]
          }
        },
        "984309ac50ff4cbd8b88b4dfd0c7e98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6318382e0faf4665982bff1976482d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71ec620da2f64cb6ba67127947515bc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2007232ca79144d78c2998a35def2424"
          }
        },
        "450a6ea2c9254ee58158f311cf3588bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_203ff9882cc843db86ed278dc64570cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9dd276310504e6095c458cc536d373a"
          }
        },
        "7ad6bccf3ada4612a4e0c6a3f4c3a5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3978ff0a7adf4dcb8b044b2a3b65fa16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:11&lt;00:00, 16656493.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fee259ac25f1413d8cd978e27f2dec17"
          }
        },
        "71ec620da2f64cb6ba67127947515bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2007232ca79144d78c2998a35def2424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "203ff9882cc843db86ed278dc64570cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9dd276310504e6095c458cc536d373a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3978ff0a7adf4dcb8b044b2a3b65fa16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fee259ac25f1413d8cd978e27f2dec17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_cifarAdam512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox_FE_xeDz6F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svakog osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1, za cifar 3\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQCkD03W_sC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29bb8c57463b44fea12ee092119410c3",
            "984309ac50ff4cbd8b88b4dfd0c7e98b",
            "6318382e0faf4665982bff1976482d4c",
            "450a6ea2c9254ee58158f311cf3588bc",
            "7ad6bccf3ada4612a4e0c6a3f4c3a5b2",
            "71ec620da2f64cb6ba67127947515bc5",
            "2007232ca79144d78c2998a35def2424",
            "203ff9882cc843db86ed278dc64570cd",
            "f9dd276310504e6095c458cc536d373a",
            "3978ff0a7adf4dcb8b044b2a3b65fa16",
            "fee259ac25f1413d8cd978e27f2dec17"
          ]
        },
        "outputId": "f7a9ab35-85a3-4df7-ea1f-fcace7305a79"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "best_acc = 0  \n",
        "start_epoch = 0  \n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    #transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    #transforms.Normalize((0.1307,), (0.3081,)), ovo je za mnist\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "#trainset = torchvision.datasets.MNIST(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=512, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "          'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#classes = ('0', '1', '2', '3', '4',\n",
        "#           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "net = net.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
        "                      #momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "start_time = time.time()\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "     \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "      train(epoch)\n",
        "      scheduler.step()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29bb8c57463b44fea12ee092119410c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "Time elapsed: 0.00 min\n",
            "0 98 Loss: 2.390 | Acc: 11.133% (57/512)\n",
            "1 98 Loss: 2.430 | Acc: 14.746% (151/1024)\n",
            "2 98 Loss: 2.337 | Acc: 16.211% (249/1536)\n",
            "3 98 Loss: 2.247 | Acc: 18.945% (388/2048)\n",
            "4 98 Loss: 2.189 | Acc: 20.430% (523/2560)\n",
            "5 98 Loss: 2.144 | Acc: 21.647% (665/3072)\n",
            "6 98 Loss: 2.092 | Acc: 23.214% (832/3584)\n",
            "7 98 Loss: 2.047 | Acc: 24.585% (1007/4096)\n",
            "8 98 Loss: 2.016 | Acc: 25.716% (1185/4608)\n",
            "9 98 Loss: 1.998 | Acc: 26.074% (1335/5120)\n",
            "10 98 Loss: 1.981 | Acc: 26.669% (1502/5632)\n",
            "11 98 Loss: 1.958 | Acc: 27.262% (1675/6144)\n",
            "12 98 Loss: 1.938 | Acc: 27.734% (1846/6656)\n",
            "13 98 Loss: 1.918 | Acc: 28.265% (2026/7168)\n",
            "14 98 Loss: 1.904 | Acc: 28.971% (2225/7680)\n",
            "15 98 Loss: 1.891 | Acc: 29.675% (2431/8192)\n",
            "16 98 Loss: 1.874 | Acc: 30.331% (2640/8704)\n",
            "17 98 Loss: 1.859 | Acc: 30.892% (2847/9216)\n",
            "18 98 Loss: 1.847 | Acc: 31.384% (3053/9728)\n",
            "19 98 Loss: 1.831 | Acc: 31.865% (3263/10240)\n",
            "20 98 Loss: 1.822 | Acc: 32.180% (3460/10752)\n",
            "21 98 Loss: 1.809 | Acc: 32.599% (3672/11264)\n",
            "22 98 Loss: 1.800 | Acc: 32.931% (3878/11776)\n",
            "23 98 Loss: 1.791 | Acc: 33.154% (4074/12288)\n",
            "24 98 Loss: 1.782 | Acc: 33.484% (4286/12800)\n",
            "25 98 Loss: 1.772 | Acc: 33.827% (4503/13312)\n",
            "26 98 Loss: 1.761 | Acc: 34.049% (4707/13824)\n",
            "27 98 Loss: 1.751 | Acc: 34.501% (4946/14336)\n",
            "28 98 Loss: 1.742 | Acc: 34.853% (5175/14848)\n",
            "29 98 Loss: 1.734 | Acc: 35.202% (5407/15360)\n",
            "30 98 Loss: 1.724 | Acc: 35.559% (5644/15872)\n",
            "31 98 Loss: 1.718 | Acc: 35.785% (5863/16384)\n",
            "32 98 Loss: 1.711 | Acc: 36.127% (6104/16896)\n",
            "33 98 Loss: 1.705 | Acc: 36.317% (6322/17408)\n",
            "34 98 Loss: 1.698 | Acc: 36.607% (6560/17920)\n",
            "35 98 Loss: 1.689 | Acc: 36.919% (6805/18432)\n",
            "36 98 Loss: 1.682 | Acc: 37.189% (7045/18944)\n",
            "37 98 Loss: 1.674 | Acc: 37.490% (7294/19456)\n",
            "38 98 Loss: 1.665 | Acc: 37.861% (7560/19968)\n",
            "39 98 Loss: 1.659 | Acc: 38.057% (7794/20480)\n",
            "40 98 Loss: 1.653 | Acc: 38.310% (8042/20992)\n",
            "41 98 Loss: 1.647 | Acc: 38.509% (8281/21504)\n",
            "42 98 Loss: 1.643 | Acc: 38.604% (8499/22016)\n",
            "43 98 Loss: 1.635 | Acc: 38.991% (8784/22528)\n",
            "44 98 Loss: 1.630 | Acc: 39.197% (9031/23040)\n",
            "45 98 Loss: 1.624 | Acc: 39.428% (9286/23552)\n",
            "46 98 Loss: 1.617 | Acc: 39.698% (9553/24064)\n",
            "47 98 Loss: 1.613 | Acc: 39.832% (9789/24576)\n",
            "48 98 Loss: 1.606 | Acc: 40.103% (10061/25088)\n",
            "49 98 Loss: 1.601 | Acc: 40.309% (10319/25600)\n",
            "50 98 Loss: 1.595 | Acc: 40.552% (10589/26112)\n",
            "51 98 Loss: 1.590 | Acc: 40.741% (10847/26624)\n",
            "52 98 Loss: 1.585 | Acc: 40.960% (11115/27136)\n",
            "53 98 Loss: 1.579 | Acc: 41.229% (11399/27648)\n",
            "54 98 Loss: 1.574 | Acc: 41.445% (11671/28160)\n",
            "55 98 Loss: 1.570 | Acc: 41.640% (11939/28672)\n",
            "56 98 Loss: 1.565 | Acc: 41.879% (12222/29184)\n",
            "57 98 Loss: 1.560 | Acc: 42.043% (12485/29696)\n",
            "58 98 Loss: 1.555 | Acc: 42.211% (12751/30208)\n",
            "59 98 Loss: 1.552 | Acc: 42.396% (13024/30720)\n",
            "60 98 Loss: 1.548 | Acc: 42.594% (13303/31232)\n",
            "61 98 Loss: 1.543 | Acc: 42.811% (13590/31744)\n",
            "62 98 Loss: 1.539 | Acc: 42.975% (13862/32256)\n",
            "63 98 Loss: 1.535 | Acc: 43.130% (14133/32768)\n",
            "64 98 Loss: 1.531 | Acc: 43.275% (14402/33280)\n",
            "65 98 Loss: 1.526 | Acc: 43.451% (14683/33792)\n",
            "66 98 Loss: 1.521 | Acc: 43.651% (14974/34304)\n",
            "67 98 Loss: 1.516 | Acc: 43.825% (15258/34816)\n",
            "68 98 Loss: 1.513 | Acc: 43.971% (15534/35328)\n",
            "69 98 Loss: 1.508 | Acc: 44.149% (15823/35840)\n",
            "70 98 Loss: 1.503 | Acc: 44.369% (16129/36352)\n",
            "71 98 Loss: 1.500 | Acc: 44.507% (16407/36864)\n",
            "72 98 Loss: 1.496 | Acc: 44.668% (16695/37376)\n",
            "73 98 Loss: 1.492 | Acc: 44.856% (16995/37888)\n",
            "74 98 Loss: 1.487 | Acc: 45.000% (17280/38400)\n",
            "75 98 Loss: 1.483 | Acc: 45.174% (17578/38912)\n",
            "76 98 Loss: 1.479 | Acc: 45.305% (17861/39424)\n",
            "77 98 Loss: 1.475 | Acc: 45.480% (18163/39936)\n",
            "78 98 Loss: 1.472 | Acc: 45.636% (18459/40448)\n",
            "79 98 Loss: 1.468 | Acc: 45.740% (18735/40960)\n",
            "80 98 Loss: 1.464 | Acc: 45.884% (19029/41472)\n",
            "81 98 Loss: 1.460 | Acc: 46.022% (19322/41984)\n",
            "82 98 Loss: 1.456 | Acc: 46.186% (19627/42496)\n",
            "83 98 Loss: 1.453 | Acc: 46.261% (19896/43008)\n",
            "84 98 Loss: 1.449 | Acc: 46.427% (20205/43520)\n",
            "85 98 Loss: 1.447 | Acc: 46.525% (20486/44032)\n",
            "86 98 Loss: 1.443 | Acc: 46.664% (20786/44544)\n",
            "87 98 Loss: 1.440 | Acc: 46.822% (21096/45056)\n",
            "88 98 Loss: 1.435 | Acc: 47.004% (21419/45568)\n",
            "89 98 Loss: 1.432 | Acc: 47.146% (21725/46080)\n",
            "90 98 Loss: 1.428 | Acc: 47.311% (22043/46592)\n",
            "91 98 Loss: 1.425 | Acc: 47.448% (22350/47104)\n",
            "92 98 Loss: 1.422 | Acc: 47.568% (22650/47616)\n",
            "93 98 Loss: 1.419 | Acc: 47.700% (22957/48128)\n",
            "94 98 Loss: 1.415 | Acc: 47.835% (23267/48640)\n",
            "95 98 Loss: 1.413 | Acc: 47.902% (23545/49152)\n",
            "96 98 Loss: 1.410 | Acc: 48.033% (23855/49664)\n",
            "97 98 Loss: 1.408 | Acc: 48.096% (24048/50000)\n",
            "\n",
            "Epoch: 1\n",
            "Time elapsed: 2.16 min\n",
            "0 98 Loss: 1.104 | Acc: 60.352% (309/512)\n",
            "1 98 Loss: 1.137 | Acc: 58.594% (600/1024)\n",
            "2 98 Loss: 1.104 | Acc: 59.831% (919/1536)\n",
            "3 98 Loss: 1.094 | Acc: 59.668% (1222/2048)\n",
            "4 98 Loss: 1.091 | Acc: 60.117% (1539/2560)\n",
            "5 98 Loss: 1.096 | Acc: 59.896% (1840/3072)\n",
            "6 98 Loss: 1.090 | Acc: 60.017% (2151/3584)\n",
            "7 98 Loss: 1.090 | Acc: 60.278% (2469/4096)\n",
            "8 98 Loss: 1.084 | Acc: 60.569% (2791/4608)\n",
            "9 98 Loss: 1.084 | Acc: 60.820% (3114/5120)\n",
            "10 98 Loss: 1.090 | Acc: 60.813% (3425/5632)\n",
            "11 98 Loss: 1.090 | Acc: 60.726% (3731/6144)\n",
            "12 98 Loss: 1.086 | Acc: 60.862% (4051/6656)\n",
            "13 98 Loss: 1.084 | Acc: 60.993% (4372/7168)\n",
            "14 98 Loss: 1.087 | Acc: 60.990% (4684/7680)\n",
            "15 98 Loss: 1.086 | Acc: 60.938% (4992/8192)\n",
            "16 98 Loss: 1.085 | Acc: 60.960% (5306/8704)\n",
            "17 98 Loss: 1.082 | Acc: 61.046% (5626/9216)\n",
            "18 98 Loss: 1.083 | Acc: 60.999% (5934/9728)\n",
            "19 98 Loss: 1.082 | Acc: 60.957% (6242/10240)\n",
            "20 98 Loss: 1.084 | Acc: 60.928% (6551/10752)\n",
            "21 98 Loss: 1.082 | Acc: 61.026% (6874/11264)\n",
            "22 98 Loss: 1.081 | Acc: 61.022% (7186/11776)\n",
            "23 98 Loss: 1.080 | Acc: 61.084% (7506/12288)\n",
            "24 98 Loss: 1.077 | Acc: 61.203% (7834/12800)\n",
            "25 98 Loss: 1.073 | Acc: 61.343% (8166/13312)\n",
            "26 98 Loss: 1.071 | Acc: 61.444% (8494/13824)\n",
            "27 98 Loss: 1.068 | Acc: 61.614% (8833/14336)\n",
            "28 98 Loss: 1.065 | Acc: 61.692% (9160/14848)\n",
            "29 98 Loss: 1.064 | Acc: 61.816% (9495/15360)\n",
            "30 98 Loss: 1.060 | Acc: 61.920% (9828/15872)\n",
            "31 98 Loss: 1.058 | Acc: 62.024% (10162/16384)\n",
            "32 98 Loss: 1.057 | Acc: 62.121% (10496/16896)\n",
            "33 98 Loss: 1.054 | Acc: 62.310% (10847/17408)\n",
            "34 98 Loss: 1.054 | Acc: 62.260% (11157/17920)\n",
            "35 98 Loss: 1.054 | Acc: 62.305% (11484/18432)\n",
            "36 98 Loss: 1.052 | Acc: 62.379% (11817/18944)\n",
            "37 98 Loss: 1.048 | Acc: 62.521% (12164/19456)\n",
            "38 98 Loss: 1.048 | Acc: 62.480% (12476/19968)\n",
            "39 98 Loss: 1.046 | Acc: 62.573% (12815/20480)\n",
            "40 98 Loss: 1.044 | Acc: 62.614% (13144/20992)\n",
            "41 98 Loss: 1.039 | Acc: 62.812% (13507/21504)\n",
            "42 98 Loss: 1.036 | Acc: 62.932% (13855/22016)\n",
            "43 98 Loss: 1.035 | Acc: 63.006% (14194/22528)\n",
            "44 98 Loss: 1.034 | Acc: 63.043% (14525/23040)\n",
            "45 98 Loss: 1.033 | Acc: 63.086% (14858/23552)\n",
            "46 98 Loss: 1.033 | Acc: 63.144% (15195/24064)\n",
            "47 98 Loss: 1.031 | Acc: 63.208% (15534/24576)\n",
            "48 98 Loss: 1.030 | Acc: 63.285% (15877/25088)\n",
            "49 98 Loss: 1.030 | Acc: 63.293% (16203/25600)\n",
            "50 98 Loss: 1.027 | Acc: 63.354% (16543/26112)\n",
            "51 98 Loss: 1.025 | Acc: 63.428% (16887/26624)\n",
            "52 98 Loss: 1.023 | Acc: 63.495% (17230/27136)\n",
            "53 98 Loss: 1.024 | Acc: 63.480% (17551/27648)\n",
            "54 98 Loss: 1.021 | Acc: 63.576% (17903/28160)\n",
            "55 98 Loss: 1.020 | Acc: 63.651% (18250/28672)\n",
            "56 98 Loss: 1.018 | Acc: 63.668% (18581/29184)\n",
            "57 98 Loss: 1.017 | Acc: 63.729% (18925/29696)\n",
            "58 98 Loss: 1.014 | Acc: 63.778% (19266/30208)\n",
            "59 98 Loss: 1.013 | Acc: 63.812% (19603/30720)\n",
            "60 98 Loss: 1.013 | Acc: 63.864% (19946/31232)\n",
            "61 98 Loss: 1.011 | Acc: 63.927% (20293/31744)\n",
            "62 98 Loss: 1.010 | Acc: 63.960% (20631/32256)\n",
            "63 98 Loss: 1.010 | Acc: 63.956% (20957/32768)\n",
            "64 98 Loss: 1.008 | Acc: 64.020% (21306/33280)\n",
            "65 98 Loss: 1.007 | Acc: 64.065% (21649/33792)\n",
            "66 98 Loss: 1.005 | Acc: 64.165% (22011/34304)\n",
            "67 98 Loss: 1.004 | Acc: 64.183% (22346/34816)\n",
            "68 98 Loss: 1.004 | Acc: 64.156% (22665/35328)\n",
            "69 98 Loss: 1.002 | Acc: 64.255% (23029/35840)\n",
            "70 98 Loss: 0.999 | Acc: 64.387% (23406/36352)\n",
            "71 98 Loss: 0.997 | Acc: 64.461% (23763/36864)\n",
            "72 98 Loss: 0.996 | Acc: 64.483% (24101/37376)\n",
            "73 98 Loss: 0.996 | Acc: 64.514% (24443/37888)\n",
            "74 98 Loss: 0.995 | Acc: 64.552% (24788/38400)\n",
            "75 98 Loss: 0.994 | Acc: 64.558% (25121/38912)\n",
            "76 98 Loss: 0.993 | Acc: 64.605% (25470/39424)\n",
            "77 98 Loss: 0.991 | Acc: 64.658% (25822/39936)\n",
            "78 98 Loss: 0.990 | Acc: 64.723% (26179/40448)\n",
            "79 98 Loss: 0.988 | Acc: 64.792% (26539/40960)\n",
            "80 98 Loss: 0.987 | Acc: 64.817% (26881/41472)\n",
            "81 98 Loss: 0.985 | Acc: 64.870% (27235/41984)\n",
            "82 98 Loss: 0.984 | Acc: 64.919% (27588/42496)\n",
            "83 98 Loss: 0.982 | Acc: 64.995% (27953/43008)\n",
            "84 98 Loss: 0.981 | Acc: 65.046% (28308/43520)\n",
            "85 98 Loss: 0.980 | Acc: 65.080% (28656/44032)\n",
            "86 98 Loss: 0.979 | Acc: 65.084% (28991/44544)\n",
            "87 98 Loss: 0.978 | Acc: 65.161% (29359/45056)\n",
            "88 98 Loss: 0.976 | Acc: 65.245% (29731/45568)\n",
            "89 98 Loss: 0.975 | Acc: 65.258% (30071/46080)\n",
            "90 98 Loss: 0.975 | Acc: 65.277% (30414/46592)\n",
            "91 98 Loss: 0.973 | Acc: 65.379% (30796/47104)\n",
            "92 98 Loss: 0.971 | Acc: 65.451% (31165/47616)\n",
            "93 98 Loss: 0.970 | Acc: 65.500% (31524/48128)\n",
            "94 98 Loss: 0.969 | Acc: 65.508% (31863/48640)\n",
            "95 98 Loss: 0.968 | Acc: 65.535% (32212/49152)\n",
            "96 98 Loss: 0.967 | Acc: 65.593% (32576/49664)\n",
            "97 98 Loss: 0.966 | Acc: 65.610% (32805/50000)\n",
            "\n",
            "Epoch: 2\n",
            "Time elapsed: 4.31 min\n",
            "0 98 Loss: 0.829 | Acc: 70.508% (361/512)\n",
            "1 98 Loss: 0.809 | Acc: 70.703% (724/1024)\n",
            "2 98 Loss: 0.837 | Acc: 70.182% (1078/1536)\n",
            "3 98 Loss: 0.835 | Acc: 70.361% (1441/2048)\n",
            "4 98 Loss: 0.834 | Acc: 70.312% (1800/2560)\n",
            "5 98 Loss: 0.834 | Acc: 70.508% (2166/3072)\n",
            "6 98 Loss: 0.832 | Acc: 70.703% (2534/3584)\n",
            "7 98 Loss: 0.839 | Acc: 70.728% (2897/4096)\n",
            "8 98 Loss: 0.826 | Acc: 71.224% (3282/4608)\n",
            "9 98 Loss: 0.832 | Acc: 70.977% (3634/5120)\n",
            "10 98 Loss: 0.835 | Acc: 70.916% (3994/5632)\n",
            "11 98 Loss: 0.829 | Acc: 71.175% (4373/6144)\n",
            "12 98 Loss: 0.827 | Acc: 71.079% (4731/6656)\n",
            "13 98 Loss: 0.826 | Acc: 70.968% (5087/7168)\n",
            "14 98 Loss: 0.827 | Acc: 70.990% (5452/7680)\n",
            "15 98 Loss: 0.830 | Acc: 70.605% (5784/8192)\n",
            "16 98 Loss: 0.835 | Acc: 70.393% (6127/8704)\n",
            "17 98 Loss: 0.839 | Acc: 70.280% (6477/9216)\n",
            "18 98 Loss: 0.838 | Acc: 70.354% (6844/9728)\n",
            "19 98 Loss: 0.839 | Acc: 70.273% (7196/10240)\n",
            "20 98 Loss: 0.837 | Acc: 70.322% (7561/10752)\n",
            "21 98 Loss: 0.836 | Acc: 70.348% (7924/11264)\n",
            "22 98 Loss: 0.835 | Acc: 70.448% (8296/11776)\n",
            "23 98 Loss: 0.832 | Acc: 70.500% (8663/12288)\n",
            "24 98 Loss: 0.832 | Acc: 70.383% (9009/12800)\n",
            "25 98 Loss: 0.832 | Acc: 70.433% (9376/13312)\n",
            "26 98 Loss: 0.830 | Acc: 70.566% (9755/13824)\n",
            "27 98 Loss: 0.826 | Acc: 70.682% (10133/14336)\n",
            "28 98 Loss: 0.826 | Acc: 70.669% (10493/14848)\n",
            "29 98 Loss: 0.825 | Acc: 70.736% (10865/15360)\n",
            "30 98 Loss: 0.825 | Acc: 70.709% (11223/15872)\n",
            "31 98 Loss: 0.825 | Acc: 70.728% (11588/16384)\n",
            "32 98 Loss: 0.824 | Acc: 70.756% (11955/16896)\n",
            "33 98 Loss: 0.818 | Acc: 71.008% (12361/17408)\n",
            "34 98 Loss: 0.817 | Acc: 71.027% (12728/17920)\n",
            "35 98 Loss: 0.816 | Acc: 71.148% (13114/18432)\n",
            "36 98 Loss: 0.816 | Acc: 71.231% (13494/18944)\n",
            "37 98 Loss: 0.814 | Acc: 71.294% (13871/19456)\n",
            "38 98 Loss: 0.813 | Acc: 71.329% (14243/19968)\n",
            "39 98 Loss: 0.813 | Acc: 71.318% (14606/20480)\n",
            "40 98 Loss: 0.812 | Acc: 71.361% (14980/20992)\n",
            "41 98 Loss: 0.812 | Acc: 71.350% (15343/21504)\n",
            "42 98 Loss: 0.812 | Acc: 71.344% (15707/22016)\n",
            "43 98 Loss: 0.811 | Acc: 71.333% (16070/22528)\n",
            "44 98 Loss: 0.809 | Acc: 71.415% (16454/23040)\n",
            "45 98 Loss: 0.810 | Acc: 71.429% (16823/23552)\n",
            "46 98 Loss: 0.809 | Acc: 71.459% (17196/24064)\n",
            "47 98 Loss: 0.808 | Acc: 71.484% (17568/24576)\n",
            "48 98 Loss: 0.806 | Acc: 71.600% (17963/25088)\n",
            "49 98 Loss: 0.805 | Acc: 71.637% (18339/25600)\n",
            "50 98 Loss: 0.802 | Acc: 71.726% (18729/26112)\n",
            "51 98 Loss: 0.803 | Acc: 71.706% (19091/26624)\n",
            "52 98 Loss: 0.803 | Acc: 71.735% (19466/27136)\n",
            "53 98 Loss: 0.802 | Acc: 71.785% (19847/27648)\n",
            "54 98 Loss: 0.801 | Acc: 71.783% (20214/28160)\n",
            "55 98 Loss: 0.800 | Acc: 71.802% (20587/28672)\n",
            "56 98 Loss: 0.798 | Acc: 71.865% (20973/29184)\n",
            "57 98 Loss: 0.798 | Acc: 71.875% (21344/29696)\n",
            "58 98 Loss: 0.796 | Acc: 71.978% (21743/30208)\n",
            "59 98 Loss: 0.795 | Acc: 72.015% (22123/30720)\n",
            "60 98 Loss: 0.795 | Acc: 72.000% (22487/31232)\n",
            "61 98 Loss: 0.793 | Acc: 72.042% (22869/31744)\n",
            "62 98 Loss: 0.794 | Acc: 72.024% (23232/32256)\n",
            "63 98 Loss: 0.793 | Acc: 72.076% (23618/32768)\n",
            "64 98 Loss: 0.792 | Acc: 72.112% (23999/33280)\n",
            "65 98 Loss: 0.790 | Acc: 72.171% (24388/33792)\n",
            "66 98 Loss: 0.789 | Acc: 72.158% (24753/34304)\n",
            "67 98 Loss: 0.789 | Acc: 72.151% (25120/34816)\n",
            "68 98 Loss: 0.788 | Acc: 72.181% (25500/35328)\n",
            "69 98 Loss: 0.787 | Acc: 72.179% (25869/35840)\n",
            "70 98 Loss: 0.787 | Acc: 72.186% (26241/36352)\n",
            "71 98 Loss: 0.786 | Acc: 72.217% (26622/36864)\n",
            "72 98 Loss: 0.785 | Acc: 72.252% (27005/37376)\n",
            "73 98 Loss: 0.784 | Acc: 72.274% (27383/37888)\n",
            "74 98 Loss: 0.783 | Acc: 72.359% (27786/38400)\n",
            "75 98 Loss: 0.781 | Acc: 72.379% (28164/38912)\n",
            "76 98 Loss: 0.780 | Acc: 72.413% (28548/39424)\n",
            "77 98 Loss: 0.780 | Acc: 72.418% (28921/39936)\n",
            "78 98 Loss: 0.779 | Acc: 72.486% (29319/40448)\n",
            "79 98 Loss: 0.778 | Acc: 72.542% (29713/40960)\n",
            "80 98 Loss: 0.776 | Acc: 72.603% (30110/41472)\n",
            "81 98 Loss: 0.776 | Acc: 72.611% (30485/41984)\n",
            "82 98 Loss: 0.776 | Acc: 72.642% (30870/42496)\n",
            "83 98 Loss: 0.776 | Acc: 72.638% (31240/43008)\n",
            "84 98 Loss: 0.776 | Acc: 72.668% (31625/43520)\n",
            "85 98 Loss: 0.775 | Acc: 72.695% (32009/44032)\n",
            "86 98 Loss: 0.774 | Acc: 72.746% (32404/44544)\n",
            "87 98 Loss: 0.774 | Acc: 72.725% (32767/45056)\n",
            "88 98 Loss: 0.773 | Acc: 72.773% (33161/45568)\n",
            "89 98 Loss: 0.773 | Acc: 72.786% (33540/46080)\n",
            "90 98 Loss: 0.772 | Acc: 72.841% (33938/46592)\n",
            "91 98 Loss: 0.771 | Acc: 72.881% (34330/47104)\n",
            "92 98 Loss: 0.770 | Acc: 72.929% (34726/47616)\n",
            "93 98 Loss: 0.769 | Acc: 72.949% (35109/48128)\n",
            "94 98 Loss: 0.769 | Acc: 72.958% (35487/48640)\n",
            "95 98 Loss: 0.768 | Acc: 72.974% (35868/49152)\n",
            "96 98 Loss: 0.767 | Acc: 73.035% (36272/49664)\n",
            "97 98 Loss: 0.766 | Acc: 73.066% (36533/50000)\n",
            "\n",
            "Epoch: 3\n",
            "Time elapsed: 6.44 min\n",
            "0 98 Loss: 0.599 | Acc: 78.516% (402/512)\n",
            "1 98 Loss: 0.627 | Acc: 77.734% (796/1024)\n",
            "2 98 Loss: 0.666 | Acc: 75.911% (1166/1536)\n",
            "3 98 Loss: 0.686 | Acc: 75.635% (1549/2048)\n",
            "4 98 Loss: 0.682 | Acc: 75.859% (1942/2560)\n",
            "5 98 Loss: 0.680 | Acc: 75.977% (2334/3072)\n",
            "6 98 Loss: 0.673 | Acc: 76.060% (2726/3584)\n",
            "7 98 Loss: 0.669 | Acc: 76.245% (3123/4096)\n",
            "8 98 Loss: 0.672 | Acc: 76.128% (3508/4608)\n",
            "9 98 Loss: 0.671 | Acc: 76.211% (3902/5120)\n",
            "10 98 Loss: 0.668 | Acc: 76.314% (4298/5632)\n",
            "11 98 Loss: 0.669 | Acc: 76.497% (4700/6144)\n",
            "12 98 Loss: 0.664 | Acc: 76.788% (5111/6656)\n",
            "13 98 Loss: 0.661 | Acc: 77.009% (5520/7168)\n",
            "14 98 Loss: 0.663 | Acc: 77.031% (5916/7680)\n",
            "15 98 Loss: 0.659 | Acc: 77.173% (6322/8192)\n",
            "16 98 Loss: 0.659 | Acc: 77.011% (6703/8704)\n",
            "17 98 Loss: 0.658 | Acc: 77.062% (7102/9216)\n",
            "18 98 Loss: 0.660 | Acc: 77.015% (7492/9728)\n",
            "19 98 Loss: 0.663 | Acc: 76.865% (7871/10240)\n",
            "20 98 Loss: 0.665 | Acc: 76.814% (8259/10752)\n",
            "21 98 Loss: 0.668 | Acc: 76.616% (8630/11264)\n",
            "22 98 Loss: 0.666 | Acc: 76.707% (9033/11776)\n",
            "23 98 Loss: 0.663 | Acc: 76.831% (9441/12288)\n",
            "24 98 Loss: 0.662 | Acc: 76.875% (9840/12800)\n",
            "25 98 Loss: 0.660 | Acc: 76.878% (10234/13312)\n",
            "26 98 Loss: 0.662 | Acc: 76.888% (10629/13824)\n",
            "27 98 Loss: 0.663 | Acc: 76.828% (11014/14336)\n",
            "28 98 Loss: 0.663 | Acc: 76.845% (11410/14848)\n",
            "29 98 Loss: 0.663 | Acc: 76.771% (11792/15360)\n",
            "30 98 Loss: 0.662 | Acc: 76.802% (12190/15872)\n",
            "31 98 Loss: 0.662 | Acc: 76.837% (12589/16384)\n",
            "32 98 Loss: 0.663 | Acc: 76.752% (12968/16896)\n",
            "33 98 Loss: 0.661 | Acc: 76.821% (13373/17408)\n",
            "34 98 Loss: 0.659 | Acc: 76.903% (13781/17920)\n",
            "35 98 Loss: 0.660 | Acc: 76.861% (14167/18432)\n",
            "36 98 Loss: 0.659 | Acc: 76.879% (14564/18944)\n",
            "37 98 Loss: 0.659 | Acc: 76.922% (14966/19456)\n",
            "38 98 Loss: 0.658 | Acc: 76.938% (15363/19968)\n",
            "39 98 Loss: 0.658 | Acc: 76.953% (15760/20480)\n",
            "40 98 Loss: 0.658 | Acc: 76.944% (16152/20992)\n",
            "41 98 Loss: 0.657 | Acc: 76.976% (16553/21504)\n",
            "42 98 Loss: 0.656 | Acc: 77.049% (16963/22016)\n",
            "43 98 Loss: 0.655 | Acc: 77.064% (17361/22528)\n",
            "44 98 Loss: 0.655 | Acc: 77.101% (17764/23040)\n",
            "45 98 Loss: 0.653 | Acc: 77.148% (18170/23552)\n",
            "46 98 Loss: 0.652 | Acc: 77.182% (18573/24064)\n",
            "47 98 Loss: 0.653 | Acc: 77.169% (18965/24576)\n",
            "48 98 Loss: 0.653 | Acc: 77.180% (19363/25088)\n",
            "49 98 Loss: 0.653 | Acc: 77.156% (19752/25600)\n",
            "50 98 Loss: 0.653 | Acc: 77.152% (20146/26112)\n",
            "51 98 Loss: 0.653 | Acc: 77.133% (20536/26624)\n",
            "52 98 Loss: 0.652 | Acc: 77.204% (20950/27136)\n",
            "53 98 Loss: 0.651 | Acc: 77.250% (21358/27648)\n",
            "54 98 Loss: 0.650 | Acc: 77.283% (21763/28160)\n",
            "55 98 Loss: 0.651 | Acc: 77.298% (22163/28672)\n",
            "56 98 Loss: 0.651 | Acc: 77.292% (22557/29184)\n",
            "57 98 Loss: 0.651 | Acc: 77.327% (22963/29696)\n",
            "58 98 Loss: 0.651 | Acc: 77.294% (23349/30208)\n",
            "59 98 Loss: 0.652 | Acc: 77.253% (23732/30720)\n",
            "60 98 Loss: 0.652 | Acc: 77.238% (24123/31232)\n",
            "61 98 Loss: 0.652 | Acc: 77.249% (24522/31744)\n",
            "62 98 Loss: 0.651 | Acc: 77.245% (24916/32256)\n",
            "63 98 Loss: 0.651 | Acc: 77.274% (25321/32768)\n",
            "64 98 Loss: 0.650 | Acc: 77.284% (25720/33280)\n",
            "65 98 Loss: 0.649 | Acc: 77.308% (26124/33792)\n",
            "66 98 Loss: 0.650 | Acc: 77.294% (26515/34304)\n",
            "67 98 Loss: 0.649 | Acc: 77.318% (26919/34816)\n",
            "68 98 Loss: 0.648 | Acc: 77.327% (27318/35328)\n",
            "69 98 Loss: 0.648 | Acc: 77.305% (27706/35840)\n",
            "70 98 Loss: 0.649 | Acc: 77.289% (28096/36352)\n",
            "71 98 Loss: 0.650 | Acc: 77.260% (28481/36864)\n",
            "72 98 Loss: 0.649 | Acc: 77.298% (28891/37376)\n",
            "73 98 Loss: 0.648 | Acc: 77.341% (29303/37888)\n",
            "74 98 Loss: 0.647 | Acc: 77.375% (29712/38400)\n",
            "75 98 Loss: 0.647 | Acc: 77.357% (30101/38912)\n",
            "76 98 Loss: 0.648 | Acc: 77.341% (30491/39424)\n",
            "77 98 Loss: 0.648 | Acc: 77.344% (30888/39936)\n",
            "78 98 Loss: 0.647 | Acc: 77.378% (31298/40448)\n",
            "79 98 Loss: 0.646 | Acc: 77.415% (31709/40960)\n",
            "80 98 Loss: 0.645 | Acc: 77.445% (32118/41472)\n",
            "81 98 Loss: 0.645 | Acc: 77.451% (32517/41984)\n",
            "82 98 Loss: 0.644 | Acc: 77.466% (32920/42496)\n",
            "83 98 Loss: 0.643 | Acc: 77.513% (33337/43008)\n",
            "84 98 Loss: 0.641 | Acc: 77.567% (33757/43520)\n",
            "85 98 Loss: 0.641 | Acc: 77.560% (34151/44032)\n",
            "86 98 Loss: 0.641 | Acc: 77.550% (34544/44544)\n",
            "87 98 Loss: 0.641 | Acc: 77.570% (34950/45056)\n",
            "88 98 Loss: 0.640 | Acc: 77.590% (35356/45568)\n",
            "89 98 Loss: 0.639 | Acc: 77.615% (35765/46080)\n",
            "90 98 Loss: 0.639 | Acc: 77.618% (36164/46592)\n",
            "91 98 Loss: 0.639 | Acc: 77.626% (36565/47104)\n",
            "92 98 Loss: 0.638 | Acc: 77.646% (36972/47616)\n",
            "93 98 Loss: 0.637 | Acc: 77.701% (37396/48128)\n",
            "94 98 Loss: 0.636 | Acc: 77.726% (37806/48640)\n",
            "95 98 Loss: 0.635 | Acc: 77.763% (38222/49152)\n",
            "96 98 Loss: 0.636 | Acc: 77.748% (38613/49664)\n",
            "97 98 Loss: 0.637 | Acc: 77.738% (38869/50000)\n",
            "\n",
            "Epoch: 4\n",
            "Time elapsed: 8.59 min\n",
            "0 98 Loss: 0.566 | Acc: 78.906% (404/512)\n",
            "1 98 Loss: 0.579 | Acc: 78.516% (804/1024)\n",
            "2 98 Loss: 0.564 | Acc: 79.297% (1218/1536)\n",
            "3 98 Loss: 0.578 | Acc: 78.809% (1614/2048)\n",
            "4 98 Loss: 0.567 | Acc: 79.297% (2030/2560)\n",
            "5 98 Loss: 0.580 | Acc: 79.297% (2436/3072)\n",
            "6 98 Loss: 0.577 | Acc: 79.381% (2845/3584)\n",
            "7 98 Loss: 0.578 | Acc: 79.321% (3249/4096)\n",
            "8 98 Loss: 0.568 | Acc: 79.883% (3681/4608)\n",
            "9 98 Loss: 0.566 | Acc: 79.883% (4090/5120)\n",
            "10 98 Loss: 0.567 | Acc: 79.794% (4494/5632)\n",
            "11 98 Loss: 0.570 | Acc: 79.736% (4899/6144)\n",
            "12 98 Loss: 0.569 | Acc: 79.778% (5310/6656)\n",
            "13 98 Loss: 0.574 | Acc: 79.618% (5707/7168)\n",
            "14 98 Loss: 0.580 | Acc: 79.583% (6112/7680)\n",
            "15 98 Loss: 0.579 | Acc: 79.529% (6515/8192)\n",
            "16 98 Loss: 0.581 | Acc: 79.458% (6916/8704)\n",
            "17 98 Loss: 0.582 | Acc: 79.395% (7317/9216)\n",
            "18 98 Loss: 0.583 | Acc: 79.348% (7719/9728)\n",
            "19 98 Loss: 0.585 | Acc: 79.316% (8122/10240)\n",
            "20 98 Loss: 0.586 | Acc: 79.297% (8526/10752)\n",
            "21 98 Loss: 0.586 | Acc: 79.332% (8936/11264)\n",
            "22 98 Loss: 0.586 | Acc: 79.356% (9345/11776)\n",
            "23 98 Loss: 0.585 | Acc: 79.403% (9757/12288)\n",
            "24 98 Loss: 0.584 | Acc: 79.477% (10173/12800)\n",
            "25 98 Loss: 0.585 | Acc: 79.417% (10572/13312)\n",
            "26 98 Loss: 0.585 | Acc: 79.391% (10975/13824)\n",
            "27 98 Loss: 0.583 | Acc: 79.492% (11396/14336)\n",
            "28 98 Loss: 0.581 | Acc: 79.546% (11811/14848)\n",
            "29 98 Loss: 0.583 | Acc: 79.486% (12209/15360)\n",
            "30 98 Loss: 0.581 | Acc: 79.543% (12625/15872)\n",
            "31 98 Loss: 0.579 | Acc: 79.620% (13045/16384)\n",
            "32 98 Loss: 0.578 | Acc: 79.640% (13456/16896)\n",
            "33 98 Loss: 0.577 | Acc: 79.659% (13867/17408)\n",
            "34 98 Loss: 0.580 | Acc: 79.475% (14242/17920)\n",
            "35 98 Loss: 0.579 | Acc: 79.541% (14661/18432)\n",
            "36 98 Loss: 0.579 | Acc: 79.540% (15068/18944)\n",
            "37 98 Loss: 0.579 | Acc: 79.564% (15480/19456)\n",
            "38 98 Loss: 0.578 | Acc: 79.587% (15892/19968)\n",
            "39 98 Loss: 0.578 | Acc: 79.614% (16305/20480)\n",
            "40 98 Loss: 0.576 | Acc: 79.673% (16725/20992)\n",
            "41 98 Loss: 0.574 | Acc: 79.706% (17140/21504)\n",
            "42 98 Loss: 0.572 | Acc: 79.787% (17566/22016)\n",
            "43 98 Loss: 0.570 | Acc: 79.812% (17980/22528)\n",
            "44 98 Loss: 0.572 | Acc: 79.770% (18379/23040)\n",
            "45 98 Loss: 0.570 | Acc: 79.836% (18803/23552)\n",
            "46 98 Loss: 0.571 | Acc: 79.866% (19219/24064)\n",
            "47 98 Loss: 0.571 | Acc: 79.834% (19620/24576)\n",
            "48 98 Loss: 0.571 | Acc: 79.871% (20038/25088)\n",
            "49 98 Loss: 0.569 | Acc: 79.938% (20464/25600)\n",
            "50 98 Loss: 0.569 | Acc: 79.956% (20878/26112)\n",
            "51 98 Loss: 0.569 | Acc: 79.984% (21295/26624)\n",
            "52 98 Loss: 0.570 | Acc: 79.945% (21694/27136)\n",
            "53 98 Loss: 0.570 | Acc: 79.999% (22118/27648)\n",
            "54 98 Loss: 0.570 | Acc: 80.018% (22533/28160)\n",
            "55 98 Loss: 0.568 | Acc: 80.103% (22967/28672)\n",
            "56 98 Loss: 0.569 | Acc: 80.095% (23375/29184)\n",
            "57 98 Loss: 0.568 | Acc: 80.129% (23795/29696)\n",
            "58 98 Loss: 0.569 | Acc: 80.085% (24192/30208)\n",
            "59 98 Loss: 0.568 | Acc: 80.140% (24619/30720)\n",
            "60 98 Loss: 0.568 | Acc: 80.142% (25030/31232)\n",
            "61 98 Loss: 0.568 | Acc: 80.166% (25448/31744)\n",
            "62 98 Loss: 0.568 | Acc: 80.143% (25851/32256)\n",
            "63 98 Loss: 0.569 | Acc: 80.099% (26247/32768)\n",
            "64 98 Loss: 0.569 | Acc: 80.108% (26660/33280)\n",
            "65 98 Loss: 0.569 | Acc: 80.134% (27079/33792)\n",
            "66 98 Loss: 0.568 | Acc: 80.163% (27499/34304)\n",
            "67 98 Loss: 0.567 | Acc: 80.182% (27916/34816)\n",
            "68 98 Loss: 0.568 | Acc: 80.172% (28323/35328)\n",
            "69 98 Loss: 0.567 | Acc: 80.212% (28748/35840)\n",
            "70 98 Loss: 0.566 | Acc: 80.284% (29185/36352)\n",
            "71 98 Loss: 0.565 | Acc: 80.322% (29610/36864)\n",
            "72 98 Loss: 0.566 | Acc: 80.295% (30011/37376)\n",
            "73 98 Loss: 0.565 | Acc: 80.334% (30437/37888)\n",
            "74 98 Loss: 0.565 | Acc: 80.344% (30852/38400)\n",
            "75 98 Loss: 0.564 | Acc: 80.384% (31279/38912)\n",
            "76 98 Loss: 0.563 | Acc: 80.403% (31698/39424)\n",
            "77 98 Loss: 0.562 | Acc: 80.429% (32120/39936)\n",
            "78 98 Loss: 0.561 | Acc: 80.461% (32545/40448)\n",
            "79 98 Loss: 0.561 | Acc: 80.454% (32954/40960)\n",
            "80 98 Loss: 0.561 | Acc: 80.481% (33377/41472)\n",
            "81 98 Loss: 0.560 | Acc: 80.497% (33796/41984)\n",
            "82 98 Loss: 0.560 | Acc: 80.516% (34216/42496)\n",
            "83 98 Loss: 0.559 | Acc: 80.534% (34636/43008)\n",
            "84 98 Loss: 0.558 | Acc: 80.581% (35069/43520)\n",
            "85 98 Loss: 0.558 | Acc: 80.580% (35481/44032)\n",
            "86 98 Loss: 0.558 | Acc: 80.594% (35900/44544)\n",
            "87 98 Loss: 0.559 | Acc: 80.591% (36311/45056)\n",
            "88 98 Loss: 0.558 | Acc: 80.611% (36733/45568)\n",
            "89 98 Loss: 0.558 | Acc: 80.610% (37145/46080)\n",
            "90 98 Loss: 0.558 | Acc: 80.621% (37563/46592)\n",
            "91 98 Loss: 0.558 | Acc: 80.613% (37972/47104)\n",
            "92 98 Loss: 0.557 | Acc: 80.639% (38397/47616)\n",
            "93 98 Loss: 0.558 | Acc: 80.627% (38804/48128)\n",
            "94 98 Loss: 0.558 | Acc: 80.615% (39211/48640)\n",
            "95 98 Loss: 0.558 | Acc: 80.634% (39633/49152)\n",
            "96 98 Loss: 0.558 | Acc: 80.616% (40037/49664)\n",
            "97 98 Loss: 0.558 | Acc: 80.618% (40309/50000)\n",
            "\n",
            "Epoch: 5\n",
            "Time elapsed: 10.74 min\n",
            "0 98 Loss: 0.537 | Acc: 83.789% (429/512)\n",
            "1 98 Loss: 0.510 | Acc: 82.520% (845/1024)\n",
            "2 98 Loss: 0.505 | Acc: 82.552% (1268/1536)\n",
            "3 98 Loss: 0.516 | Acc: 82.275% (1685/2048)\n",
            "4 98 Loss: 0.522 | Acc: 81.875% (2096/2560)\n",
            "5 98 Loss: 0.524 | Acc: 81.966% (2518/3072)\n",
            "6 98 Loss: 0.513 | Acc: 82.171% (2945/3584)\n",
            "7 98 Loss: 0.508 | Acc: 82.300% (3371/4096)\n",
            "8 98 Loss: 0.510 | Acc: 82.096% (3783/4608)\n",
            "9 98 Loss: 0.510 | Acc: 82.188% (4208/5120)\n",
            "10 98 Loss: 0.505 | Acc: 82.493% (4646/5632)\n",
            "11 98 Loss: 0.504 | Acc: 82.568% (5073/6144)\n",
            "12 98 Loss: 0.499 | Acc: 82.647% (5501/6656)\n",
            "13 98 Loss: 0.498 | Acc: 82.687% (5927/7168)\n",
            "14 98 Loss: 0.499 | Acc: 82.695% (6351/7680)\n",
            "15 98 Loss: 0.494 | Acc: 82.849% (6787/8192)\n",
            "16 98 Loss: 0.497 | Acc: 82.675% (7196/8704)\n",
            "17 98 Loss: 0.496 | Acc: 82.693% (7621/9216)\n",
            "18 98 Loss: 0.493 | Acc: 82.812% (8056/9728)\n",
            "19 98 Loss: 0.495 | Acc: 82.656% (8464/10240)\n",
            "20 98 Loss: 0.500 | Acc: 82.515% (8872/10752)\n",
            "21 98 Loss: 0.503 | Acc: 82.386% (9280/11264)\n",
            "22 98 Loss: 0.504 | Acc: 82.413% (9705/11776)\n",
            "23 98 Loss: 0.505 | Acc: 82.422% (10128/12288)\n",
            "24 98 Loss: 0.506 | Acc: 82.422% (10550/12800)\n",
            "25 98 Loss: 0.506 | Acc: 82.362% (10964/13312)\n",
            "26 98 Loss: 0.506 | Acc: 82.284% (11375/13824)\n",
            "27 98 Loss: 0.507 | Acc: 82.234% (11789/14336)\n",
            "28 98 Loss: 0.507 | Acc: 82.294% (12219/14848)\n",
            "29 98 Loss: 0.507 | Acc: 82.272% (12637/15360)\n",
            "30 98 Loss: 0.506 | Acc: 82.283% (13060/15872)\n",
            "31 98 Loss: 0.508 | Acc: 82.220% (13471/16384)\n",
            "32 98 Loss: 0.509 | Acc: 82.156% (13881/16896)\n",
            "33 98 Loss: 0.509 | Acc: 82.158% (14302/17408)\n",
            "34 98 Loss: 0.510 | Acc: 82.137% (14719/17920)\n",
            "35 98 Loss: 0.511 | Acc: 82.102% (15133/18432)\n",
            "36 98 Loss: 0.511 | Acc: 82.079% (15549/18944)\n",
            "37 98 Loss: 0.510 | Acc: 82.119% (15977/19456)\n",
            "38 98 Loss: 0.510 | Acc: 82.126% (16399/19968)\n",
            "39 98 Loss: 0.511 | Acc: 82.124% (16819/20480)\n",
            "40 98 Loss: 0.512 | Acc: 82.107% (17236/20992)\n",
            "41 98 Loss: 0.513 | Acc: 82.073% (17649/21504)\n",
            "42 98 Loss: 0.513 | Acc: 82.049% (18064/22016)\n",
            "43 98 Loss: 0.512 | Acc: 82.071% (18489/22528)\n",
            "44 98 Loss: 0.512 | Acc: 82.070% (18909/23040)\n",
            "45 98 Loss: 0.511 | Acc: 82.057% (19326/23552)\n",
            "46 98 Loss: 0.511 | Acc: 82.027% (19739/24064)\n",
            "47 98 Loss: 0.510 | Acc: 82.092% (20175/24576)\n",
            "48 98 Loss: 0.510 | Acc: 82.107% (20599/25088)\n",
            "49 98 Loss: 0.510 | Acc: 82.117% (21022/25600)\n",
            "50 98 Loss: 0.510 | Acc: 82.127% (21445/26112)\n",
            "51 98 Loss: 0.509 | Acc: 82.136% (21868/26624)\n",
            "52 98 Loss: 0.509 | Acc: 82.168% (22297/27136)\n",
            "53 98 Loss: 0.509 | Acc: 82.143% (22711/27648)\n",
            "54 98 Loss: 0.508 | Acc: 82.156% (23135/28160)\n",
            "55 98 Loss: 0.508 | Acc: 82.181% (23563/28672)\n",
            "56 98 Loss: 0.507 | Acc: 82.220% (23995/29184)\n",
            "57 98 Loss: 0.506 | Acc: 82.260% (24428/29696)\n",
            "58 98 Loss: 0.505 | Acc: 82.296% (24860/30208)\n",
            "59 98 Loss: 0.506 | Acc: 82.246% (25266/30720)\n",
            "60 98 Loss: 0.505 | Acc: 82.284% (25699/31232)\n",
            "61 98 Loss: 0.505 | Acc: 82.296% (26124/31744)\n",
            "62 98 Loss: 0.505 | Acc: 82.295% (26545/32256)\n",
            "63 98 Loss: 0.504 | Acc: 82.333% (26979/32768)\n",
            "64 98 Loss: 0.504 | Acc: 82.326% (27398/33280)\n",
            "65 98 Loss: 0.504 | Acc: 82.327% (27820/33792)\n",
            "66 98 Loss: 0.503 | Acc: 82.372% (28257/34304)\n",
            "67 98 Loss: 0.504 | Acc: 82.364% (28676/34816)\n",
            "68 98 Loss: 0.504 | Acc: 82.354% (29094/35328)\n",
            "69 98 Loss: 0.504 | Acc: 82.349% (29514/35840)\n",
            "70 98 Loss: 0.504 | Acc: 82.342% (29933/36352)\n",
            "71 98 Loss: 0.504 | Acc: 82.376% (30367/36864)\n",
            "72 98 Loss: 0.503 | Acc: 82.422% (30806/37376)\n",
            "73 98 Loss: 0.502 | Acc: 82.461% (31243/37888)\n",
            "74 98 Loss: 0.502 | Acc: 82.445% (31659/38400)\n",
            "75 98 Loss: 0.502 | Acc: 82.422% (32072/38912)\n",
            "76 98 Loss: 0.501 | Acc: 82.460% (32509/39424)\n",
            "77 98 Loss: 0.500 | Acc: 82.520% (32955/39936)\n",
            "78 98 Loss: 0.498 | Acc: 82.568% (33397/40448)\n",
            "79 98 Loss: 0.498 | Acc: 82.588% (33828/40960)\n",
            "80 98 Loss: 0.498 | Acc: 82.579% (34247/41472)\n",
            "81 98 Loss: 0.499 | Acc: 82.572% (34667/41984)\n",
            "82 98 Loss: 0.499 | Acc: 82.584% (35095/42496)\n",
            "83 98 Loss: 0.498 | Acc: 82.606% (35527/43008)\n",
            "84 98 Loss: 0.498 | Acc: 82.631% (35961/43520)\n",
            "85 98 Loss: 0.497 | Acc: 82.663% (36398/44032)\n",
            "86 98 Loss: 0.496 | Acc: 82.678% (36828/44544)\n",
            "87 98 Loss: 0.496 | Acc: 82.677% (37251/45056)\n",
            "88 98 Loss: 0.496 | Acc: 82.716% (37692/45568)\n",
            "89 98 Loss: 0.496 | Acc: 82.706% (38111/46080)\n",
            "90 98 Loss: 0.495 | Acc: 82.694% (38529/46592)\n",
            "91 98 Loss: 0.495 | Acc: 82.696% (38953/47104)\n",
            "92 98 Loss: 0.494 | Acc: 82.716% (39386/47616)\n",
            "93 98 Loss: 0.495 | Acc: 82.677% (39791/48128)\n",
            "94 98 Loss: 0.496 | Acc: 82.675% (40213/48640)\n",
            "95 98 Loss: 0.495 | Acc: 82.707% (40652/49152)\n",
            "96 98 Loss: 0.495 | Acc: 82.720% (41082/49664)\n",
            "97 98 Loss: 0.494 | Acc: 82.742% (41371/50000)\n",
            "\n",
            "Epoch: 6\n",
            "Time elapsed: 12.88 min\n",
            "0 98 Loss: 0.481 | Acc: 83.203% (426/512)\n",
            "1 98 Loss: 0.461 | Acc: 84.473% (865/1024)\n",
            "2 98 Loss: 0.443 | Acc: 84.570% (1299/1536)\n",
            "3 98 Loss: 0.435 | Acc: 84.961% (1740/2048)\n",
            "4 98 Loss: 0.443 | Acc: 84.570% (2165/2560)\n",
            "5 98 Loss: 0.453 | Acc: 84.082% (2583/3072)\n",
            "6 98 Loss: 0.438 | Acc: 84.626% (3033/3584)\n",
            "7 98 Loss: 0.444 | Acc: 84.595% (3465/4096)\n",
            "8 98 Loss: 0.447 | Acc: 84.657% (3901/4608)\n",
            "9 98 Loss: 0.443 | Acc: 84.648% (4334/5120)\n",
            "10 98 Loss: 0.448 | Acc: 84.659% (4768/5632)\n",
            "11 98 Loss: 0.441 | Acc: 84.814% (5211/6144)\n",
            "12 98 Loss: 0.440 | Acc: 84.781% (5643/6656)\n",
            "13 98 Loss: 0.445 | Acc: 84.668% (6069/7168)\n",
            "14 98 Loss: 0.444 | Acc: 84.792% (6512/7680)\n",
            "15 98 Loss: 0.443 | Acc: 84.827% (6949/8192)\n",
            "16 98 Loss: 0.440 | Acc: 84.949% (7394/8704)\n",
            "17 98 Loss: 0.438 | Acc: 85.069% (7840/9216)\n",
            "18 98 Loss: 0.440 | Acc: 84.899% (8259/9728)\n",
            "19 98 Loss: 0.437 | Acc: 84.941% (8698/10240)\n",
            "20 98 Loss: 0.440 | Acc: 84.877% (9126/10752)\n",
            "21 98 Loss: 0.438 | Acc: 84.934% (9567/11264)\n",
            "22 98 Loss: 0.439 | Acc: 84.893% (9997/11776)\n",
            "23 98 Loss: 0.436 | Acc: 85.010% (10446/12288)\n",
            "24 98 Loss: 0.435 | Acc: 85.008% (10881/12800)\n",
            "25 98 Loss: 0.435 | Acc: 85.014% (11317/13312)\n",
            "26 98 Loss: 0.437 | Acc: 84.918% (11739/13824)\n",
            "27 98 Loss: 0.437 | Acc: 84.968% (12181/14336)\n",
            "28 98 Loss: 0.436 | Acc: 84.974% (12617/14848)\n",
            "29 98 Loss: 0.435 | Acc: 85.000% (13056/15360)\n",
            "30 98 Loss: 0.434 | Acc: 85.005% (13492/15872)\n",
            "31 98 Loss: 0.435 | Acc: 84.949% (13918/16384)\n",
            "32 98 Loss: 0.436 | Acc: 84.961% (14355/16896)\n",
            "33 98 Loss: 0.435 | Acc: 84.990% (14795/17408)\n",
            "34 98 Loss: 0.434 | Acc: 85.006% (15233/17920)\n",
            "35 98 Loss: 0.437 | Acc: 84.918% (15652/18432)\n",
            "36 98 Loss: 0.438 | Acc: 84.861% (16076/18944)\n",
            "37 98 Loss: 0.437 | Acc: 84.853% (16509/19456)\n",
            "38 98 Loss: 0.438 | Acc: 84.821% (16937/19968)\n",
            "39 98 Loss: 0.439 | Acc: 84.712% (17349/20480)\n",
            "40 98 Loss: 0.440 | Acc: 84.680% (17776/20992)\n",
            "41 98 Loss: 0.440 | Acc: 84.659% (18205/21504)\n",
            "42 98 Loss: 0.440 | Acc: 84.684% (18644/22016)\n",
            "43 98 Loss: 0.441 | Acc: 84.686% (19078/22528)\n",
            "44 98 Loss: 0.440 | Acc: 84.696% (19514/23040)\n",
            "45 98 Loss: 0.440 | Acc: 84.719% (19953/23552)\n",
            "46 98 Loss: 0.442 | Acc: 84.637% (20367/24064)\n",
            "47 98 Loss: 0.443 | Acc: 84.619% (20796/24576)\n",
            "48 98 Loss: 0.443 | Acc: 84.606% (21226/25088)\n",
            "49 98 Loss: 0.443 | Acc: 84.566% (21649/25600)\n",
            "50 98 Loss: 0.443 | Acc: 84.612% (22094/26112)\n",
            "51 98 Loss: 0.442 | Acc: 84.608% (22526/26624)\n",
            "52 98 Loss: 0.441 | Acc: 84.651% (22971/27136)\n",
            "53 98 Loss: 0.442 | Acc: 84.625% (23397/27648)\n",
            "54 98 Loss: 0.442 | Acc: 84.616% (23828/28160)\n",
            "55 98 Loss: 0.442 | Acc: 84.567% (24247/28672)\n",
            "56 98 Loss: 0.442 | Acc: 84.598% (24689/29184)\n",
            "57 98 Loss: 0.441 | Acc: 84.631% (25132/29696)\n",
            "58 98 Loss: 0.441 | Acc: 84.640% (25568/30208)\n",
            "59 98 Loss: 0.441 | Acc: 84.652% (26005/30720)\n",
            "60 98 Loss: 0.441 | Acc: 84.673% (26445/31232)\n",
            "61 98 Loss: 0.443 | Acc: 84.621% (26862/31744)\n",
            "62 98 Loss: 0.443 | Acc: 84.601% (27289/32256)\n",
            "63 98 Loss: 0.443 | Acc: 84.601% (27722/32768)\n",
            "64 98 Loss: 0.443 | Acc: 84.597% (28154/33280)\n",
            "65 98 Loss: 0.442 | Acc: 84.635% (28600/33792)\n",
            "66 98 Loss: 0.441 | Acc: 84.678% (29048/34304)\n",
            "67 98 Loss: 0.442 | Acc: 84.628% (29464/34816)\n",
            "68 98 Loss: 0.443 | Acc: 84.587% (29883/35328)\n",
            "69 98 Loss: 0.443 | Acc: 84.595% (30319/35840)\n",
            "70 98 Loss: 0.442 | Acc: 84.625% (30763/36352)\n",
            "71 98 Loss: 0.441 | Acc: 84.630% (31198/36864)\n",
            "72 98 Loss: 0.441 | Acc: 84.661% (31643/37376)\n",
            "73 98 Loss: 0.442 | Acc: 84.628% (32064/37888)\n",
            "74 98 Loss: 0.442 | Acc: 84.648% (32505/38400)\n",
            "75 98 Loss: 0.441 | Acc: 84.671% (32947/38912)\n",
            "76 98 Loss: 0.442 | Acc: 84.672% (33381/39424)\n",
            "77 98 Loss: 0.441 | Acc: 84.685% (33820/39936)\n",
            "78 98 Loss: 0.441 | Acc: 84.694% (34257/40448)\n",
            "79 98 Loss: 0.442 | Acc: 84.680% (34685/40960)\n",
            "80 98 Loss: 0.441 | Acc: 84.681% (35119/41472)\n",
            "81 98 Loss: 0.441 | Acc: 84.685% (35554/41984)\n",
            "82 98 Loss: 0.441 | Acc: 84.676% (35984/42496)\n",
            "83 98 Loss: 0.442 | Acc: 84.668% (36414/43008)\n",
            "84 98 Loss: 0.442 | Acc: 84.651% (36840/43520)\n",
            "85 98 Loss: 0.442 | Acc: 84.638% (37268/44032)\n",
            "86 98 Loss: 0.442 | Acc: 84.624% (37695/44544)\n",
            "87 98 Loss: 0.443 | Acc: 84.624% (38128/45056)\n",
            "88 98 Loss: 0.443 | Acc: 84.601% (38551/45568)\n",
            "89 98 Loss: 0.443 | Acc: 84.594% (38981/46080)\n",
            "90 98 Loss: 0.443 | Acc: 84.590% (39412/46592)\n",
            "91 98 Loss: 0.444 | Acc: 84.566% (39834/47104)\n",
            "92 98 Loss: 0.444 | Acc: 84.570% (40269/47616)\n",
            "93 98 Loss: 0.444 | Acc: 84.568% (40701/48128)\n",
            "94 98 Loss: 0.445 | Acc: 84.542% (41121/48640)\n",
            "95 98 Loss: 0.444 | Acc: 84.552% (41559/49152)\n",
            "96 98 Loss: 0.444 | Acc: 84.564% (41998/49664)\n",
            "97 98 Loss: 0.443 | Acc: 84.566% (42283/50000)\n",
            "\n",
            "Epoch: 7\n",
            "Time elapsed: 15.02 min\n",
            "0 98 Loss: 0.424 | Acc: 84.180% (431/512)\n",
            "1 98 Loss: 0.438 | Acc: 83.984% (860/1024)\n",
            "2 98 Loss: 0.440 | Acc: 83.984% (1290/1536)\n",
            "3 98 Loss: 0.421 | Acc: 85.059% (1742/2048)\n",
            "4 98 Loss: 0.426 | Acc: 84.805% (2171/2560)\n",
            "5 98 Loss: 0.420 | Acc: 84.798% (2605/3072)\n",
            "6 98 Loss: 0.424 | Acc: 84.654% (3034/3584)\n",
            "7 98 Loss: 0.414 | Acc: 85.156% (3488/4096)\n",
            "8 98 Loss: 0.410 | Acc: 85.243% (3928/4608)\n",
            "9 98 Loss: 0.411 | Acc: 85.352% (4370/5120)\n",
            "10 98 Loss: 0.407 | Acc: 85.458% (4813/5632)\n",
            "11 98 Loss: 0.405 | Acc: 85.563% (5257/6144)\n",
            "12 98 Loss: 0.407 | Acc: 85.607% (5698/6656)\n",
            "13 98 Loss: 0.405 | Acc: 85.784% (6149/7168)\n",
            "14 98 Loss: 0.401 | Acc: 86.003% (6605/7680)\n",
            "15 98 Loss: 0.403 | Acc: 85.889% (7036/8192)\n",
            "16 98 Loss: 0.404 | Acc: 85.892% (7476/8704)\n",
            "17 98 Loss: 0.406 | Acc: 85.818% (7909/9216)\n",
            "18 98 Loss: 0.406 | Acc: 85.907% (8357/9728)\n",
            "19 98 Loss: 0.408 | Acc: 85.898% (8796/10240)\n",
            "20 98 Loss: 0.407 | Acc: 85.984% (9245/10752)\n",
            "21 98 Loss: 0.409 | Acc: 85.893% (9675/11264)\n",
            "22 98 Loss: 0.408 | Acc: 85.988% (10126/11776)\n",
            "23 98 Loss: 0.409 | Acc: 85.938% (10560/12288)\n",
            "24 98 Loss: 0.412 | Acc: 85.875% (10992/12800)\n",
            "25 98 Loss: 0.411 | Acc: 85.862% (11430/13312)\n",
            "26 98 Loss: 0.414 | Acc: 85.764% (11856/13824)\n",
            "27 98 Loss: 0.413 | Acc: 85.728% (12290/14336)\n",
            "28 98 Loss: 0.412 | Acc: 85.735% (12730/14848)\n",
            "29 98 Loss: 0.414 | Acc: 85.651% (13156/15360)\n",
            "30 98 Loss: 0.411 | Acc: 85.736% (13608/15872)\n",
            "31 98 Loss: 0.411 | Acc: 85.693% (14040/16384)\n",
            "32 98 Loss: 0.411 | Acc: 85.713% (14482/16896)\n",
            "33 98 Loss: 0.409 | Acc: 85.771% (14931/17408)\n",
            "34 98 Loss: 0.407 | Acc: 85.843% (15383/17920)\n",
            "35 98 Loss: 0.407 | Acc: 85.802% (15815/18432)\n",
            "36 98 Loss: 0.407 | Acc: 85.784% (16251/18944)\n",
            "37 98 Loss: 0.406 | Acc: 85.830% (16699/19456)\n",
            "38 98 Loss: 0.406 | Acc: 85.832% (17139/19968)\n",
            "39 98 Loss: 0.405 | Acc: 85.845% (17581/20480)\n",
            "40 98 Loss: 0.407 | Acc: 85.771% (18005/20992)\n",
            "41 98 Loss: 0.406 | Acc: 85.826% (18456/21504)\n",
            "42 98 Loss: 0.405 | Acc: 85.833% (18897/22016)\n",
            "43 98 Loss: 0.405 | Acc: 85.813% (19332/22528)\n",
            "44 98 Loss: 0.405 | Acc: 85.829% (19775/23040)\n",
            "45 98 Loss: 0.407 | Acc: 85.785% (20204/23552)\n",
            "46 98 Loss: 0.408 | Acc: 85.746% (20634/24064)\n",
            "47 98 Loss: 0.410 | Acc: 85.706% (21063/24576)\n",
            "48 98 Loss: 0.410 | Acc: 85.690% (21498/25088)\n",
            "49 98 Loss: 0.409 | Acc: 85.688% (21936/25600)\n",
            "50 98 Loss: 0.408 | Acc: 85.742% (22389/26112)\n",
            "51 98 Loss: 0.408 | Acc: 85.753% (22831/26624)\n",
            "52 98 Loss: 0.407 | Acc: 85.775% (23276/27136)\n",
            "53 98 Loss: 0.407 | Acc: 85.800% (23722/27648)\n",
            "54 98 Loss: 0.407 | Acc: 85.803% (24162/28160)\n",
            "55 98 Loss: 0.407 | Acc: 85.829% (24609/28672)\n",
            "56 98 Loss: 0.406 | Acc: 85.848% (25054/29184)\n",
            "57 98 Loss: 0.406 | Acc: 85.877% (25502/29696)\n",
            "58 98 Loss: 0.405 | Acc: 85.888% (25945/30208)\n",
            "59 98 Loss: 0.406 | Acc: 85.843% (26371/30720)\n",
            "60 98 Loss: 0.407 | Acc: 85.857% (26815/31232)\n",
            "61 98 Loss: 0.408 | Acc: 85.818% (27242/31744)\n",
            "62 98 Loss: 0.408 | Acc: 85.829% (27685/32256)\n",
            "63 98 Loss: 0.408 | Acc: 85.831% (28125/32768)\n",
            "64 98 Loss: 0.407 | Acc: 85.847% (28570/33280)\n",
            "65 98 Loss: 0.408 | Acc: 85.834% (29005/33792)\n",
            "66 98 Loss: 0.407 | Acc: 85.847% (29449/34304)\n",
            "67 98 Loss: 0.408 | Acc: 85.840% (29886/34816)\n",
            "68 98 Loss: 0.408 | Acc: 85.838% (30325/35328)\n",
            "69 98 Loss: 0.407 | Acc: 85.854% (30770/35840)\n",
            "70 98 Loss: 0.407 | Acc: 85.858% (31211/36352)\n",
            "71 98 Loss: 0.406 | Acc: 85.862% (31652/36864)\n",
            "72 98 Loss: 0.406 | Acc: 85.879% (32098/37376)\n",
            "73 98 Loss: 0.407 | Acc: 85.872% (32535/37888)\n",
            "74 98 Loss: 0.408 | Acc: 85.831% (32959/38400)\n",
            "75 98 Loss: 0.408 | Acc: 85.835% (33400/38912)\n",
            "76 98 Loss: 0.408 | Acc: 85.823% (33835/39424)\n",
            "77 98 Loss: 0.408 | Acc: 85.815% (34271/39936)\n",
            "78 98 Loss: 0.407 | Acc: 85.839% (34720/40448)\n",
            "79 98 Loss: 0.407 | Acc: 85.837% (35159/40960)\n",
            "80 98 Loss: 0.407 | Acc: 85.843% (35601/41472)\n",
            "81 98 Loss: 0.407 | Acc: 85.830% (36035/41984)\n",
            "82 98 Loss: 0.406 | Acc: 85.834% (36476/42496)\n",
            "83 98 Loss: 0.407 | Acc: 85.838% (36917/43008)\n",
            "84 98 Loss: 0.407 | Acc: 85.836% (37356/43520)\n",
            "85 98 Loss: 0.407 | Acc: 85.822% (37789/44032)\n",
            "86 98 Loss: 0.407 | Acc: 85.819% (38227/44544)\n",
            "87 98 Loss: 0.406 | Acc: 85.860% (38685/45056)\n",
            "88 98 Loss: 0.406 | Acc: 85.863% (39126/45568)\n",
            "89 98 Loss: 0.406 | Acc: 85.840% (39555/46080)\n",
            "90 98 Loss: 0.406 | Acc: 85.847% (39998/46592)\n",
            "91 98 Loss: 0.405 | Acc: 85.859% (40443/47104)\n",
            "92 98 Loss: 0.404 | Acc: 85.893% (40899/47616)\n",
            "93 98 Loss: 0.404 | Acc: 85.902% (41343/48128)\n",
            "94 98 Loss: 0.404 | Acc: 85.909% (41786/48640)\n",
            "95 98 Loss: 0.404 | Acc: 85.915% (42229/49152)\n",
            "96 98 Loss: 0.404 | Acc: 85.915% (42669/49664)\n",
            "97 98 Loss: 0.404 | Acc: 85.920% (42960/50000)\n",
            "\n",
            "Epoch: 8\n",
            "Time elapsed: 17.15 min\n",
            "0 98 Loss: 0.368 | Acc: 87.500% (448/512)\n",
            "1 98 Loss: 0.372 | Acc: 88.184% (903/1024)\n",
            "2 98 Loss: 0.385 | Acc: 87.565% (1345/1536)\n",
            "3 98 Loss: 0.386 | Acc: 87.109% (1784/2048)\n",
            "4 98 Loss: 0.382 | Acc: 87.070% (2229/2560)\n",
            "5 98 Loss: 0.380 | Acc: 87.077% (2675/3072)\n",
            "6 98 Loss: 0.375 | Acc: 87.137% (3123/3584)\n",
            "7 98 Loss: 0.365 | Acc: 87.524% (3585/4096)\n",
            "8 98 Loss: 0.364 | Acc: 87.652% (4039/4608)\n",
            "9 98 Loss: 0.363 | Acc: 87.598% (4485/5120)\n",
            "10 98 Loss: 0.366 | Acc: 87.482% (4927/5632)\n",
            "11 98 Loss: 0.365 | Acc: 87.484% (5375/6144)\n",
            "12 98 Loss: 0.369 | Acc: 87.380% (5816/6656)\n",
            "13 98 Loss: 0.369 | Acc: 87.235% (6253/7168)\n",
            "14 98 Loss: 0.371 | Acc: 87.135% (6692/7680)\n",
            "15 98 Loss: 0.369 | Acc: 87.170% (7141/8192)\n",
            "16 98 Loss: 0.367 | Acc: 87.213% (7591/8704)\n",
            "17 98 Loss: 0.368 | Acc: 87.326% (8048/9216)\n",
            "18 98 Loss: 0.370 | Acc: 87.294% (8492/9728)\n",
            "19 98 Loss: 0.369 | Acc: 87.334% (8943/10240)\n",
            "20 98 Loss: 0.371 | Acc: 87.314% (9388/10752)\n",
            "21 98 Loss: 0.369 | Acc: 87.393% (9844/11264)\n",
            "22 98 Loss: 0.368 | Acc: 87.441% (10297/11776)\n",
            "23 98 Loss: 0.370 | Acc: 87.329% (10731/12288)\n",
            "24 98 Loss: 0.370 | Acc: 87.336% (11179/12800)\n",
            "25 98 Loss: 0.370 | Acc: 87.282% (11619/13312)\n",
            "26 98 Loss: 0.369 | Acc: 87.326% (12072/13824)\n",
            "27 98 Loss: 0.369 | Acc: 87.319% (12518/14336)\n",
            "28 98 Loss: 0.371 | Acc: 87.271% (12958/14848)\n",
            "29 98 Loss: 0.370 | Acc: 87.233% (13399/15360)\n",
            "30 98 Loss: 0.370 | Acc: 87.216% (13843/15872)\n",
            "31 98 Loss: 0.369 | Acc: 87.286% (14301/16384)\n",
            "32 98 Loss: 0.371 | Acc: 87.234% (14739/16896)\n",
            "33 98 Loss: 0.374 | Acc: 87.109% (15164/17408)\n",
            "34 98 Loss: 0.375 | Acc: 87.059% (15601/17920)\n",
            "35 98 Loss: 0.376 | Acc: 86.990% (16034/18432)\n",
            "36 98 Loss: 0.375 | Acc: 86.972% (16476/18944)\n",
            "37 98 Loss: 0.375 | Acc: 86.991% (16925/19456)\n",
            "38 98 Loss: 0.374 | Acc: 87.024% (17377/19968)\n",
            "39 98 Loss: 0.373 | Acc: 87.085% (17835/20480)\n",
            "40 98 Loss: 0.373 | Acc: 87.067% (18277/20992)\n",
            "41 98 Loss: 0.374 | Acc: 87.081% (18726/21504)\n",
            "42 98 Loss: 0.374 | Acc: 87.082% (19172/22016)\n",
            "43 98 Loss: 0.373 | Acc: 87.092% (19620/22528)\n",
            "44 98 Loss: 0.373 | Acc: 87.088% (20065/23040)\n",
            "45 98 Loss: 0.373 | Acc: 87.080% (20509/23552)\n",
            "46 98 Loss: 0.374 | Acc: 87.047% (20947/24064)\n",
            "47 98 Loss: 0.374 | Acc: 87.024% (21387/24576)\n",
            "48 98 Loss: 0.375 | Acc: 86.998% (21826/25088)\n",
            "49 98 Loss: 0.376 | Acc: 86.961% (22262/25600)\n",
            "50 98 Loss: 0.376 | Acc: 86.914% (22695/26112)\n",
            "51 98 Loss: 0.377 | Acc: 86.884% (23132/26624)\n",
            "52 98 Loss: 0.378 | Acc: 86.888% (23578/27136)\n",
            "53 98 Loss: 0.379 | Acc: 86.849% (24012/27648)\n",
            "54 98 Loss: 0.379 | Acc: 86.843% (24455/28160)\n",
            "55 98 Loss: 0.378 | Acc: 86.872% (24908/28672)\n",
            "56 98 Loss: 0.378 | Acc: 86.887% (25357/29184)\n",
            "57 98 Loss: 0.377 | Acc: 86.934% (25816/29696)\n",
            "58 98 Loss: 0.376 | Acc: 86.967% (26271/30208)\n",
            "59 98 Loss: 0.375 | Acc: 86.969% (26717/30720)\n",
            "60 98 Loss: 0.375 | Acc: 86.965% (27161/31232)\n",
            "61 98 Loss: 0.375 | Acc: 86.942% (27599/31744)\n",
            "62 98 Loss: 0.376 | Acc: 86.923% (28038/32256)\n",
            "63 98 Loss: 0.375 | Acc: 86.945% (28490/32768)\n",
            "64 98 Loss: 0.375 | Acc: 86.968% (28943/33280)\n",
            "65 98 Loss: 0.374 | Acc: 86.991% (29396/33792)\n",
            "66 98 Loss: 0.375 | Acc: 86.975% (29836/34304)\n",
            "67 98 Loss: 0.376 | Acc: 86.972% (30280/34816)\n",
            "68 98 Loss: 0.376 | Acc: 86.954% (30719/35328)\n",
            "69 98 Loss: 0.375 | Acc: 86.992% (31178/35840)\n",
            "70 98 Loss: 0.375 | Acc: 87.013% (31631/36352)\n",
            "71 98 Loss: 0.375 | Acc: 87.012% (32076/36864)\n",
            "72 98 Loss: 0.376 | Acc: 86.970% (32506/37376)\n",
            "73 98 Loss: 0.376 | Acc: 86.967% (32950/37888)\n",
            "74 98 Loss: 0.376 | Acc: 86.951% (33389/38400)\n",
            "75 98 Loss: 0.376 | Acc: 86.945% (33832/38912)\n",
            "76 98 Loss: 0.375 | Acc: 86.975% (34289/39424)\n",
            "77 98 Loss: 0.376 | Acc: 86.957% (34727/39936)\n",
            "78 98 Loss: 0.376 | Acc: 86.966% (35176/40448)\n",
            "79 98 Loss: 0.375 | Acc: 86.963% (35620/40960)\n",
            "80 98 Loss: 0.375 | Acc: 86.972% (36069/41472)\n",
            "81 98 Loss: 0.376 | Acc: 86.952% (36506/41984)\n",
            "82 98 Loss: 0.377 | Acc: 86.902% (36930/42496)\n",
            "83 98 Loss: 0.377 | Acc: 86.898% (37373/43008)\n",
            "84 98 Loss: 0.377 | Acc: 86.903% (37820/43520)\n",
            "85 98 Loss: 0.377 | Acc: 86.896% (38262/44032)\n",
            "86 98 Loss: 0.376 | Acc: 86.923% (38719/44544)\n",
            "87 98 Loss: 0.376 | Acc: 86.936% (39170/45056)\n",
            "88 98 Loss: 0.375 | Acc: 86.949% (39621/45568)\n",
            "89 98 Loss: 0.375 | Acc: 86.981% (40081/46080)\n",
            "90 98 Loss: 0.375 | Acc: 86.987% (40529/46592)\n",
            "91 98 Loss: 0.375 | Acc: 86.993% (40977/47104)\n",
            "92 98 Loss: 0.375 | Acc: 86.977% (41415/47616)\n",
            "93 98 Loss: 0.375 | Acc: 86.970% (41857/48128)\n",
            "94 98 Loss: 0.375 | Acc: 86.976% (42305/48640)\n",
            "95 98 Loss: 0.375 | Acc: 86.957% (42741/49152)\n",
            "96 98 Loss: 0.375 | Acc: 86.948% (43182/49664)\n",
            "97 98 Loss: 0.375 | Acc: 86.940% (43470/50000)\n",
            "\n",
            "Epoch: 9\n",
            "Time elapsed: 19.30 min\n",
            "0 98 Loss: 0.295 | Acc: 90.234% (462/512)\n",
            "1 98 Loss: 0.338 | Acc: 88.184% (903/1024)\n",
            "2 98 Loss: 0.344 | Acc: 88.477% (1359/1536)\n",
            "3 98 Loss: 0.319 | Acc: 89.355% (1830/2048)\n",
            "4 98 Loss: 0.326 | Acc: 88.984% (2278/2560)\n",
            "5 98 Loss: 0.325 | Acc: 88.835% (2729/3072)\n",
            "6 98 Loss: 0.321 | Acc: 89.062% (3192/3584)\n",
            "7 98 Loss: 0.322 | Acc: 88.818% (3638/4096)\n",
            "8 98 Loss: 0.324 | Acc: 88.607% (4083/4608)\n",
            "9 98 Loss: 0.324 | Acc: 88.555% (4534/5120)\n",
            "10 98 Loss: 0.327 | Acc: 88.459% (4982/5632)\n",
            "11 98 Loss: 0.326 | Acc: 88.477% (5436/6144)\n",
            "12 98 Loss: 0.327 | Acc: 88.507% (5891/6656)\n",
            "13 98 Loss: 0.329 | Acc: 88.365% (6334/7168)\n",
            "14 98 Loss: 0.336 | Acc: 88.190% (6773/7680)\n",
            "15 98 Loss: 0.335 | Acc: 88.306% (7234/8192)\n",
            "16 98 Loss: 0.333 | Acc: 88.396% (7694/8704)\n",
            "17 98 Loss: 0.334 | Acc: 88.401% (8147/9216)\n",
            "18 98 Loss: 0.332 | Acc: 88.497% (8609/9728)\n",
            "19 98 Loss: 0.332 | Acc: 88.477% (9060/10240)\n",
            "20 98 Loss: 0.334 | Acc: 88.365% (9501/10752)\n",
            "21 98 Loss: 0.335 | Acc: 88.308% (9947/11264)\n",
            "22 98 Loss: 0.335 | Acc: 88.409% (10411/11776)\n",
            "23 98 Loss: 0.336 | Acc: 88.371% (10859/12288)\n",
            "24 98 Loss: 0.335 | Acc: 88.375% (11312/12800)\n",
            "25 98 Loss: 0.334 | Acc: 88.371% (11764/13312)\n",
            "26 98 Loss: 0.332 | Acc: 88.455% (12228/13824)\n",
            "27 98 Loss: 0.333 | Acc: 88.421% (12676/14336)\n",
            "28 98 Loss: 0.333 | Acc: 88.389% (13124/14848)\n",
            "29 98 Loss: 0.334 | Acc: 88.359% (13572/15360)\n",
            "30 98 Loss: 0.333 | Acc: 88.325% (14019/15872)\n",
            "31 98 Loss: 0.334 | Acc: 88.312% (14469/16384)\n",
            "32 98 Loss: 0.336 | Acc: 88.198% (14902/16896)\n",
            "33 98 Loss: 0.335 | Acc: 88.253% (15363/17408)\n",
            "34 98 Loss: 0.335 | Acc: 88.276% (15819/17920)\n",
            "35 98 Loss: 0.334 | Acc: 88.281% (16272/18432)\n",
            "36 98 Loss: 0.334 | Acc: 88.271% (16722/18944)\n",
            "37 98 Loss: 0.335 | Acc: 88.256% (17171/19456)\n",
            "38 98 Loss: 0.334 | Acc: 88.301% (17632/19968)\n",
            "39 98 Loss: 0.334 | Acc: 88.311% (18086/20480)\n",
            "40 98 Loss: 0.334 | Acc: 88.286% (18533/20992)\n",
            "41 98 Loss: 0.335 | Acc: 88.291% (18986/21504)\n",
            "42 98 Loss: 0.335 | Acc: 88.286% (19437/22016)\n",
            "43 98 Loss: 0.336 | Acc: 88.228% (19876/22528)\n",
            "44 98 Loss: 0.336 | Acc: 88.203% (20322/23040)\n",
            "45 98 Loss: 0.336 | Acc: 88.205% (20774/23552)\n",
            "46 98 Loss: 0.336 | Acc: 88.244% (21235/24064)\n",
            "47 98 Loss: 0.336 | Acc: 88.245% (21687/24576)\n",
            "48 98 Loss: 0.336 | Acc: 88.225% (22134/25088)\n",
            "49 98 Loss: 0.338 | Acc: 88.164% (22570/25600)\n",
            "50 98 Loss: 0.338 | Acc: 88.193% (23029/26112)\n",
            "51 98 Loss: 0.338 | Acc: 88.180% (23477/26624)\n",
            "52 98 Loss: 0.338 | Acc: 88.163% (23924/27136)\n",
            "53 98 Loss: 0.339 | Acc: 88.166% (24376/27648)\n",
            "54 98 Loss: 0.338 | Acc: 88.200% (24837/28160)\n",
            "55 98 Loss: 0.338 | Acc: 88.170% (25280/28672)\n",
            "56 98 Loss: 0.339 | Acc: 88.161% (25729/29184)\n",
            "57 98 Loss: 0.339 | Acc: 88.140% (26174/29696)\n",
            "58 98 Loss: 0.340 | Acc: 88.122% (26620/30208)\n",
            "59 98 Loss: 0.339 | Acc: 88.118% (27070/30720)\n",
            "60 98 Loss: 0.339 | Acc: 88.131% (27525/31232)\n",
            "61 98 Loss: 0.339 | Acc: 88.105% (27968/31744)\n",
            "62 98 Loss: 0.339 | Acc: 88.129% (28427/32256)\n",
            "63 98 Loss: 0.340 | Acc: 88.107% (28871/32768)\n",
            "64 98 Loss: 0.339 | Acc: 88.143% (29334/33280)\n",
            "65 98 Loss: 0.340 | Acc: 88.133% (29782/33792)\n",
            "66 98 Loss: 0.340 | Acc: 88.112% (30226/34304)\n",
            "67 98 Loss: 0.341 | Acc: 88.115% (30678/34816)\n",
            "68 98 Loss: 0.340 | Acc: 88.134% (31136/35328)\n",
            "69 98 Loss: 0.340 | Acc: 88.150% (31593/35840)\n",
            "70 98 Loss: 0.340 | Acc: 88.138% (32040/36352)\n",
            "71 98 Loss: 0.340 | Acc: 88.154% (32497/36864)\n",
            "72 98 Loss: 0.339 | Acc: 88.164% (32952/37376)\n",
            "73 98 Loss: 0.338 | Acc: 88.205% (33419/37888)\n",
            "74 98 Loss: 0.339 | Acc: 88.188% (33864/38400)\n",
            "75 98 Loss: 0.339 | Acc: 88.207% (34323/38912)\n",
            "76 98 Loss: 0.340 | Acc: 88.172% (34761/39424)\n",
            "77 98 Loss: 0.341 | Acc: 88.149% (35203/39936)\n",
            "78 98 Loss: 0.341 | Acc: 88.153% (35656/40448)\n",
            "79 98 Loss: 0.342 | Acc: 88.123% (36095/40960)\n",
            "80 98 Loss: 0.342 | Acc: 88.117% (36544/41472)\n",
            "81 98 Loss: 0.341 | Acc: 88.160% (37013/41984)\n",
            "82 98 Loss: 0.341 | Acc: 88.171% (37469/42496)\n",
            "83 98 Loss: 0.342 | Acc: 88.139% (37907/43008)\n",
            "84 98 Loss: 0.342 | Acc: 88.120% (38350/43520)\n",
            "85 98 Loss: 0.342 | Acc: 88.118% (38800/44032)\n",
            "86 98 Loss: 0.342 | Acc: 88.124% (39254/44544)\n",
            "87 98 Loss: 0.343 | Acc: 88.128% (39707/45056)\n",
            "88 98 Loss: 0.343 | Acc: 88.132% (40160/45568)\n",
            "89 98 Loss: 0.343 | Acc: 88.116% (40604/46080)\n",
            "90 98 Loss: 0.343 | Acc: 88.095% (41045/46592)\n",
            "91 98 Loss: 0.344 | Acc: 88.063% (41481/47104)\n",
            "92 98 Loss: 0.344 | Acc: 88.086% (41943/47616)\n",
            "93 98 Loss: 0.344 | Acc: 88.084% (42393/48128)\n",
            "94 98 Loss: 0.344 | Acc: 88.061% (42833/48640)\n",
            "95 98 Loss: 0.344 | Acc: 88.082% (43294/49152)\n",
            "96 98 Loss: 0.344 | Acc: 88.090% (43749/49664)\n",
            "97 98 Loss: 0.345 | Acc: 88.074% (44037/50000)\n"
          ]
        }
      ]
    }
  ]
}