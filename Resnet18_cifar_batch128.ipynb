{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN20MN26OiCqA786bB/5Cjx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_cifar_batch128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox_FE_xeDz6F",
        "outputId": "e0e3a3ac-ddce-4637-86c1-58932f047577"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svako osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    #transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "])\n",
        "\n",
        "\n",
        "#trainset = torchvision.datasets.CIFAR10(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#          'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4',\n",
        "           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "       \n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+200):\n",
        "train(start_epoch)\n",
        "#    scheduler.step()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "0 469 Loss: 2.409 | Acc: 10.156% (13/128)\n",
            "1 469 Loss: 2.343 | Acc: 13.281% (34/256)\n",
            "2 469 Loss: 2.305 | Acc: 15.104% (58/384)\n",
            "3 469 Loss: 2.278 | Acc: 16.406% (84/512)\n",
            "4 469 Loss: 2.265 | Acc: 17.188% (110/640)\n",
            "5 469 Loss: 2.234 | Acc: 17.578% (135/768)\n",
            "6 469 Loss: 2.188 | Acc: 19.531% (175/896)\n",
            "7 469 Loss: 2.169 | Acc: 19.727% (202/1024)\n",
            "8 469 Loss: 2.125 | Acc: 21.528% (248/1152)\n",
            "9 469 Loss: 2.097 | Acc: 22.031% (282/1280)\n",
            "10 469 Loss: 2.079 | Acc: 21.946% (309/1408)\n",
            "11 469 Loss: 2.047 | Acc: 22.852% (351/1536)\n",
            "12 469 Loss: 2.031 | Acc: 23.438% (390/1664)\n",
            "13 469 Loss: 2.011 | Acc: 24.442% (438/1792)\n",
            "14 469 Loss: 1.991 | Acc: 25.156% (483/1920)\n",
            "15 469 Loss: 1.970 | Acc: 25.684% (526/2048)\n",
            "16 469 Loss: 1.942 | Acc: 26.562% (578/2176)\n",
            "17 469 Loss: 1.928 | Acc: 27.561% (635/2304)\n",
            "18 469 Loss: 1.906 | Acc: 28.495% (693/2432)\n",
            "19 469 Loss: 1.878 | Acc: 30.039% (769/2560)\n",
            "20 469 Loss: 1.858 | Acc: 31.399% (844/2688)\n",
            "21 469 Loss: 1.829 | Acc: 32.564% (917/2816)\n",
            "22 469 Loss: 1.802 | Acc: 33.696% (992/2944)\n",
            "23 469 Loss: 1.766 | Acc: 35.091% (1078/3072)\n",
            "24 469 Loss: 1.739 | Acc: 36.281% (1161/3200)\n",
            "25 469 Loss: 1.712 | Acc: 37.139% (1236/3328)\n",
            "26 469 Loss: 1.682 | Acc: 38.426% (1328/3456)\n",
            "27 469 Loss: 1.651 | Acc: 39.927% (1431/3584)\n",
            "28 469 Loss: 1.621 | Acc: 41.002% (1522/3712)\n",
            "29 469 Loss: 1.595 | Acc: 42.005% (1613/3840)\n",
            "30 469 Loss: 1.564 | Acc: 43.246% (1716/3968)\n",
            "31 469 Loss: 1.536 | Acc: 44.312% (1815/4096)\n",
            "32 469 Loss: 1.512 | Acc: 45.289% (1913/4224)\n",
            "33 469 Loss: 1.487 | Acc: 46.232% (2012/4352)\n",
            "34 469 Loss: 1.462 | Acc: 47.210% (2115/4480)\n",
            "35 469 Loss: 1.437 | Acc: 48.155% (2219/4608)\n",
            "36 469 Loss: 1.412 | Acc: 49.134% (2327/4736)\n",
            "37 469 Loss: 1.390 | Acc: 49.836% (2424/4864)\n",
            "38 469 Loss: 1.372 | Acc: 50.621% (2527/4992)\n",
            "39 469 Loss: 1.349 | Acc: 51.465% (2635/5120)\n",
            "40 469 Loss: 1.326 | Acc: 52.287% (2744/5248)\n",
            "41 469 Loss: 1.309 | Acc: 52.939% (2846/5376)\n",
            "42 469 Loss: 1.292 | Acc: 53.634% (2952/5504)\n",
            "43 469 Loss: 1.273 | Acc: 54.315% (3059/5632)\n",
            "44 469 Loss: 1.254 | Acc: 55.087% (3173/5760)\n",
            "45 469 Loss: 1.238 | Acc: 55.740% (3282/5888)\n",
            "46 469 Loss: 1.221 | Acc: 56.350% (3390/6016)\n",
            "47 469 Loss: 1.204 | Acc: 56.917% (3497/6144)\n",
            "48 469 Loss: 1.190 | Acc: 57.366% (3598/6272)\n",
            "49 469 Loss: 1.176 | Acc: 57.844% (3702/6400)\n",
            "50 469 Loss: 1.162 | Acc: 58.395% (3812/6528)\n",
            "51 469 Loss: 1.151 | Acc: 58.849% (3917/6656)\n",
            "52 469 Loss: 1.138 | Acc: 59.287% (4022/6784)\n",
            "53 469 Loss: 1.127 | Acc: 59.751% (4130/6912)\n",
            "54 469 Loss: 1.119 | Acc: 60.071% (4229/7040)\n",
            "55 469 Loss: 1.108 | Acc: 60.435% (4332/7168)\n",
            "56 469 Loss: 1.098 | Acc: 60.800% (4436/7296)\n",
            "57 469 Loss: 1.087 | Acc: 61.193% (4543/7424)\n",
            "58 469 Loss: 1.074 | Acc: 61.666% (4657/7552)\n",
            "59 469 Loss: 1.062 | Acc: 62.044% (4765/7680)\n",
            "60 469 Loss: 1.051 | Acc: 62.474% (4878/7808)\n",
            "61 469 Loss: 1.042 | Acc: 62.752% (4980/7936)\n",
            "62 469 Loss: 1.033 | Acc: 63.095% (5088/8064)\n",
            "63 469 Loss: 1.022 | Acc: 63.501% (5202/8192)\n",
            "64 469 Loss: 1.014 | Acc: 63.810% (5309/8320)\n",
            "65 469 Loss: 1.005 | Acc: 64.134% (5418/8448)\n",
            "66 469 Loss: 0.996 | Acc: 64.482% (5530/8576)\n",
            "67 469 Loss: 0.987 | Acc: 64.798% (5640/8704)\n",
            "68 469 Loss: 0.978 | Acc: 65.149% (5754/8832)\n",
            "69 469 Loss: 0.968 | Acc: 65.480% (5867/8960)\n",
            "70 469 Loss: 0.959 | Acc: 65.768% (5977/9088)\n",
            "71 469 Loss: 0.951 | Acc: 66.081% (6090/9216)\n",
            "72 469 Loss: 0.942 | Acc: 66.428% (6207/9344)\n",
            "73 469 Loss: 0.936 | Acc: 66.649% (6313/9472)\n",
            "74 469 Loss: 0.929 | Acc: 66.948% (6427/9600)\n",
            "75 469 Loss: 0.921 | Acc: 67.249% (6542/9728)\n",
            "76 469 Loss: 0.913 | Acc: 67.522% (6655/9856)\n",
            "77 469 Loss: 0.904 | Acc: 67.819% (6771/9984)\n",
            "78 469 Loss: 0.900 | Acc: 68.028% (6879/10112)\n",
            "79 469 Loss: 0.893 | Acc: 68.330% (6997/10240)\n",
            "80 469 Loss: 0.885 | Acc: 68.663% (7119/10368)\n",
            "81 469 Loss: 0.877 | Acc: 68.960% (7238/10496)\n",
            "82 469 Loss: 0.870 | Acc: 69.239% (7356/10624)\n",
            "83 469 Loss: 0.863 | Acc: 69.513% (7474/10752)\n",
            "84 469 Loss: 0.857 | Acc: 69.733% (7587/10880)\n",
            "85 469 Loss: 0.850 | Acc: 69.949% (7700/11008)\n",
            "86 469 Loss: 0.843 | Acc: 70.250% (7823/11136)\n",
            "87 469 Loss: 0.839 | Acc: 70.437% (7934/11264)\n",
            "88 469 Loss: 0.832 | Acc: 70.672% (8051/11392)\n",
            "89 469 Loss: 0.825 | Acc: 70.920% (8170/11520)\n",
            "90 469 Loss: 0.820 | Acc: 71.111% (8283/11648)\n",
            "91 469 Loss: 0.814 | Acc: 71.315% (8398/11776)\n",
            "92 469 Loss: 0.810 | Acc: 71.480% (8509/11904)\n",
            "93 469 Loss: 0.804 | Acc: 71.725% (8630/12032)\n",
            "94 469 Loss: 0.798 | Acc: 71.900% (8743/12160)\n",
            "95 469 Loss: 0.793 | Acc: 72.078% (8857/12288)\n",
            "96 469 Loss: 0.788 | Acc: 72.254% (8971/12416)\n",
            "97 469 Loss: 0.783 | Acc: 72.425% (9085/12544)\n",
            "98 469 Loss: 0.777 | Acc: 72.656% (9207/12672)\n",
            "99 469 Loss: 0.772 | Acc: 72.852% (9325/12800)\n",
            "100 469 Loss: 0.766 | Acc: 73.066% (9446/12928)\n",
            "101 469 Loss: 0.761 | Acc: 73.277% (9567/13056)\n",
            "102 469 Loss: 0.757 | Acc: 73.445% (9683/13184)\n",
            "103 469 Loss: 0.751 | Acc: 73.663% (9806/13312)\n",
            "104 469 Loss: 0.746 | Acc: 73.854% (9926/13440)\n",
            "105 469 Loss: 0.742 | Acc: 73.990% (10039/13568)\n",
            "106 469 Loss: 0.737 | Acc: 74.146% (10155/13696)\n",
            "107 469 Loss: 0.734 | Acc: 74.291% (10270/13824)\n",
            "108 469 Loss: 0.729 | Acc: 74.470% (10390/13952)\n",
            "109 469 Loss: 0.725 | Acc: 74.631% (10508/14080)\n",
            "110 469 Loss: 0.720 | Acc: 74.817% (10630/14208)\n",
            "111 469 Loss: 0.715 | Acc: 74.993% (10751/14336)\n",
            "112 469 Loss: 0.711 | Acc: 75.145% (10869/14464)\n",
            "113 469 Loss: 0.707 | Acc: 75.308% (10989/14592)\n",
            "114 469 Loss: 0.702 | Acc: 75.489% (11112/14720)\n",
            "115 469 Loss: 0.698 | Acc: 75.660% (11234/14848)\n",
            "116 469 Loss: 0.693 | Acc: 75.821% (11355/14976)\n",
            "117 469 Loss: 0.690 | Acc: 75.973% (11475/15104)\n",
            "118 469 Loss: 0.685 | Acc: 76.136% (11597/15232)\n",
            "119 469 Loss: 0.682 | Acc: 76.257% (11713/15360)\n",
            "120 469 Loss: 0.678 | Acc: 76.382% (11830/15488)\n",
            "121 469 Loss: 0.674 | Acc: 76.530% (11951/15616)\n",
            "122 469 Loss: 0.671 | Acc: 76.658% (12069/15744)\n",
            "123 469 Loss: 0.667 | Acc: 76.821% (12193/15872)\n",
            "124 469 Loss: 0.663 | Acc: 76.950% (12312/16000)\n",
            "125 469 Loss: 0.660 | Acc: 77.083% (12432/16128)\n",
            "126 469 Loss: 0.656 | Acc: 77.208% (12551/16256)\n",
            "127 469 Loss: 0.652 | Acc: 77.356% (12674/16384)\n",
            "128 469 Loss: 0.649 | Acc: 77.489% (12795/16512)\n",
            "129 469 Loss: 0.646 | Acc: 77.608% (12914/16640)\n",
            "130 469 Loss: 0.642 | Acc: 77.767% (13040/16768)\n",
            "131 469 Loss: 0.639 | Acc: 77.871% (13157/16896)\n",
            "132 469 Loss: 0.636 | Acc: 77.961% (13272/17024)\n",
            "133 469 Loss: 0.633 | Acc: 78.073% (13391/17152)\n",
            "134 469 Loss: 0.629 | Acc: 78.218% (13516/17280)\n",
            "135 469 Loss: 0.627 | Acc: 78.309% (13632/17408)\n",
            "136 469 Loss: 0.623 | Acc: 78.439% (13755/17536)\n",
            "137 469 Loss: 0.620 | Acc: 78.555% (13876/17664)\n",
            "138 469 Loss: 0.617 | Acc: 78.659% (13995/17792)\n",
            "139 469 Loss: 0.614 | Acc: 78.783% (14118/17920)\n",
            "140 469 Loss: 0.611 | Acc: 78.867% (14234/18048)\n",
            "141 469 Loss: 0.609 | Acc: 78.945% (14349/18176)\n",
            "142 469 Loss: 0.606 | Acc: 79.076% (14474/18304)\n",
            "143 469 Loss: 0.602 | Acc: 79.210% (14600/18432)\n",
            "144 469 Loss: 0.599 | Acc: 79.310% (14720/18560)\n",
            "145 469 Loss: 0.596 | Acc: 79.425% (14843/18688)\n",
            "146 469 Loss: 0.594 | Acc: 79.528% (14964/18816)\n",
            "147 469 Loss: 0.591 | Acc: 79.619% (15083/18944)\n",
            "148 469 Loss: 0.588 | Acc: 79.724% (15205/19072)\n",
            "149 469 Loss: 0.586 | Acc: 79.807% (15323/19200)\n",
            "150 469 Loss: 0.583 | Acc: 79.915% (15446/19328)\n",
            "151 469 Loss: 0.580 | Acc: 80.011% (15567/19456)\n",
            "152 469 Loss: 0.578 | Acc: 80.096% (15686/19584)\n",
            "153 469 Loss: 0.575 | Acc: 80.185% (15806/19712)\n",
            "154 469 Loss: 0.573 | Acc: 80.272% (15926/19840)\n",
            "155 469 Loss: 0.570 | Acc: 80.354% (16045/19968)\n",
            "156 469 Loss: 0.568 | Acc: 80.444% (16166/20096)\n",
            "157 469 Loss: 0.566 | Acc: 80.503% (16281/20224)\n",
            "158 469 Loss: 0.563 | Acc: 80.616% (16407/20352)\n",
            "159 469 Loss: 0.562 | Acc: 80.674% (16522/20480)\n",
            "160 469 Loss: 0.559 | Acc: 80.750% (16641/20608)\n",
            "161 469 Loss: 0.557 | Acc: 80.821% (16759/20736)\n",
            "162 469 Loss: 0.555 | Acc: 80.905% (16880/20864)\n",
            "163 469 Loss: 0.553 | Acc: 80.993% (17002/20992)\n",
            "164 469 Loss: 0.551 | Acc: 81.065% (17121/21120)\n",
            "165 469 Loss: 0.548 | Acc: 81.161% (17245/21248)\n",
            "166 469 Loss: 0.546 | Acc: 81.245% (17367/21376)\n",
            "167 469 Loss: 0.544 | Acc: 81.315% (17486/21504)\n",
            "168 469 Loss: 0.541 | Acc: 81.403% (17609/21632)\n",
            "169 469 Loss: 0.539 | Acc: 81.475% (17729/21760)\n",
            "170 469 Loss: 0.537 | Acc: 81.538% (17847/21888)\n",
            "171 469 Loss: 0.534 | Acc: 81.622% (17970/22016)\n",
            "172 469 Loss: 0.532 | Acc: 81.702% (18092/22144)\n",
            "173 469 Loss: 0.530 | Acc: 81.771% (18212/22272)\n",
            "174 469 Loss: 0.529 | Acc: 81.808% (18325/22400)\n",
            "175 469 Loss: 0.527 | Acc: 81.867% (18443/22528)\n",
            "176 469 Loss: 0.526 | Acc: 81.925% (18561/22656)\n",
            "177 469 Loss: 0.524 | Acc: 81.983% (18679/22784)\n",
            "178 469 Loss: 0.522 | Acc: 82.040% (18797/22912)\n",
            "179 469 Loss: 0.521 | Acc: 82.092% (18914/23040)\n",
            "180 469 Loss: 0.519 | Acc: 82.152% (19033/23168)\n",
            "181 469 Loss: 0.518 | Acc: 82.194% (19148/23296)\n",
            "182 469 Loss: 0.516 | Acc: 82.253% (19267/23424)\n",
            "183 469 Loss: 0.514 | Acc: 82.307% (19385/23552)\n",
            "184 469 Loss: 0.512 | Acc: 82.365% (19504/23680)\n",
            "185 469 Loss: 0.511 | Acc: 82.434% (19626/23808)\n",
            "186 469 Loss: 0.508 | Acc: 82.512% (19750/23936)\n",
            "187 469 Loss: 0.506 | Acc: 82.576% (19871/24064)\n",
            "188 469 Loss: 0.505 | Acc: 82.631% (19990/24192)\n",
            "189 469 Loss: 0.503 | Acc: 82.697% (20112/24320)\n",
            "190 469 Loss: 0.501 | Acc: 82.763% (20234/24448)\n",
            "191 469 Loss: 0.499 | Acc: 82.829% (20356/24576)\n",
            "192 469 Loss: 0.497 | Acc: 82.889% (20477/24704)\n",
            "193 469 Loss: 0.496 | Acc: 82.945% (20597/24832)\n",
            "194 469 Loss: 0.494 | Acc: 83.005% (20718/24960)\n",
            "195 469 Loss: 0.492 | Acc: 83.068% (20840/25088)\n",
            "196 469 Loss: 0.490 | Acc: 83.130% (20962/25216)\n",
            "197 469 Loss: 0.489 | Acc: 83.191% (21084/25344)\n",
            "198 469 Loss: 0.487 | Acc: 83.248% (21205/25472)\n",
            "199 469 Loss: 0.486 | Acc: 83.293% (21323/25600)\n",
            "200 469 Loss: 0.484 | Acc: 83.349% (21444/25728)\n",
            "201 469 Loss: 0.483 | Acc: 83.385% (21560/25856)\n",
            "202 469 Loss: 0.481 | Acc: 83.448% (21683/25984)\n",
            "203 469 Loss: 0.479 | Acc: 83.525% (21810/26112)\n",
            "204 469 Loss: 0.478 | Acc: 83.579% (21931/26240)\n",
            "205 469 Loss: 0.476 | Acc: 83.643% (22055/26368)\n",
            "206 469 Loss: 0.474 | Acc: 83.699% (22177/26496)\n",
            "207 469 Loss: 0.473 | Acc: 83.759% (22300/26624)\n",
            "208 469 Loss: 0.471 | Acc: 83.826% (22425/26752)\n",
            "209 469 Loss: 0.469 | Acc: 83.891% (22550/26880)\n",
            "210 469 Loss: 0.468 | Acc: 83.945% (22672/27008)\n",
            "211 469 Loss: 0.466 | Acc: 84.010% (22797/27136)\n",
            "212 469 Loss: 0.464 | Acc: 84.063% (22919/27264)\n",
            "213 469 Loss: 0.463 | Acc: 84.112% (23040/27392)\n",
            "214 469 Loss: 0.461 | Acc: 84.175% (23165/27520)\n",
            "215 469 Loss: 0.460 | Acc: 84.230% (23288/27648)\n",
            "216 469 Loss: 0.458 | Acc: 84.292% (23413/27776)\n",
            "217 469 Loss: 0.457 | Acc: 84.346% (23536/27904)\n",
            "218 469 Loss: 0.456 | Acc: 84.396% (23658/28032)\n",
            "219 469 Loss: 0.455 | Acc: 84.432% (23776/28160)\n",
            "220 469 Loss: 0.454 | Acc: 84.467% (23894/28288)\n",
            "221 469 Loss: 0.453 | Acc: 84.526% (24019/28416)\n",
            "222 469 Loss: 0.451 | Acc: 84.589% (24145/28544)\n",
            "223 469 Loss: 0.450 | Acc: 84.626% (24264/28672)\n",
            "224 469 Loss: 0.449 | Acc: 84.677% (24387/28800)\n",
            "225 469 Loss: 0.447 | Acc: 84.728% (24510/28928)\n",
            "226 469 Loss: 0.446 | Acc: 84.778% (24633/29056)\n",
            "227 469 Loss: 0.445 | Acc: 84.796% (24747/29184)\n",
            "228 469 Loss: 0.444 | Acc: 84.846% (24870/29312)\n",
            "229 469 Loss: 0.442 | Acc: 84.885% (24990/29440)\n",
            "230 469 Loss: 0.441 | Acc: 84.933% (25113/29568)\n",
            "231 469 Loss: 0.440 | Acc: 84.978% (25235/29696)\n",
            "232 469 Loss: 0.438 | Acc: 85.019% (25356/29824)\n",
            "233 469 Loss: 0.437 | Acc: 85.059% (25477/29952)\n",
            "234 469 Loss: 0.436 | Acc: 85.100% (25598/30080)\n",
            "235 469 Loss: 0.435 | Acc: 85.143% (25720/30208)\n",
            "236 469 Loss: 0.434 | Acc: 85.186% (25842/30336)\n",
            "237 469 Loss: 0.432 | Acc: 85.238% (25967/30464)\n",
            "238 469 Loss: 0.431 | Acc: 85.290% (26092/30592)\n",
            "239 469 Loss: 0.430 | Acc: 85.322% (26211/30720)\n",
            "240 469 Loss: 0.428 | Acc: 85.370% (26335/30848)\n",
            "241 469 Loss: 0.427 | Acc: 85.405% (26455/30976)\n",
            "242 469 Loss: 0.426 | Acc: 85.455% (26580/31104)\n",
            "243 469 Loss: 0.425 | Acc: 85.505% (26705/31232)\n",
            "244 469 Loss: 0.423 | Acc: 85.552% (26829/31360)\n",
            "245 469 Loss: 0.422 | Acc: 85.585% (26949/31488)\n",
            "246 469 Loss: 0.421 | Acc: 85.634% (27074/31616)\n",
            "247 469 Loss: 0.419 | Acc: 85.676% (27197/31744)\n",
            "248 469 Loss: 0.418 | Acc: 85.718% (27320/31872)\n",
            "249 469 Loss: 0.417 | Acc: 85.763% (27444/32000)\n",
            "250 469 Loss: 0.416 | Acc: 85.804% (27567/32128)\n",
            "251 469 Loss: 0.415 | Acc: 85.851% (27692/32256)\n",
            "252 469 Loss: 0.414 | Acc: 85.885% (27813/32384)\n",
            "253 469 Loss: 0.413 | Acc: 85.922% (27935/32512)\n",
            "254 469 Loss: 0.411 | Acc: 85.968% (28060/32640)\n",
            "255 469 Loss: 0.410 | Acc: 86.008% (28183/32768)\n",
            "256 469 Loss: 0.409 | Acc: 86.044% (28305/32896)\n",
            "257 469 Loss: 0.408 | Acc: 86.080% (28427/33024)\n",
            "258 469 Loss: 0.407 | Acc: 86.115% (28549/33152)\n",
            "259 469 Loss: 0.406 | Acc: 86.148% (28670/33280)\n",
            "260 469 Loss: 0.404 | Acc: 86.195% (28796/33408)\n",
            "261 469 Loss: 0.403 | Acc: 86.242% (28922/33536)\n",
            "262 469 Loss: 0.402 | Acc: 86.273% (29043/33664)\n",
            "263 469 Loss: 0.401 | Acc: 86.316% (29168/33792)\n",
            "264 469 Loss: 0.400 | Acc: 86.353% (29291/33920)\n",
            "265 469 Loss: 0.399 | Acc: 86.393% (29415/34048)\n",
            "266 469 Loss: 0.398 | Acc: 86.423% (29536/34176)\n",
            "267 469 Loss: 0.397 | Acc: 86.448% (29655/34304)\n",
            "268 469 Loss: 0.396 | Acc: 86.481% (29777/34432)\n",
            "269 469 Loss: 0.395 | Acc: 86.516% (29900/34560)\n",
            "270 469 Loss: 0.394 | Acc: 86.549% (30022/34688)\n",
            "271 469 Loss: 0.393 | Acc: 86.578% (30143/34816)\n",
            "272 469 Loss: 0.392 | Acc: 86.619% (30268/34944)\n",
            "273 469 Loss: 0.391 | Acc: 86.662% (30394/35072)\n",
            "274 469 Loss: 0.390 | Acc: 86.696% (30517/35200)\n",
            "275 469 Loss: 0.389 | Acc: 86.736% (30642/35328)\n",
            "276 469 Loss: 0.388 | Acc: 86.770% (30765/35456)\n",
            "277 469 Loss: 0.387 | Acc: 86.803% (30888/35584)\n",
            "278 469 Loss: 0.386 | Acc: 86.831% (31009/35712)\n",
            "279 469 Loss: 0.385 | Acc: 86.867% (31133/35840)\n",
            "280 469 Loss: 0.384 | Acc: 86.897% (31255/35968)\n",
            "281 469 Loss: 0.383 | Acc: 86.929% (31378/36096)\n",
            "282 469 Loss: 0.383 | Acc: 86.959% (31500/36224)\n",
            "283 469 Loss: 0.382 | Acc: 86.994% (31624/36352)\n",
            "284 469 Loss: 0.381 | Acc: 87.020% (31745/36480)\n",
            "285 469 Loss: 0.380 | Acc: 87.052% (31868/36608)\n",
            "286 469 Loss: 0.379 | Acc: 87.084% (31991/36736)\n",
            "287 469 Loss: 0.378 | Acc: 87.123% (32117/36864)\n",
            "288 469 Loss: 0.377 | Acc: 87.146% (32237/36992)\n",
            "289 469 Loss: 0.377 | Acc: 87.179% (32361/37120)\n",
            "290 469 Loss: 0.376 | Acc: 87.210% (32484/37248)\n",
            "291 469 Loss: 0.375 | Acc: 87.232% (32604/37376)\n",
            "292 469 Loss: 0.374 | Acc: 87.265% (32728/37504)\n",
            "293 469 Loss: 0.373 | Acc: 87.295% (32851/37632)\n",
            "294 469 Loss: 0.372 | Acc: 87.317% (32971/37760)\n",
            "295 469 Loss: 0.371 | Acc: 87.344% (33093/37888)\n",
            "296 469 Loss: 0.370 | Acc: 87.376% (33217/38016)\n",
            "297 469 Loss: 0.369 | Acc: 87.413% (33343/38144)\n",
            "298 469 Loss: 0.369 | Acc: 87.432% (33462/38272)\n",
            "299 469 Loss: 0.368 | Acc: 87.458% (33584/38400)\n",
            "300 469 Loss: 0.367 | Acc: 87.495% (33710/38528)\n",
            "301 469 Loss: 0.366 | Acc: 87.526% (33834/38656)\n",
            "302 469 Loss: 0.365 | Acc: 87.559% (33959/38784)\n",
            "303 469 Loss: 0.364 | Acc: 87.577% (34078/38912)\n",
            "304 469 Loss: 0.364 | Acc: 87.600% (34199/39040)\n",
            "305 469 Loss: 0.363 | Acc: 87.635% (34325/39168)\n",
            "306 469 Loss: 0.362 | Acc: 87.660% (34447/39296)\n",
            "307 469 Loss: 0.361 | Acc: 87.690% (34571/39424)\n",
            "308 469 Loss: 0.360 | Acc: 87.720% (34695/39552)\n",
            "309 469 Loss: 0.359 | Acc: 87.742% (34816/39680)\n",
            "310 469 Loss: 0.359 | Acc: 87.769% (34939/39808)\n",
            "311 469 Loss: 0.358 | Acc: 87.788% (35059/39936)\n",
            "312 469 Loss: 0.357 | Acc: 87.819% (35184/40064)\n",
            "313 469 Loss: 0.357 | Acc: 87.841% (35305/40192)\n",
            "314 469 Loss: 0.356 | Acc: 87.862% (35426/40320)\n",
            "315 469 Loss: 0.355 | Acc: 87.886% (35548/40448)\n",
            "316 469 Loss: 0.355 | Acc: 87.909% (35670/40576)\n",
            "317 469 Loss: 0.354 | Acc: 87.937% (35794/40704)\n",
            "318 469 Loss: 0.353 | Acc: 87.968% (35919/40832)\n",
            "319 469 Loss: 0.352 | Acc: 87.993% (36042/40960)\n",
            "320 469 Loss: 0.351 | Acc: 88.018% (36165/41088)\n",
            "321 469 Loss: 0.351 | Acc: 88.043% (36288/41216)\n",
            "322 469 Loss: 0.350 | Acc: 88.068% (36411/41344)\n",
            "323 469 Loss: 0.349 | Acc: 88.096% (36535/41472)\n",
            "324 469 Loss: 0.349 | Acc: 88.115% (36656/41600)\n",
            "325 469 Loss: 0.348 | Acc: 88.135% (36777/41728)\n",
            "326 469 Loss: 0.347 | Acc: 88.164% (36902/41856)\n",
            "327 469 Loss: 0.347 | Acc: 88.176% (37020/41984)\n",
            "328 469 Loss: 0.346 | Acc: 88.212% (37148/42112)\n",
            "329 469 Loss: 0.345 | Acc: 88.224% (37266/42240)\n",
            "330 469 Loss: 0.345 | Acc: 88.248% (37389/42368)\n",
            "331 469 Loss: 0.344 | Acc: 88.265% (37509/42496)\n",
            "332 469 Loss: 0.344 | Acc: 88.281% (37629/42624)\n",
            "333 469 Loss: 0.343 | Acc: 88.312% (37755/42752)\n",
            "334 469 Loss: 0.342 | Acc: 88.340% (37880/42880)\n",
            "335 469 Loss: 0.341 | Acc: 88.365% (38004/43008)\n",
            "336 469 Loss: 0.341 | Acc: 88.390% (38128/43136)\n",
            "337 469 Loss: 0.340 | Acc: 88.411% (38250/43264)\n",
            "338 469 Loss: 0.339 | Acc: 88.438% (38375/43392)\n",
            "339 469 Loss: 0.338 | Acc: 88.467% (38501/43520)\n",
            "340 469 Loss: 0.338 | Acc: 88.492% (38625/43648)\n",
            "341 469 Loss: 0.337 | Acc: 88.512% (38747/43776)\n",
            "342 469 Loss: 0.336 | Acc: 88.541% (38873/43904)\n",
            "343 469 Loss: 0.336 | Acc: 88.567% (38998/44032)\n",
            "344 469 Loss: 0.335 | Acc: 88.589% (39121/44160)\n",
            "345 469 Loss: 0.334 | Acc: 88.618% (39247/44288)\n",
            "346 469 Loss: 0.334 | Acc: 88.641% (39371/44416)\n",
            "347 469 Loss: 0.333 | Acc: 88.667% (39496/44544)\n",
            "348 469 Loss: 0.332 | Acc: 88.684% (39617/44672)\n",
            "349 469 Loss: 0.332 | Acc: 88.708% (39741/44800)\n",
            "350 469 Loss: 0.331 | Acc: 88.731% (39865/44928)\n",
            "351 469 Loss: 0.330 | Acc: 88.758% (39991/45056)\n",
            "352 469 Loss: 0.330 | Acc: 88.779% (40114/45184)\n",
            "353 469 Loss: 0.329 | Acc: 88.806% (40240/45312)\n",
            "354 469 Loss: 0.328 | Acc: 88.820% (40360/45440)\n",
            "355 469 Loss: 0.328 | Acc: 88.845% (40485/45568)\n",
            "356 469 Loss: 0.327 | Acc: 88.870% (40610/45696)\n",
            "357 469 Loss: 0.327 | Acc: 88.881% (40729/45824)\n",
            "358 469 Loss: 0.326 | Acc: 88.901% (40852/45952)\n",
            "359 469 Loss: 0.325 | Acc: 88.924% (40976/46080)\n",
            "360 469 Loss: 0.325 | Acc: 88.950% (41102/46208)\n",
            "361 469 Loss: 0.324 | Acc: 88.974% (41227/46336)\n",
            "362 469 Loss: 0.323 | Acc: 89.000% (41353/46464)\n",
            "363 469 Loss: 0.322 | Acc: 89.026% (41479/46592)\n",
            "364 469 Loss: 0.322 | Acc: 89.052% (41605/46720)\n",
            "365 469 Loss: 0.321 | Acc: 89.075% (41730/46848)\n",
            "366 469 Loss: 0.321 | Acc: 89.088% (41850/46976)\n",
            "367 469 Loss: 0.320 | Acc: 89.107% (41973/47104)\n",
            "368 469 Loss: 0.320 | Acc: 89.130% (42098/47232)\n",
            "369 469 Loss: 0.319 | Acc: 89.149% (42221/47360)\n",
            "370 469 Loss: 0.319 | Acc: 89.176% (42348/47488)\n",
            "371 469 Loss: 0.318 | Acc: 89.195% (42471/47616)\n",
            "372 469 Loss: 0.317 | Acc: 89.222% (42598/47744)\n",
            "373 469 Loss: 0.317 | Acc: 89.242% (42722/47872)\n",
            "374 469 Loss: 0.317 | Acc: 89.254% (42842/48000)\n",
            "375 469 Loss: 0.316 | Acc: 89.272% (42965/48128)\n",
            "376 469 Loss: 0.315 | Acc: 89.297% (43091/48256)\n",
            "377 469 Loss: 0.315 | Acc: 89.315% (43214/48384)\n",
            "378 469 Loss: 0.314 | Acc: 89.337% (43339/48512)\n",
            "379 469 Loss: 0.313 | Acc: 89.363% (43466/48640)\n",
            "380 469 Loss: 0.313 | Acc: 89.382% (43590/48768)\n",
            "381 469 Loss: 0.312 | Acc: 89.400% (43713/48896)\n",
            "382 469 Loss: 0.312 | Acc: 89.415% (43835/49024)\n",
            "383 469 Loss: 0.311 | Acc: 89.433% (43958/49152)\n",
            "384 469 Loss: 0.311 | Acc: 89.458% (44085/49280)\n",
            "385 469 Loss: 0.310 | Acc: 89.477% (44209/49408)\n",
            "386 469 Loss: 0.310 | Acc: 89.497% (44333/49536)\n",
            "387 469 Loss: 0.309 | Acc: 89.514% (44456/49664)\n",
            "388 469 Loss: 0.308 | Acc: 89.538% (44583/49792)\n",
            "389 469 Loss: 0.308 | Acc: 89.555% (44706/49920)\n",
            "390 469 Loss: 0.307 | Acc: 89.572% (44829/50048)\n",
            "391 469 Loss: 0.307 | Acc: 89.589% (44952/50176)\n",
            "392 469 Loss: 0.306 | Acc: 89.605% (45075/50304)\n",
            "393 469 Loss: 0.306 | Acc: 89.622% (45198/50432)\n",
            "394 469 Loss: 0.305 | Acc: 89.636% (45320/50560)\n",
            "395 469 Loss: 0.305 | Acc: 89.658% (45446/50688)\n",
            "396 469 Loss: 0.304 | Acc: 89.680% (45572/50816)\n",
            "397 469 Loss: 0.304 | Acc: 89.698% (45696/50944)\n",
            "398 469 Loss: 0.303 | Acc: 89.709% (45816/51072)\n",
            "399 469 Loss: 0.303 | Acc: 89.730% (45942/51200)\n",
            "400 469 Loss: 0.302 | Acc: 89.739% (46061/51328)\n",
            "401 469 Loss: 0.302 | Acc: 89.752% (46183/51456)\n",
            "402 469 Loss: 0.301 | Acc: 89.770% (46307/51584)\n",
            "403 469 Loss: 0.301 | Acc: 89.790% (46432/51712)\n",
            "404 469 Loss: 0.300 | Acc: 89.811% (46558/51840)\n",
            "405 469 Loss: 0.300 | Acc: 89.830% (46683/51968)\n",
            "406 469 Loss: 0.299 | Acc: 89.846% (46806/52096)\n",
            "407 469 Loss: 0.299 | Acc: 89.861% (46929/52224)\n",
            "408 469 Loss: 0.299 | Acc: 89.870% (47049/52352)\n",
            "409 469 Loss: 0.298 | Acc: 89.886% (47172/52480)\n",
            "410 469 Loss: 0.298 | Acc: 89.906% (47298/52608)\n",
            "411 469 Loss: 0.297 | Acc: 89.921% (47421/52736)\n",
            "412 469 Loss: 0.297 | Acc: 89.929% (47540/52864)\n",
            "413 469 Loss: 0.297 | Acc: 89.946% (47664/52992)\n",
            "414 469 Loss: 0.296 | Acc: 89.962% (47788/53120)\n",
            "415 469 Loss: 0.296 | Acc: 89.983% (47914/53248)\n",
            "416 469 Loss: 0.295 | Acc: 90.003% (48040/53376)\n",
            "417 469 Loss: 0.295 | Acc: 90.019% (48164/53504)\n",
            "418 469 Loss: 0.294 | Acc: 90.032% (48286/53632)\n",
            "419 469 Loss: 0.294 | Acc: 90.056% (48414/53760)\n",
            "420 469 Loss: 0.293 | Acc: 90.076% (48540/53888)\n",
            "421 469 Loss: 0.293 | Acc: 90.090% (48663/54016)\n",
            "422 469 Loss: 0.292 | Acc: 90.110% (48789/54144)\n",
            "423 469 Loss: 0.292 | Acc: 90.128% (48914/54272)\n",
            "424 469 Loss: 0.291 | Acc: 90.142% (49037/54400)\n",
            "425 469 Loss: 0.291 | Acc: 90.154% (49159/54528)\n",
            "426 469 Loss: 0.291 | Acc: 90.171% (49284/54656)\n",
            "427 469 Loss: 0.290 | Acc: 90.192% (49411/54784)\n",
            "428 469 Loss: 0.289 | Acc: 90.212% (49537/54912)\n",
            "429 469 Loss: 0.289 | Acc: 90.231% (49663/55040)\n",
            "430 469 Loss: 0.288 | Acc: 90.248% (49788/55168)\n",
            "431 469 Loss: 0.288 | Acc: 90.258% (49909/55296)\n",
            "432 469 Loss: 0.287 | Acc: 90.277% (50035/55424)\n",
            "433 469 Loss: 0.287 | Acc: 90.290% (50158/55552)\n",
            "434 469 Loss: 0.286 | Acc: 90.311% (50285/55680)\n",
            "435 469 Loss: 0.286 | Acc: 90.320% (50406/55808)\n",
            "436 469 Loss: 0.286 | Acc: 90.335% (50530/55936)\n",
            "437 469 Loss: 0.285 | Acc: 90.349% (50653/56064)\n",
            "438 469 Loss: 0.285 | Acc: 90.363% (50777/56192)\n",
            "439 469 Loss: 0.285 | Acc: 90.378% (50901/56320)\n",
            "440 469 Loss: 0.284 | Acc: 90.395% (51026/56448)\n",
            "441 469 Loss: 0.284 | Acc: 90.408% (51149/56576)\n",
            "442 469 Loss: 0.284 | Acc: 90.420% (51272/56704)\n",
            "443 469 Loss: 0.283 | Acc: 90.433% (51395/56832)\n",
            "444 469 Loss: 0.283 | Acc: 90.446% (51518/56960)\n",
            "445 469 Loss: 0.283 | Acc: 90.460% (51642/57088)\n",
            "446 469 Loss: 0.282 | Acc: 90.473% (51765/57216)\n",
            "447 469 Loss: 0.282 | Acc: 90.484% (51887/57344)\n",
            "448 469 Loss: 0.281 | Acc: 90.491% (52007/57472)\n",
            "449 469 Loss: 0.281 | Acc: 90.505% (52131/57600)\n",
            "450 469 Loss: 0.281 | Acc: 90.519% (52255/57728)\n",
            "451 469 Loss: 0.280 | Acc: 90.530% (52377/57856)\n",
            "452 469 Loss: 0.280 | Acc: 90.544% (52501/57984)\n",
            "453 469 Loss: 0.280 | Acc: 90.553% (52622/58112)\n",
            "454 469 Loss: 0.279 | Acc: 90.561% (52743/58240)\n",
            "455 469 Loss: 0.279 | Acc: 90.577% (52868/58368)\n",
            "456 469 Loss: 0.279 | Acc: 90.589% (52991/58496)\n",
            "457 469 Loss: 0.278 | Acc: 90.601% (53114/58624)\n",
            "458 469 Loss: 0.278 | Acc: 90.610% (53235/58752)\n",
            "459 469 Loss: 0.278 | Acc: 90.622% (53358/58880)\n",
            "460 469 Loss: 0.277 | Acc: 90.637% (53483/59008)\n",
            "461 469 Loss: 0.277 | Acc: 90.650% (53607/59136)\n",
            "462 469 Loss: 0.277 | Acc: 90.659% (53728/59264)\n",
            "463 469 Loss: 0.276 | Acc: 90.670% (53851/59392)\n",
            "464 469 Loss: 0.276 | Acc: 90.689% (53978/59520)\n",
            "465 469 Loss: 0.276 | Acc: 90.697% (54099/59648)\n",
            "466 469 Loss: 0.275 | Acc: 90.712% (54224/59776)\n",
            "467 469 Loss: 0.275 | Acc: 90.720% (54345/59904)\n",
            "468 469 Loss: 0.274 | Acc: 90.733% (54440/60000)\n"
          ]
        }
      ]
    }
  ]
}