{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrpmJVx+KqBig2+G4rQbrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_cifar_batch128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox_FE_xeDz6F",
        "outputId": "7a519357-35fc-46ee-8262-5b7292413b9a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svako osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1, za cifar 3\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    #transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    #transforms.Normalize((0.1307,), (0.3081,)), ovo je za mnist\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "#trainset = torchvision.datasets.MNIST(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "          'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#classes = ('0', '1', '2', '3', '4',\n",
        "#           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "       \n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+200):\n",
        "train(start_epoch)\n",
        "#    scheduler.step()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.533 | Acc: 7.812% (10/128)\n",
            "1 391 Loss: 2.411 | Acc: 10.547% (27/256)\n",
            "2 391 Loss: 2.366 | Acc: 12.500% (48/384)\n",
            "3 391 Loss: 2.343 | Acc: 13.086% (67/512)\n",
            "4 391 Loss: 2.358 | Acc: 13.906% (89/640)\n",
            "5 391 Loss: 2.322 | Acc: 14.974% (115/768)\n",
            "6 391 Loss: 2.296 | Acc: 15.960% (143/896)\n",
            "7 391 Loss: 2.286 | Acc: 17.480% (179/1024)\n",
            "8 391 Loss: 2.269 | Acc: 18.490% (213/1152)\n",
            "9 391 Loss: 2.239 | Acc: 19.297% (247/1280)\n",
            "10 391 Loss: 2.222 | Acc: 19.389% (273/1408)\n",
            "11 391 Loss: 2.210 | Acc: 19.987% (307/1536)\n",
            "12 391 Loss: 2.201 | Acc: 20.613% (343/1664)\n",
            "13 391 Loss: 2.187 | Acc: 20.871% (374/1792)\n",
            "14 391 Loss: 2.178 | Acc: 21.250% (408/1920)\n",
            "15 391 Loss: 2.162 | Acc: 21.680% (444/2048)\n",
            "16 391 Loss: 2.150 | Acc: 21.875% (476/2176)\n",
            "17 391 Loss: 2.138 | Acc: 22.005% (507/2304)\n",
            "18 391 Loss: 2.131 | Acc: 22.451% (546/2432)\n",
            "19 391 Loss: 2.121 | Acc: 22.461% (575/2560)\n",
            "20 391 Loss: 2.114 | Acc: 22.731% (611/2688)\n",
            "21 391 Loss: 2.107 | Acc: 22.869% (644/2816)\n",
            "22 391 Loss: 2.107 | Acc: 22.792% (671/2944)\n",
            "23 391 Loss: 2.101 | Acc: 22.852% (702/3072)\n",
            "24 391 Loss: 2.097 | Acc: 23.062% (738/3200)\n",
            "25 391 Loss: 2.089 | Acc: 23.167% (771/3328)\n",
            "26 391 Loss: 2.078 | Acc: 23.438% (810/3456)\n",
            "27 391 Loss: 2.072 | Acc: 23.772% (852/3584)\n",
            "28 391 Loss: 2.070 | Acc: 23.842% (885/3712)\n",
            "29 391 Loss: 2.062 | Acc: 24.245% (931/3840)\n",
            "30 391 Loss: 2.054 | Acc: 24.496% (972/3968)\n",
            "31 391 Loss: 2.046 | Acc: 24.878% (1019/4096)\n",
            "32 391 Loss: 2.040 | Acc: 24.929% (1053/4224)\n",
            "33 391 Loss: 2.036 | Acc: 25.023% (1089/4352)\n",
            "34 391 Loss: 2.032 | Acc: 25.134% (1126/4480)\n",
            "35 391 Loss: 2.023 | Acc: 25.304% (1166/4608)\n",
            "36 391 Loss: 2.015 | Acc: 25.591% (1212/4736)\n",
            "37 391 Loss: 2.008 | Acc: 25.905% (1260/4864)\n",
            "38 391 Loss: 2.003 | Acc: 26.142% (1305/4992)\n",
            "39 391 Loss: 1.999 | Acc: 26.270% (1345/5120)\n",
            "40 391 Loss: 1.990 | Acc: 26.543% (1393/5248)\n",
            "41 391 Loss: 1.984 | Acc: 26.730% (1437/5376)\n",
            "42 391 Loss: 1.980 | Acc: 26.871% (1479/5504)\n",
            "43 391 Loss: 1.974 | Acc: 27.006% (1521/5632)\n",
            "44 391 Loss: 1.968 | Acc: 27.309% (1573/5760)\n",
            "45 391 Loss: 1.964 | Acc: 27.497% (1619/5888)\n",
            "46 391 Loss: 1.960 | Acc: 27.660% (1664/6016)\n",
            "47 391 Loss: 1.954 | Acc: 27.816% (1709/6144)\n",
            "48 391 Loss: 1.951 | Acc: 27.902% (1750/6272)\n",
            "49 391 Loss: 1.946 | Acc: 28.062% (1796/6400)\n",
            "50 391 Loss: 1.942 | Acc: 28.156% (1838/6528)\n",
            "51 391 Loss: 1.938 | Acc: 28.140% (1873/6656)\n",
            "52 391 Loss: 1.935 | Acc: 28.169% (1911/6784)\n",
            "53 391 Loss: 1.930 | Acc: 28.356% (1960/6912)\n",
            "54 391 Loss: 1.926 | Acc: 28.537% (2009/7040)\n",
            "55 391 Loss: 1.920 | Acc: 28.823% (2066/7168)\n",
            "56 391 Loss: 1.915 | Acc: 29.030% (2118/7296)\n",
            "57 391 Loss: 1.911 | Acc: 29.256% (2172/7424)\n",
            "58 391 Loss: 1.909 | Acc: 29.370% (2218/7552)\n",
            "59 391 Loss: 1.906 | Acc: 29.440% (2261/7680)\n",
            "60 391 Loss: 1.902 | Acc: 29.534% (2306/7808)\n",
            "61 391 Loss: 1.898 | Acc: 29.574% (2347/7936)\n",
            "62 391 Loss: 1.894 | Acc: 29.700% (2395/8064)\n",
            "63 391 Loss: 1.888 | Acc: 29.883% (2448/8192)\n",
            "64 391 Loss: 1.881 | Acc: 30.096% (2504/8320)\n",
            "65 391 Loss: 1.877 | Acc: 30.220% (2553/8448)\n",
            "66 391 Loss: 1.874 | Acc: 30.329% (2601/8576)\n",
            "67 391 Loss: 1.874 | Acc: 30.400% (2646/8704)\n",
            "68 391 Loss: 1.869 | Acc: 30.514% (2695/8832)\n",
            "69 391 Loss: 1.868 | Acc: 30.614% (2743/8960)\n",
            "70 391 Loss: 1.866 | Acc: 30.689% (2789/9088)\n",
            "71 391 Loss: 1.863 | Acc: 30.827% (2841/9216)\n",
            "72 391 Loss: 1.860 | Acc: 30.908% (2888/9344)\n",
            "73 391 Loss: 1.858 | Acc: 30.976% (2934/9472)\n",
            "74 391 Loss: 1.855 | Acc: 31.031% (2979/9600)\n",
            "75 391 Loss: 1.854 | Acc: 31.137% (3029/9728)\n",
            "76 391 Loss: 1.850 | Acc: 31.240% (3079/9856)\n",
            "77 391 Loss: 1.844 | Acc: 31.370% (3132/9984)\n",
            "78 391 Loss: 1.842 | Acc: 31.428% (3178/10112)\n",
            "79 391 Loss: 1.840 | Acc: 31.553% (3231/10240)\n",
            "80 391 Loss: 1.838 | Acc: 31.617% (3278/10368)\n",
            "81 391 Loss: 1.836 | Acc: 31.688% (3326/10496)\n",
            "82 391 Loss: 1.833 | Acc: 31.739% (3372/10624)\n",
            "83 391 Loss: 1.833 | Acc: 31.808% (3420/10752)\n",
            "84 391 Loss: 1.831 | Acc: 31.783% (3458/10880)\n",
            "85 391 Loss: 1.830 | Acc: 31.795% (3500/11008)\n",
            "86 391 Loss: 1.827 | Acc: 31.932% (3556/11136)\n",
            "87 391 Loss: 1.825 | Acc: 32.013% (3606/11264)\n",
            "88 391 Loss: 1.824 | Acc: 32.031% (3649/11392)\n",
            "89 391 Loss: 1.820 | Acc: 32.161% (3705/11520)\n",
            "90 391 Loss: 1.820 | Acc: 32.177% (3748/11648)\n",
            "91 391 Loss: 1.818 | Acc: 32.278% (3801/11776)\n",
            "92 391 Loss: 1.814 | Acc: 32.409% (3858/11904)\n",
            "93 391 Loss: 1.812 | Acc: 32.555% (3917/12032)\n",
            "94 391 Loss: 1.813 | Acc: 32.516% (3954/12160)\n",
            "95 391 Loss: 1.812 | Acc: 32.560% (4001/12288)\n",
            "96 391 Loss: 1.811 | Acc: 32.635% (4052/12416)\n",
            "97 391 Loss: 1.809 | Acc: 32.685% (4100/12544)\n",
            "98 391 Loss: 1.806 | Acc: 32.797% (4156/12672)\n",
            "99 391 Loss: 1.806 | Acc: 32.852% (4205/12800)\n",
            "100 391 Loss: 1.805 | Acc: 32.913% (4255/12928)\n",
            "101 391 Loss: 1.803 | Acc: 32.966% (4304/13056)\n",
            "102 391 Loss: 1.798 | Acc: 33.078% (4361/13184)\n",
            "103 391 Loss: 1.797 | Acc: 33.151% (4413/13312)\n",
            "104 391 Loss: 1.794 | Acc: 33.274% (4472/13440)\n",
            "105 391 Loss: 1.790 | Acc: 33.432% (4536/13568)\n",
            "106 391 Loss: 1.787 | Acc: 33.557% (4596/13696)\n",
            "107 391 Loss: 1.785 | Acc: 33.695% (4658/13824)\n",
            "108 391 Loss: 1.781 | Acc: 33.802% (4716/13952)\n",
            "109 391 Loss: 1.779 | Acc: 33.864% (4768/14080)\n",
            "110 391 Loss: 1.775 | Acc: 33.953% (4824/14208)\n",
            "111 391 Loss: 1.772 | Acc: 34.047% (4881/14336)\n",
            "112 391 Loss: 1.771 | Acc: 34.092% (4931/14464)\n",
            "113 391 Loss: 1.769 | Acc: 34.197% (4990/14592)\n",
            "114 391 Loss: 1.767 | Acc: 34.246% (5041/14720)\n",
            "115 391 Loss: 1.764 | Acc: 34.341% (5099/14848)\n",
            "116 391 Loss: 1.763 | Acc: 34.375% (5148/14976)\n",
            "117 391 Loss: 1.761 | Acc: 34.401% (5196/15104)\n",
            "118 391 Loss: 1.758 | Acc: 34.513% (5257/15232)\n",
            "119 391 Loss: 1.756 | Acc: 34.590% (5313/15360)\n",
            "120 391 Loss: 1.755 | Acc: 34.646% (5366/15488)\n",
            "121 391 Loss: 1.753 | Acc: 34.702% (5419/15616)\n",
            "122 391 Loss: 1.752 | Acc: 34.775% (5475/15744)\n",
            "123 391 Loss: 1.751 | Acc: 34.803% (5524/15872)\n",
            "124 391 Loss: 1.748 | Acc: 34.919% (5587/16000)\n",
            "125 391 Loss: 1.745 | Acc: 35.057% (5654/16128)\n",
            "126 391 Loss: 1.743 | Acc: 35.175% (5718/16256)\n",
            "127 391 Loss: 1.741 | Acc: 35.223% (5771/16384)\n",
            "128 391 Loss: 1.740 | Acc: 35.271% (5824/16512)\n",
            "129 391 Loss: 1.737 | Acc: 35.379% (5887/16640)\n",
            "130 391 Loss: 1.734 | Acc: 35.526% (5957/16768)\n",
            "131 391 Loss: 1.733 | Acc: 35.565% (6009/16896)\n",
            "132 391 Loss: 1.733 | Acc: 35.538% (6050/17024)\n",
            "133 391 Loss: 1.731 | Acc: 35.623% (6110/17152)\n",
            "134 391 Loss: 1.729 | Acc: 35.741% (6176/17280)\n",
            "135 391 Loss: 1.726 | Acc: 35.817% (6235/17408)\n",
            "136 391 Loss: 1.723 | Acc: 35.903% (6296/17536)\n",
            "137 391 Loss: 1.720 | Acc: 36.028% (6364/17664)\n",
            "138 391 Loss: 1.718 | Acc: 36.095% (6422/17792)\n",
            "139 391 Loss: 1.717 | Acc: 36.172% (6482/17920)\n",
            "140 391 Loss: 1.717 | Acc: 36.187% (6531/18048)\n",
            "141 391 Loss: 1.715 | Acc: 36.251% (6589/18176)\n",
            "142 391 Loss: 1.713 | Acc: 36.293% (6643/18304)\n",
            "143 391 Loss: 1.711 | Acc: 36.361% (6702/18432)\n",
            "144 391 Loss: 1.709 | Acc: 36.433% (6762/18560)\n",
            "145 391 Loss: 1.707 | Acc: 36.505% (6822/18688)\n",
            "146 391 Loss: 1.705 | Acc: 36.575% (6882/18816)\n",
            "147 391 Loss: 1.704 | Acc: 36.655% (6944/18944)\n",
            "148 391 Loss: 1.703 | Acc: 36.672% (6994/19072)\n",
            "149 391 Loss: 1.701 | Acc: 36.714% (7049/19200)\n",
            "150 391 Loss: 1.699 | Acc: 36.786% (7110/19328)\n",
            "151 391 Loss: 1.697 | Acc: 36.883% (7176/19456)\n",
            "152 391 Loss: 1.696 | Acc: 36.882% (7223/19584)\n",
            "153 391 Loss: 1.695 | Acc: 36.912% (7276/19712)\n",
            "154 391 Loss: 1.693 | Acc: 37.011% (7343/19840)\n",
            "155 391 Loss: 1.690 | Acc: 37.124% (7413/19968)\n",
            "156 391 Loss: 1.689 | Acc: 37.157% (7467/20096)\n",
            "157 391 Loss: 1.687 | Acc: 37.233% (7530/20224)\n",
            "158 391 Loss: 1.685 | Acc: 37.303% (7592/20352)\n",
            "159 391 Loss: 1.683 | Acc: 37.339% (7647/20480)\n",
            "160 391 Loss: 1.682 | Acc: 37.422% (7712/20608)\n",
            "161 391 Loss: 1.681 | Acc: 37.452% (7766/20736)\n",
            "162 391 Loss: 1.680 | Acc: 37.481% (7820/20864)\n",
            "163 391 Loss: 1.679 | Acc: 37.514% (7875/20992)\n",
            "164 391 Loss: 1.677 | Acc: 37.590% (7939/21120)\n",
            "165 391 Loss: 1.675 | Acc: 37.646% (7999/21248)\n",
            "166 391 Loss: 1.673 | Acc: 37.715% (8062/21376)\n",
            "167 391 Loss: 1.672 | Acc: 37.807% (8130/21504)\n",
            "168 391 Loss: 1.671 | Acc: 37.893% (8197/21632)\n",
            "169 391 Loss: 1.669 | Acc: 37.927% (8253/21760)\n",
            "170 391 Loss: 1.669 | Acc: 37.971% (8311/21888)\n",
            "171 391 Loss: 1.668 | Acc: 38.022% (8371/22016)\n",
            "172 391 Loss: 1.666 | Acc: 38.078% (8432/22144)\n",
            "173 391 Loss: 1.665 | Acc: 38.120% (8490/22272)\n",
            "174 391 Loss: 1.664 | Acc: 38.165% (8549/22400)\n",
            "175 391 Loss: 1.662 | Acc: 38.246% (8616/22528)\n",
            "176 391 Loss: 1.661 | Acc: 38.264% (8669/22656)\n",
            "177 391 Loss: 1.659 | Acc: 38.325% (8732/22784)\n",
            "178 391 Loss: 1.659 | Acc: 38.373% (8792/22912)\n",
            "179 391 Loss: 1.657 | Acc: 38.433% (8855/23040)\n",
            "180 391 Loss: 1.656 | Acc: 38.475% (8914/23168)\n",
            "181 391 Loss: 1.654 | Acc: 38.543% (8979/23296)\n",
            "182 391 Loss: 1.653 | Acc: 38.614% (9045/23424)\n",
            "183 391 Loss: 1.650 | Acc: 38.710% (9117/23552)\n",
            "184 391 Loss: 1.650 | Acc: 38.775% (9182/23680)\n",
            "185 391 Loss: 1.648 | Acc: 38.810% (9240/23808)\n",
            "186 391 Loss: 1.646 | Acc: 38.870% (9304/23936)\n",
            "187 391 Loss: 1.645 | Acc: 38.909% (9363/24064)\n",
            "188 391 Loss: 1.644 | Acc: 38.963% (9426/24192)\n",
            "189 391 Loss: 1.642 | Acc: 39.025% (9491/24320)\n",
            "190 391 Loss: 1.641 | Acc: 39.112% (9562/24448)\n",
            "191 391 Loss: 1.639 | Acc: 39.156% (9623/24576)\n",
            "192 391 Loss: 1.639 | Acc: 39.172% (9677/24704)\n",
            "193 391 Loss: 1.638 | Acc: 39.228% (9741/24832)\n",
            "194 391 Loss: 1.636 | Acc: 39.275% (9803/24960)\n",
            "195 391 Loss: 1.635 | Acc: 39.314% (9863/25088)\n",
            "196 391 Loss: 1.633 | Acc: 39.380% (9930/25216)\n",
            "197 391 Loss: 1.632 | Acc: 39.441% (9996/25344)\n",
            "198 391 Loss: 1.630 | Acc: 39.522% (10067/25472)\n",
            "199 391 Loss: 1.629 | Acc: 39.582% (10133/25600)\n",
            "200 391 Loss: 1.628 | Acc: 39.603% (10189/25728)\n",
            "201 391 Loss: 1.627 | Acc: 39.666% (10256/25856)\n",
            "202 391 Loss: 1.625 | Acc: 39.736% (10325/25984)\n",
            "203 391 Loss: 1.624 | Acc: 39.790% (10390/26112)\n",
            "204 391 Loss: 1.623 | Acc: 39.844% (10455/26240)\n",
            "205 391 Loss: 1.621 | Acc: 39.912% (10524/26368)\n",
            "206 391 Loss: 1.620 | Acc: 39.949% (10585/26496)\n",
            "207 391 Loss: 1.619 | Acc: 40.017% (10654/26624)\n",
            "208 391 Loss: 1.618 | Acc: 40.064% (10718/26752)\n",
            "209 391 Loss: 1.617 | Acc: 40.093% (10777/26880)\n",
            "210 391 Loss: 1.616 | Acc: 40.158% (10846/27008)\n",
            "211 391 Loss: 1.615 | Acc: 40.198% (10908/27136)\n",
            "212 391 Loss: 1.615 | Acc: 40.240% (10971/27264)\n",
            "213 391 Loss: 1.614 | Acc: 40.260% (11028/27392)\n",
            "214 391 Loss: 1.613 | Acc: 40.294% (11089/27520)\n",
            "215 391 Loss: 1.612 | Acc: 40.346% (11155/27648)\n",
            "216 391 Loss: 1.611 | Acc: 40.369% (11213/27776)\n",
            "217 391 Loss: 1.610 | Acc: 40.406% (11275/27904)\n",
            "218 391 Loss: 1.610 | Acc: 40.397% (11324/28032)\n",
            "219 391 Loss: 1.609 | Acc: 40.444% (11389/28160)\n",
            "220 391 Loss: 1.608 | Acc: 40.469% (11448/28288)\n",
            "221 391 Loss: 1.608 | Acc: 40.505% (11510/28416)\n",
            "222 391 Loss: 1.606 | Acc: 40.569% (11580/28544)\n",
            "223 391 Loss: 1.605 | Acc: 40.649% (11655/28672)\n",
            "224 391 Loss: 1.604 | Acc: 40.694% (11720/28800)\n",
            "225 391 Loss: 1.603 | Acc: 40.725% (11781/28928)\n",
            "226 391 Loss: 1.602 | Acc: 40.749% (11840/29056)\n",
            "227 391 Loss: 1.601 | Acc: 40.748% (11892/29184)\n",
            "228 391 Loss: 1.600 | Acc: 40.802% (11960/29312)\n",
            "229 391 Loss: 1.598 | Acc: 40.839% (12023/29440)\n",
            "230 391 Loss: 1.599 | Acc: 40.835% (12074/29568)\n",
            "231 391 Loss: 1.597 | Acc: 40.894% (12144/29696)\n",
            "232 391 Loss: 1.597 | Acc: 40.950% (12213/29824)\n",
            "233 391 Loss: 1.595 | Acc: 41.019% (12286/29952)\n",
            "234 391 Loss: 1.594 | Acc: 41.057% (12350/30080)\n",
            "235 391 Loss: 1.593 | Acc: 41.082% (12410/30208)\n",
            "236 391 Loss: 1.592 | Acc: 41.110% (12471/30336)\n",
            "237 391 Loss: 1.591 | Acc: 41.167% (12541/30464)\n",
            "238 391 Loss: 1.589 | Acc: 41.233% (12614/30592)\n",
            "239 391 Loss: 1.588 | Acc: 41.283% (12682/30720)\n",
            "240 391 Loss: 1.587 | Acc: 41.312% (12744/30848)\n",
            "241 391 Loss: 1.587 | Acc: 41.287% (12789/30976)\n",
            "242 391 Loss: 1.586 | Acc: 41.332% (12856/31104)\n",
            "243 391 Loss: 1.585 | Acc: 41.397% (12929/31232)\n",
            "244 391 Loss: 1.583 | Acc: 41.464% (13003/31360)\n",
            "245 391 Loss: 1.582 | Acc: 41.492% (13065/31488)\n",
            "246 391 Loss: 1.582 | Acc: 41.507% (13123/31616)\n",
            "247 391 Loss: 1.581 | Acc: 41.548% (13189/31744)\n",
            "248 391 Loss: 1.580 | Acc: 41.591% (13256/31872)\n",
            "249 391 Loss: 1.579 | Acc: 41.650% (13328/32000)\n",
            "250 391 Loss: 1.578 | Acc: 41.674% (13389/32128)\n",
            "251 391 Loss: 1.577 | Acc: 41.710% (13454/32256)\n",
            "252 391 Loss: 1.576 | Acc: 41.752% (13521/32384)\n",
            "253 391 Loss: 1.575 | Acc: 41.775% (13582/32512)\n",
            "254 391 Loss: 1.574 | Acc: 41.835% (13655/32640)\n",
            "255 391 Loss: 1.573 | Acc: 41.879% (13723/32768)\n",
            "256 391 Loss: 1.572 | Acc: 41.914% (13788/32896)\n",
            "257 391 Loss: 1.571 | Acc: 41.963% (13858/33024)\n",
            "258 391 Loss: 1.570 | Acc: 41.988% (13920/33152)\n",
            "259 391 Loss: 1.569 | Acc: 42.037% (13990/33280)\n",
            "260 391 Loss: 1.568 | Acc: 42.092% (14062/33408)\n",
            "261 391 Loss: 1.567 | Acc: 42.119% (14125/33536)\n",
            "262 391 Loss: 1.566 | Acc: 42.170% (14196/33664)\n",
            "263 391 Loss: 1.565 | Acc: 42.211% (14264/33792)\n",
            "264 391 Loss: 1.564 | Acc: 42.276% (14340/33920)\n",
            "265 391 Loss: 1.563 | Acc: 42.337% (14415/34048)\n",
            "266 391 Loss: 1.562 | Acc: 42.366% (14479/34176)\n",
            "267 391 Loss: 1.561 | Acc: 42.432% (14556/34304)\n",
            "268 391 Loss: 1.559 | Acc: 42.495% (14632/34432)\n",
            "269 391 Loss: 1.558 | Acc: 42.538% (14701/34560)\n",
            "270 391 Loss: 1.556 | Acc: 42.614% (14782/34688)\n",
            "271 391 Loss: 1.555 | Acc: 42.650% (14849/34816)\n",
            "272 391 Loss: 1.554 | Acc: 42.708% (14924/34944)\n",
            "273 391 Loss: 1.553 | Acc: 42.735% (14988/35072)\n",
            "274 391 Loss: 1.552 | Acc: 42.787% (15061/35200)\n",
            "275 391 Loss: 1.551 | Acc: 42.824% (15129/35328)\n",
            "276 391 Loss: 1.550 | Acc: 42.862% (15197/35456)\n",
            "277 391 Loss: 1.549 | Acc: 42.913% (15270/35584)\n",
            "278 391 Loss: 1.547 | Acc: 42.960% (15342/35712)\n",
            "279 391 Loss: 1.547 | Acc: 43.005% (15413/35840)\n",
            "280 391 Loss: 1.546 | Acc: 43.019% (15473/35968)\n",
            "281 391 Loss: 1.545 | Acc: 43.068% (15546/36096)\n",
            "282 391 Loss: 1.544 | Acc: 43.098% (15612/36224)\n",
            "283 391 Loss: 1.542 | Acc: 43.159% (15689/36352)\n",
            "284 391 Loss: 1.540 | Acc: 43.235% (15772/36480)\n",
            "285 391 Loss: 1.539 | Acc: 43.288% (15847/36608)\n",
            "286 391 Loss: 1.538 | Acc: 43.320% (15914/36736)\n",
            "287 391 Loss: 1.538 | Acc: 43.357% (15983/36864)\n",
            "288 391 Loss: 1.537 | Acc: 43.393% (16052/36992)\n",
            "289 391 Loss: 1.535 | Acc: 43.456% (16131/37120)\n",
            "290 391 Loss: 1.534 | Acc: 43.541% (16218/37248)\n",
            "291 391 Loss: 1.532 | Acc: 43.592% (16293/37376)\n",
            "292 391 Loss: 1.531 | Acc: 43.638% (16366/37504)\n",
            "293 391 Loss: 1.530 | Acc: 43.697% (16444/37632)\n",
            "294 391 Loss: 1.528 | Acc: 43.769% (16527/37760)\n",
            "295 391 Loss: 1.528 | Acc: 43.779% (16587/37888)\n",
            "296 391 Loss: 1.528 | Acc: 43.816% (16657/38016)\n",
            "297 391 Loss: 1.527 | Acc: 43.844% (16724/38144)\n",
            "298 391 Loss: 1.526 | Acc: 43.886% (16796/38272)\n",
            "299 391 Loss: 1.525 | Acc: 43.919% (16865/38400)\n",
            "300 391 Loss: 1.524 | Acc: 43.952% (16934/38528)\n",
            "301 391 Loss: 1.523 | Acc: 43.975% (16999/38656)\n",
            "302 391 Loss: 1.522 | Acc: 44.003% (17066/38784)\n",
            "303 391 Loss: 1.521 | Acc: 44.069% (17148/38912)\n",
            "304 391 Loss: 1.520 | Acc: 44.109% (17220/39040)\n",
            "305 391 Loss: 1.519 | Acc: 44.120% (17281/39168)\n",
            "306 391 Loss: 1.518 | Acc: 44.157% (17352/39296)\n",
            "307 391 Loss: 1.517 | Acc: 44.204% (17427/39424)\n",
            "308 391 Loss: 1.516 | Acc: 44.251% (17502/39552)\n",
            "309 391 Loss: 1.514 | Acc: 44.289% (17574/39680)\n",
            "310 391 Loss: 1.513 | Acc: 44.340% (17651/39808)\n",
            "311 391 Loss: 1.513 | Acc: 44.371% (17720/39936)\n",
            "312 391 Loss: 1.512 | Acc: 44.396% (17787/40064)\n",
            "313 391 Loss: 1.512 | Acc: 44.404% (17847/40192)\n",
            "314 391 Loss: 1.511 | Acc: 44.449% (17922/40320)\n",
            "315 391 Loss: 1.509 | Acc: 44.484% (17993/40448)\n",
            "316 391 Loss: 1.509 | Acc: 44.529% (18068/40576)\n",
            "317 391 Loss: 1.507 | Acc: 44.563% (18139/40704)\n",
            "318 391 Loss: 1.506 | Acc: 44.605% (18213/40832)\n",
            "319 391 Loss: 1.505 | Acc: 44.661% (18293/40960)\n",
            "320 391 Loss: 1.504 | Acc: 44.685% (18360/41088)\n",
            "321 391 Loss: 1.503 | Acc: 44.740% (18440/41216)\n",
            "322 391 Loss: 1.502 | Acc: 44.776% (18512/41344)\n",
            "323 391 Loss: 1.501 | Acc: 44.823% (18589/41472)\n",
            "324 391 Loss: 1.500 | Acc: 44.865% (18664/41600)\n",
            "325 391 Loss: 1.499 | Acc: 44.910% (18740/41728)\n",
            "326 391 Loss: 1.497 | Acc: 44.976% (18825/41856)\n",
            "327 391 Loss: 1.496 | Acc: 45.034% (18907/41984)\n",
            "328 391 Loss: 1.495 | Acc: 45.075% (18982/42112)\n",
            "329 391 Loss: 1.495 | Acc: 45.095% (19048/42240)\n",
            "330 391 Loss: 1.494 | Acc: 45.131% (19121/42368)\n",
            "331 391 Loss: 1.492 | Acc: 45.188% (19203/42496)\n",
            "332 391 Loss: 1.491 | Acc: 45.254% (19289/42624)\n",
            "333 391 Loss: 1.489 | Acc: 45.315% (19373/42752)\n",
            "334 391 Loss: 1.488 | Acc: 45.352% (19447/42880)\n",
            "335 391 Loss: 1.488 | Acc: 45.373% (19514/43008)\n",
            "336 391 Loss: 1.487 | Acc: 45.408% (19587/43136)\n",
            "337 391 Loss: 1.486 | Acc: 45.440% (19659/43264)\n",
            "338 391 Loss: 1.485 | Acc: 45.481% (19735/43392)\n",
            "339 391 Loss: 1.485 | Acc: 45.492% (19798/43520)\n",
            "340 391 Loss: 1.484 | Acc: 45.519% (19868/43648)\n",
            "341 391 Loss: 1.483 | Acc: 45.543% (19937/43776)\n",
            "342 391 Loss: 1.481 | Acc: 45.609% (20024/43904)\n",
            "343 391 Loss: 1.481 | Acc: 45.649% (20100/44032)\n",
            "344 391 Loss: 1.480 | Acc: 45.682% (20173/44160)\n",
            "345 391 Loss: 1.479 | Acc: 45.705% (20242/44288)\n",
            "346 391 Loss: 1.479 | Acc: 45.729% (20311/44416)\n",
            "347 391 Loss: 1.478 | Acc: 45.777% (20391/44544)\n",
            "348 391 Loss: 1.476 | Acc: 45.825% (20471/44672)\n",
            "349 391 Loss: 1.476 | Acc: 45.859% (20545/44800)\n",
            "350 391 Loss: 1.474 | Acc: 45.929% (20635/44928)\n",
            "351 391 Loss: 1.472 | Acc: 45.989% (20721/45056)\n",
            "352 391 Loss: 1.471 | Acc: 46.034% (20800/45184)\n",
            "353 391 Loss: 1.471 | Acc: 46.067% (20874/45312)\n",
            "354 391 Loss: 1.470 | Acc: 46.120% (20957/45440)\n",
            "355 391 Loss: 1.469 | Acc: 46.151% (21030/45568)\n",
            "356 391 Loss: 1.468 | Acc: 46.194% (21109/45696)\n",
            "357 391 Loss: 1.467 | Acc: 46.236% (21187/45824)\n",
            "358 391 Loss: 1.465 | Acc: 46.287% (21270/45952)\n",
            "359 391 Loss: 1.464 | Acc: 46.328% (21348/46080)\n",
            "360 391 Loss: 1.464 | Acc: 46.340% (21413/46208)\n",
            "361 391 Loss: 1.463 | Acc: 46.387% (21494/46336)\n",
            "362 391 Loss: 1.462 | Acc: 46.412% (21565/46464)\n",
            "363 391 Loss: 1.461 | Acc: 46.463% (21648/46592)\n",
            "364 391 Loss: 1.460 | Acc: 46.483% (21717/46720)\n",
            "365 391 Loss: 1.459 | Acc: 46.510% (21789/46848)\n",
            "366 391 Loss: 1.458 | Acc: 46.551% (21868/46976)\n",
            "367 391 Loss: 1.457 | Acc: 46.595% (21948/47104)\n",
            "368 391 Loss: 1.456 | Acc: 46.615% (22017/47232)\n",
            "369 391 Loss: 1.455 | Acc: 46.641% (22089/47360)\n",
            "370 391 Loss: 1.455 | Acc: 46.681% (22168/47488)\n",
            "371 391 Loss: 1.453 | Acc: 46.730% (22251/47616)\n",
            "372 391 Loss: 1.453 | Acc: 46.760% (22325/47744)\n",
            "373 391 Loss: 1.452 | Acc: 46.791% (22400/47872)\n",
            "374 391 Loss: 1.451 | Acc: 46.831% (22479/48000)\n",
            "375 391 Loss: 1.450 | Acc: 46.885% (22565/48128)\n",
            "376 391 Loss: 1.449 | Acc: 46.921% (22642/48256)\n",
            "377 391 Loss: 1.448 | Acc: 46.968% (22725/48384)\n",
            "378 391 Loss: 1.447 | Acc: 47.011% (22806/48512)\n",
            "379 391 Loss: 1.446 | Acc: 47.044% (22882/48640)\n",
            "380 391 Loss: 1.445 | Acc: 47.088% (22964/48768)\n",
            "381 391 Loss: 1.444 | Acc: 47.135% (23047/48896)\n",
            "382 391 Loss: 1.443 | Acc: 47.171% (23125/49024)\n",
            "383 391 Loss: 1.442 | Acc: 47.201% (23200/49152)\n",
            "384 391 Loss: 1.441 | Acc: 47.226% (23273/49280)\n",
            "385 391 Loss: 1.440 | Acc: 47.280% (23360/49408)\n",
            "386 391 Loss: 1.439 | Acc: 47.315% (23438/49536)\n",
            "387 391 Loss: 1.438 | Acc: 47.352% (23517/49664)\n",
            "388 391 Loss: 1.437 | Acc: 47.381% (23592/49792)\n",
            "389 391 Loss: 1.437 | Acc: 47.394% (23659/49920)\n",
            "390 391 Loss: 1.436 | Acc: 47.416% (23708/50000)\n"
          ]
        }
      ]
    }
  ]
}