{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkV2egi2VtXrcQfX2XzpAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "741a806953fc427598f3c1c971e05343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_686157e9c7f8421f963a09378cdab872",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a766503fc9e45e887067a5e25d4acb3",
              "IPY_MODEL_a239025030ec43439c77d7d294015984",
              "IPY_MODEL_3425386f69ba4bfeaaf316ad39a89dd0"
            ]
          }
        },
        "686157e9c7f8421f963a09378cdab872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a766503fc9e45e887067a5e25d4acb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db1d5dd97d394730b9ddc8b1b0c51e58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2c0d8ca769744b78798af207eda16e3"
          }
        },
        "a239025030ec43439c77d7d294015984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c606a28900a24d46adffbf7f68a697b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d7049fc354145a7a1733022aa8ed046"
          }
        },
        "3425386f69ba4bfeaaf316ad39a89dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49e1606cefd84c568d29779adbf4d6d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 48654795.32it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77aee7325ba24164a72dececcd3c859b"
          }
        },
        "db1d5dd97d394730b9ddc8b1b0c51e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2c0d8ca769744b78798af207eda16e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c606a28900a24d46adffbf7f68a697b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d7049fc354145a7a1733022aa8ed046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49e1606cefd84c568d29779adbf4d6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77aee7325ba24164a72dececcd3c859b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_cifar_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "741a806953fc427598f3c1c971e05343",
            "686157e9c7f8421f963a09378cdab872",
            "2a766503fc9e45e887067a5e25d4acb3",
            "a239025030ec43439c77d7d294015984",
            "3425386f69ba4bfeaaf316ad39a89dd0",
            "db1d5dd97d394730b9ddc8b1b0c51e58",
            "a2c0d8ca769744b78798af207eda16e3",
            "c606a28900a24d46adffbf7f68a697b8",
            "8d7049fc354145a7a1733022aa8ed046",
            "49e1606cefd84c568d29779adbf4d6d0",
            "77aee7325ba24164a72dececcd3c859b"
          ]
        },
        "id": "ox_FE_xeDz6F",
        "outputId": "8243fb5f-9fd7-4db6-841c-cab19b4f6ff3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svako osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1, za cifar 3\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    #transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    #transforms.Normalize((0.1307,), (0.3081,)), ovo je za mnist\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "#trainset = torchvision.datasets.MNIST(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "          'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#classes = ('0', '1', '2', '3', '4',\n",
        "#           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
        "                      #momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "       \n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+200):\n",
        "train(start_epoch)\n",
        "#    scheduler.step()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "741a806953fc427598f3c1c971e05343",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.427 | Acc: 10.938% (14/128)\n",
            "1 391 Loss: 2.414 | Acc: 10.156% (26/256)\n",
            "2 391 Loss: 2.401 | Acc: 12.760% (49/384)\n",
            "3 391 Loss: 2.365 | Acc: 13.477% (69/512)\n",
            "4 391 Loss: 2.330 | Acc: 14.688% (94/640)\n",
            "5 391 Loss: 2.279 | Acc: 15.495% (119/768)\n",
            "6 391 Loss: 2.241 | Acc: 16.741% (150/896)\n",
            "7 391 Loss: 2.198 | Acc: 17.969% (184/1024)\n",
            "8 391 Loss: 2.173 | Acc: 18.924% (218/1152)\n",
            "9 391 Loss: 2.138 | Acc: 20.469% (262/1280)\n",
            "10 391 Loss: 2.110 | Acc: 22.230% (313/1408)\n",
            "11 391 Loss: 2.098 | Acc: 23.047% (354/1536)\n",
            "12 391 Loss: 2.086 | Acc: 23.858% (397/1664)\n",
            "13 391 Loss: 2.064 | Acc: 24.888% (446/1792)\n",
            "14 391 Loss: 2.052 | Acc: 25.104% (482/1920)\n",
            "15 391 Loss: 2.039 | Acc: 25.439% (521/2048)\n",
            "16 391 Loss: 2.023 | Acc: 25.460% (554/2176)\n",
            "17 391 Loss: 2.009 | Acc: 26.042% (600/2304)\n",
            "18 391 Loss: 1.997 | Acc: 26.357% (641/2432)\n",
            "19 391 Loss: 1.991 | Acc: 26.680% (683/2560)\n",
            "20 391 Loss: 1.976 | Acc: 27.381% (736/2688)\n",
            "21 391 Loss: 1.966 | Acc: 27.308% (769/2816)\n",
            "22 391 Loss: 1.961 | Acc: 27.548% (811/2944)\n",
            "23 391 Loss: 1.954 | Acc: 27.799% (854/3072)\n",
            "24 391 Loss: 1.948 | Acc: 28.000% (896/3200)\n",
            "25 391 Loss: 1.936 | Acc: 28.035% (933/3328)\n",
            "26 391 Loss: 1.927 | Acc: 28.328% (979/3456)\n",
            "27 391 Loss: 1.919 | Acc: 28.767% (1031/3584)\n",
            "28 391 Loss: 1.909 | Acc: 29.122% (1081/3712)\n",
            "29 391 Loss: 1.905 | Acc: 29.297% (1125/3840)\n",
            "30 391 Loss: 1.898 | Acc: 29.587% (1174/3968)\n",
            "31 391 Loss: 1.889 | Acc: 29.761% (1219/4096)\n",
            "32 391 Loss: 1.883 | Acc: 30.090% (1271/4224)\n",
            "33 391 Loss: 1.878 | Acc: 30.400% (1323/4352)\n",
            "34 391 Loss: 1.876 | Acc: 30.424% (1363/4480)\n",
            "35 391 Loss: 1.872 | Acc: 30.447% (1403/4608)\n",
            "36 391 Loss: 1.864 | Acc: 30.828% (1460/4736)\n",
            "37 391 Loss: 1.858 | Acc: 30.983% (1507/4864)\n",
            "38 391 Loss: 1.849 | Acc: 31.270% (1561/4992)\n",
            "39 391 Loss: 1.845 | Acc: 31.289% (1602/5120)\n",
            "40 391 Loss: 1.841 | Acc: 31.383% (1647/5248)\n",
            "41 391 Loss: 1.841 | Acc: 31.399% (1688/5376)\n",
            "42 391 Loss: 1.833 | Acc: 31.686% (1744/5504)\n",
            "43 391 Loss: 1.833 | Acc: 31.712% (1786/5632)\n",
            "44 391 Loss: 1.828 | Acc: 31.875% (1836/5760)\n",
            "45 391 Loss: 1.824 | Acc: 31.997% (1884/5888)\n",
            "46 391 Loss: 1.824 | Acc: 31.932% (1921/6016)\n",
            "47 391 Loss: 1.822 | Acc: 31.917% (1961/6144)\n",
            "48 391 Loss: 1.816 | Acc: 32.063% (2011/6272)\n",
            "49 391 Loss: 1.815 | Acc: 32.234% (2063/6400)\n",
            "50 391 Loss: 1.812 | Acc: 32.338% (2111/6528)\n",
            "51 391 Loss: 1.809 | Acc: 32.542% (2166/6656)\n",
            "52 391 Loss: 1.804 | Acc: 32.636% (2214/6784)\n",
            "53 391 Loss: 1.799 | Acc: 32.841% (2270/6912)\n",
            "54 391 Loss: 1.797 | Acc: 32.940% (2319/7040)\n",
            "55 391 Loss: 1.793 | Acc: 33.050% (2369/7168)\n",
            "56 391 Loss: 1.789 | Acc: 33.292% (2429/7296)\n",
            "57 391 Loss: 1.786 | Acc: 33.446% (2483/7424)\n",
            "58 391 Loss: 1.780 | Acc: 33.686% (2544/7552)\n",
            "59 391 Loss: 1.779 | Acc: 33.646% (2584/7680)\n",
            "60 391 Loss: 1.777 | Acc: 33.735% (2634/7808)\n",
            "61 391 Loss: 1.774 | Acc: 33.821% (2684/7936)\n",
            "62 391 Loss: 1.771 | Acc: 33.966% (2739/8064)\n",
            "63 391 Loss: 1.768 | Acc: 34.119% (2795/8192)\n",
            "64 391 Loss: 1.766 | Acc: 34.279% (2852/8320)\n",
            "65 391 Loss: 1.764 | Acc: 34.351% (2902/8448)\n",
            "66 391 Loss: 1.764 | Acc: 34.340% (2945/8576)\n",
            "67 391 Loss: 1.761 | Acc: 34.536% (3006/8704)\n",
            "68 391 Loss: 1.757 | Acc: 34.681% (3063/8832)\n",
            "69 391 Loss: 1.756 | Acc: 34.777% (3116/8960)\n",
            "70 391 Loss: 1.753 | Acc: 34.870% (3169/9088)\n",
            "71 391 Loss: 1.748 | Acc: 35.124% (3237/9216)\n",
            "72 391 Loss: 1.745 | Acc: 35.178% (3287/9344)\n",
            "73 391 Loss: 1.743 | Acc: 35.146% (3329/9472)\n",
            "74 391 Loss: 1.741 | Acc: 35.260% (3385/9600)\n",
            "75 391 Loss: 1.738 | Acc: 35.434% (3447/9728)\n",
            "76 391 Loss: 1.736 | Acc: 35.471% (3496/9856)\n",
            "77 391 Loss: 1.733 | Acc: 35.557% (3550/9984)\n",
            "78 391 Loss: 1.732 | Acc: 35.641% (3604/10112)\n",
            "79 391 Loss: 1.729 | Acc: 35.732% (3659/10240)\n",
            "80 391 Loss: 1.728 | Acc: 35.764% (3708/10368)\n",
            "81 391 Loss: 1.726 | Acc: 35.842% (3762/10496)\n",
            "82 391 Loss: 1.722 | Acc: 35.947% (3819/10624)\n",
            "83 391 Loss: 1.721 | Acc: 36.012% (3872/10752)\n",
            "84 391 Loss: 1.718 | Acc: 36.121% (3930/10880)\n",
            "85 391 Loss: 1.715 | Acc: 36.237% (3989/11008)\n",
            "86 391 Loss: 1.712 | Acc: 36.324% (4045/11136)\n",
            "87 391 Loss: 1.710 | Acc: 36.479% (4109/11264)\n",
            "88 391 Loss: 1.706 | Acc: 36.578% (4167/11392)\n",
            "89 391 Loss: 1.705 | Acc: 36.675% (4225/11520)\n",
            "90 391 Loss: 1.704 | Acc: 36.710% (4276/11648)\n",
            "91 391 Loss: 1.703 | Acc: 36.795% (4333/11776)\n",
            "92 391 Loss: 1.700 | Acc: 36.920% (4395/11904)\n",
            "93 391 Loss: 1.697 | Acc: 36.935% (4444/12032)\n",
            "94 391 Loss: 1.695 | Acc: 37.048% (4505/12160)\n",
            "95 391 Loss: 1.692 | Acc: 37.142% (4564/12288)\n",
            "96 391 Loss: 1.690 | Acc: 37.258% (4626/12416)\n",
            "97 391 Loss: 1.689 | Acc: 37.285% (4677/12544)\n",
            "98 391 Loss: 1.686 | Acc: 37.318% (4729/12672)\n",
            "99 391 Loss: 1.685 | Acc: 37.438% (4792/12800)\n",
            "100 391 Loss: 1.683 | Acc: 37.492% (4847/12928)\n",
            "101 391 Loss: 1.679 | Acc: 37.653% (4916/13056)\n",
            "102 391 Loss: 1.677 | Acc: 37.750% (4977/13184)\n",
            "103 391 Loss: 1.673 | Acc: 37.868% (5041/13312)\n",
            "104 391 Loss: 1.671 | Acc: 37.932% (5098/13440)\n",
            "105 391 Loss: 1.669 | Acc: 38.038% (5161/13568)\n",
            "106 391 Loss: 1.666 | Acc: 38.135% (5223/13696)\n",
            "107 391 Loss: 1.664 | Acc: 38.194% (5280/13824)\n",
            "108 391 Loss: 1.662 | Acc: 38.310% (5345/13952)\n",
            "109 391 Loss: 1.660 | Acc: 38.388% (5405/14080)\n",
            "110 391 Loss: 1.657 | Acc: 38.443% (5462/14208)\n",
            "111 391 Loss: 1.655 | Acc: 38.539% (5525/14336)\n",
            "112 391 Loss: 1.652 | Acc: 38.641% (5589/14464)\n",
            "113 391 Loss: 1.649 | Acc: 38.754% (5655/14592)\n",
            "114 391 Loss: 1.647 | Acc: 38.838% (5717/14720)\n",
            "115 391 Loss: 1.644 | Acc: 38.887% (5774/14848)\n",
            "116 391 Loss: 1.642 | Acc: 39.029% (5845/14976)\n",
            "117 391 Loss: 1.642 | Acc: 39.049% (5898/15104)\n",
            "118 391 Loss: 1.639 | Acc: 39.128% (5960/15232)\n",
            "119 391 Loss: 1.637 | Acc: 39.180% (6018/15360)\n",
            "120 391 Loss: 1.635 | Acc: 39.243% (6078/15488)\n",
            "121 391 Loss: 1.634 | Acc: 39.299% (6137/15616)\n",
            "122 391 Loss: 1.633 | Acc: 39.291% (6186/15744)\n",
            "123 391 Loss: 1.631 | Acc: 39.378% (6250/15872)\n",
            "124 391 Loss: 1.629 | Acc: 39.438% (6310/16000)\n",
            "125 391 Loss: 1.627 | Acc: 39.521% (6374/16128)\n",
            "126 391 Loss: 1.624 | Acc: 39.653% (6446/16256)\n",
            "127 391 Loss: 1.622 | Acc: 39.783% (6518/16384)\n",
            "128 391 Loss: 1.620 | Acc: 39.844% (6579/16512)\n",
            "129 391 Loss: 1.619 | Acc: 39.898% (6639/16640)\n",
            "130 391 Loss: 1.617 | Acc: 39.933% (6696/16768)\n",
            "131 391 Loss: 1.616 | Acc: 40.015% (6761/16896)\n",
            "132 391 Loss: 1.614 | Acc: 40.102% (6827/17024)\n",
            "133 391 Loss: 1.612 | Acc: 40.141% (6885/17152)\n",
            "134 391 Loss: 1.610 | Acc: 40.203% (6947/17280)\n",
            "135 391 Loss: 1.608 | Acc: 40.315% (7018/17408)\n",
            "136 391 Loss: 1.605 | Acc: 40.437% (7091/17536)\n",
            "137 391 Loss: 1.604 | Acc: 40.483% (7151/17664)\n",
            "138 391 Loss: 1.603 | Acc: 40.546% (7214/17792)\n",
            "139 391 Loss: 1.601 | Acc: 40.636% (7282/17920)\n",
            "140 391 Loss: 1.600 | Acc: 40.691% (7344/18048)\n",
            "141 391 Loss: 1.599 | Acc: 40.757% (7408/18176)\n",
            "142 391 Loss: 1.598 | Acc: 40.794% (7467/18304)\n",
            "143 391 Loss: 1.596 | Acc: 40.815% (7523/18432)\n",
            "144 391 Loss: 1.594 | Acc: 40.873% (7586/18560)\n",
            "145 391 Loss: 1.593 | Acc: 40.962% (7655/18688)\n",
            "146 391 Loss: 1.592 | Acc: 41.008% (7716/18816)\n",
            "147 391 Loss: 1.590 | Acc: 41.090% (7784/18944)\n",
            "148 391 Loss: 1.588 | Acc: 41.170% (7852/19072)\n",
            "149 391 Loss: 1.588 | Acc: 41.240% (7918/19200)\n",
            "150 391 Loss: 1.586 | Acc: 41.261% (7975/19328)\n",
            "151 391 Loss: 1.584 | Acc: 41.360% (8047/19456)\n",
            "152 391 Loss: 1.583 | Acc: 41.401% (8108/19584)\n",
            "153 391 Loss: 1.580 | Acc: 41.457% (8172/19712)\n",
            "154 391 Loss: 1.579 | Acc: 41.492% (8232/19840)\n",
            "155 391 Loss: 1.577 | Acc: 41.541% (8295/19968)\n",
            "156 391 Loss: 1.575 | Acc: 41.590% (8358/20096)\n",
            "157 391 Loss: 1.573 | Acc: 41.678% (8429/20224)\n",
            "158 391 Loss: 1.573 | Acc: 41.691% (8485/20352)\n",
            "159 391 Loss: 1.572 | Acc: 41.743% (8549/20480)\n",
            "160 391 Loss: 1.571 | Acc: 41.775% (8609/20608)\n",
            "161 391 Loss: 1.569 | Acc: 41.831% (8674/20736)\n",
            "162 391 Loss: 1.568 | Acc: 41.919% (8746/20864)\n",
            "163 391 Loss: 1.566 | Acc: 42.002% (8817/20992)\n",
            "164 391 Loss: 1.564 | Acc: 42.093% (8890/21120)\n",
            "165 391 Loss: 1.562 | Acc: 42.197% (8966/21248)\n",
            "166 391 Loss: 1.561 | Acc: 42.239% (9029/21376)\n",
            "167 391 Loss: 1.559 | Acc: 42.285% (9093/21504)\n",
            "168 391 Loss: 1.558 | Acc: 42.317% (9154/21632)\n",
            "169 391 Loss: 1.556 | Acc: 42.390% (9224/21760)\n",
            "170 391 Loss: 1.555 | Acc: 42.443% (9290/21888)\n",
            "171 391 Loss: 1.553 | Acc: 42.533% (9364/22016)\n",
            "172 391 Loss: 1.552 | Acc: 42.562% (9425/22144)\n",
            "173 391 Loss: 1.551 | Acc: 42.614% (9491/22272)\n",
            "174 391 Loss: 1.550 | Acc: 42.661% (9556/22400)\n",
            "175 391 Loss: 1.548 | Acc: 42.725% (9625/22528)\n",
            "176 391 Loss: 1.545 | Acc: 42.814% (9700/22656)\n",
            "177 391 Loss: 1.543 | Acc: 42.920% (9779/22784)\n",
            "178 391 Loss: 1.542 | Acc: 42.986% (9849/22912)\n",
            "179 391 Loss: 1.542 | Acc: 43.008% (9909/23040)\n",
            "180 391 Loss: 1.540 | Acc: 43.072% (9979/23168)\n",
            "181 391 Loss: 1.538 | Acc: 43.102% (10041/23296)\n",
            "182 391 Loss: 1.537 | Acc: 43.157% (10109/23424)\n",
            "183 391 Loss: 1.536 | Acc: 43.224% (10180/23552)\n",
            "184 391 Loss: 1.535 | Acc: 43.290% (10251/23680)\n",
            "185 391 Loss: 1.533 | Acc: 43.359% (10323/23808)\n",
            "186 391 Loss: 1.531 | Acc: 43.445% (10399/23936)\n",
            "187 391 Loss: 1.530 | Acc: 43.534% (10476/24064)\n",
            "188 391 Loss: 1.529 | Acc: 43.576% (10542/24192)\n",
            "189 391 Loss: 1.526 | Acc: 43.684% (10624/24320)\n",
            "190 391 Loss: 1.526 | Acc: 43.721% (10689/24448)\n",
            "191 391 Loss: 1.525 | Acc: 43.730% (10747/24576)\n",
            "192 391 Loss: 1.525 | Acc: 43.750% (10808/24704)\n",
            "193 391 Loss: 1.523 | Acc: 43.810% (10879/24832)\n",
            "194 391 Loss: 1.522 | Acc: 43.882% (10953/24960)\n",
            "195 391 Loss: 1.520 | Acc: 43.953% (11027/25088)\n",
            "196 391 Loss: 1.519 | Acc: 43.996% (11094/25216)\n",
            "197 391 Loss: 1.517 | Acc: 44.058% (11166/25344)\n",
            "198 391 Loss: 1.516 | Acc: 44.099% (11233/25472)\n",
            "199 391 Loss: 1.513 | Acc: 44.172% (11308/25600)\n",
            "200 391 Loss: 1.512 | Acc: 44.236% (11381/25728)\n",
            "201 391 Loss: 1.511 | Acc: 44.303% (11455/25856)\n",
            "202 391 Loss: 1.509 | Acc: 44.400% (11537/25984)\n",
            "203 391 Loss: 1.508 | Acc: 44.439% (11604/26112)\n",
            "204 391 Loss: 1.506 | Acc: 44.531% (11685/26240)\n",
            "205 391 Loss: 1.504 | Acc: 44.584% (11756/26368)\n",
            "206 391 Loss: 1.503 | Acc: 44.637% (11827/26496)\n",
            "207 391 Loss: 1.502 | Acc: 44.700% (11901/26624)\n",
            "208 391 Loss: 1.500 | Acc: 44.744% (11970/26752)\n",
            "209 391 Loss: 1.499 | Acc: 44.795% (12041/26880)\n",
            "210 391 Loss: 1.497 | Acc: 44.839% (12110/27008)\n",
            "211 391 Loss: 1.496 | Acc: 44.881% (12179/27136)\n",
            "212 391 Loss: 1.495 | Acc: 44.927% (12249/27264)\n",
            "213 391 Loss: 1.494 | Acc: 44.995% (12325/27392)\n",
            "214 391 Loss: 1.493 | Acc: 45.055% (12399/27520)\n",
            "215 391 Loss: 1.491 | Acc: 45.110% (12472/27648)\n",
            "216 391 Loss: 1.489 | Acc: 45.190% (12552/27776)\n",
            "217 391 Loss: 1.488 | Acc: 45.230% (12621/27904)\n",
            "218 391 Loss: 1.487 | Acc: 45.263% (12688/28032)\n",
            "219 391 Loss: 1.485 | Acc: 45.316% (12761/28160)\n",
            "220 391 Loss: 1.484 | Acc: 45.373% (12835/28288)\n",
            "221 391 Loss: 1.482 | Acc: 45.439% (12912/28416)\n",
            "222 391 Loss: 1.481 | Acc: 45.505% (12989/28544)\n",
            "223 391 Loss: 1.480 | Acc: 45.536% (13056/28672)\n",
            "224 391 Loss: 1.478 | Acc: 45.597% (13132/28800)\n",
            "225 391 Loss: 1.477 | Acc: 45.644% (13204/28928)\n",
            "226 391 Loss: 1.477 | Acc: 45.660% (13267/29056)\n",
            "227 391 Loss: 1.476 | Acc: 45.672% (13329/29184)\n",
            "228 391 Loss: 1.474 | Acc: 45.763% (13414/29312)\n",
            "229 391 Loss: 1.473 | Acc: 45.808% (13486/29440)\n",
            "230 391 Loss: 1.472 | Acc: 45.847% (13556/29568)\n",
            "231 391 Loss: 1.471 | Acc: 45.888% (13627/29696)\n",
            "232 391 Loss: 1.470 | Acc: 45.933% (13699/29824)\n",
            "233 391 Loss: 1.469 | Acc: 45.977% (13771/29952)\n",
            "234 391 Loss: 1.468 | Acc: 46.007% (13839/30080)\n",
            "235 391 Loss: 1.468 | Acc: 46.044% (13909/30208)\n",
            "236 391 Loss: 1.466 | Acc: 46.123% (13992/30336)\n",
            "237 391 Loss: 1.465 | Acc: 46.169% (14065/30464)\n",
            "238 391 Loss: 1.463 | Acc: 46.234% (14144/30592)\n",
            "239 391 Loss: 1.462 | Acc: 46.286% (14219/30720)\n",
            "240 391 Loss: 1.461 | Acc: 46.324% (14290/30848)\n",
            "241 391 Loss: 1.460 | Acc: 46.378% (14366/30976)\n",
            "242 391 Loss: 1.459 | Acc: 46.422% (14439/31104)\n",
            "243 391 Loss: 1.457 | Acc: 46.491% (14520/31232)\n",
            "244 391 Loss: 1.456 | Acc: 46.534% (14593/31360)\n",
            "245 391 Loss: 1.454 | Acc: 46.602% (14674/31488)\n",
            "246 391 Loss: 1.453 | Acc: 46.631% (14743/31616)\n",
            "247 391 Loss: 1.452 | Acc: 46.648% (14808/31744)\n",
            "248 391 Loss: 1.451 | Acc: 46.687% (14880/31872)\n",
            "249 391 Loss: 1.450 | Acc: 46.753% (14961/32000)\n",
            "250 391 Loss: 1.449 | Acc: 46.807% (15038/32128)\n",
            "251 391 Loss: 1.448 | Acc: 46.844% (15110/32256)\n",
            "252 391 Loss: 1.447 | Acc: 46.887% (15184/32384)\n",
            "253 391 Loss: 1.446 | Acc: 46.918% (15254/32512)\n",
            "254 391 Loss: 1.445 | Acc: 46.952% (15325/32640)\n",
            "255 391 Loss: 1.444 | Acc: 46.997% (15400/32768)\n",
            "256 391 Loss: 1.443 | Acc: 47.009% (15464/32896)\n",
            "257 391 Loss: 1.443 | Acc: 47.035% (15533/33024)\n",
            "258 391 Loss: 1.441 | Acc: 47.119% (15621/33152)\n",
            "259 391 Loss: 1.440 | Acc: 47.148% (15691/33280)\n",
            "260 391 Loss: 1.438 | Acc: 47.204% (15770/33408)\n",
            "261 391 Loss: 1.437 | Acc: 47.251% (15846/33536)\n",
            "262 391 Loss: 1.436 | Acc: 47.294% (15921/33664)\n",
            "263 391 Loss: 1.435 | Acc: 47.337% (15996/33792)\n",
            "264 391 Loss: 1.434 | Acc: 47.388% (16074/33920)\n",
            "265 391 Loss: 1.434 | Acc: 47.433% (16150/34048)\n",
            "266 391 Loss: 1.432 | Acc: 47.472% (16224/34176)\n",
            "267 391 Loss: 1.431 | Acc: 47.519% (16301/34304)\n",
            "268 391 Loss: 1.430 | Acc: 47.566% (16378/34432)\n",
            "269 391 Loss: 1.430 | Acc: 47.581% (16444/34560)\n",
            "270 391 Loss: 1.429 | Acc: 47.639% (16525/34688)\n",
            "271 391 Loss: 1.428 | Acc: 47.717% (16613/34816)\n",
            "272 391 Loss: 1.426 | Acc: 47.759% (16689/34944)\n",
            "273 391 Loss: 1.425 | Acc: 47.813% (16769/35072)\n",
            "274 391 Loss: 1.423 | Acc: 47.875% (16852/35200)\n",
            "275 391 Loss: 1.422 | Acc: 47.925% (16931/35328)\n",
            "276 391 Loss: 1.421 | Acc: 47.961% (17005/35456)\n",
            "277 391 Loss: 1.420 | Acc: 48.005% (17082/35584)\n",
            "278 391 Loss: 1.419 | Acc: 48.054% (17161/35712)\n",
            "279 391 Loss: 1.418 | Acc: 48.097% (17238/35840)\n",
            "280 391 Loss: 1.417 | Acc: 48.129% (17311/35968)\n",
            "281 391 Loss: 1.415 | Acc: 48.183% (17392/36096)\n",
            "282 391 Loss: 1.415 | Acc: 48.200% (17460/36224)\n",
            "283 391 Loss: 1.414 | Acc: 48.242% (17537/36352)\n",
            "284 391 Loss: 1.414 | Acc: 48.273% (17610/36480)\n",
            "285 391 Loss: 1.412 | Acc: 48.309% (17685/36608)\n",
            "286 391 Loss: 1.412 | Acc: 48.350% (17762/36736)\n",
            "287 391 Loss: 1.410 | Acc: 48.410% (17846/36864)\n",
            "288 391 Loss: 1.409 | Acc: 48.446% (17921/36992)\n",
            "289 391 Loss: 1.408 | Acc: 48.481% (17996/37120)\n",
            "290 391 Loss: 1.408 | Acc: 48.531% (18077/37248)\n",
            "291 391 Loss: 1.406 | Acc: 48.566% (18152/37376)\n",
            "292 391 Loss: 1.405 | Acc: 48.611% (18231/37504)\n",
            "293 391 Loss: 1.404 | Acc: 48.647% (18307/37632)\n",
            "294 391 Loss: 1.403 | Acc: 48.689% (18385/37760)\n",
            "295 391 Loss: 1.402 | Acc: 48.717% (18458/37888)\n",
            "296 391 Loss: 1.401 | Acc: 48.748% (18532/38016)\n",
            "297 391 Loss: 1.400 | Acc: 48.781% (18607/38144)\n",
            "298 391 Loss: 1.399 | Acc: 48.853% (18697/38272)\n",
            "299 391 Loss: 1.398 | Acc: 48.888% (18773/38400)\n",
            "300 391 Loss: 1.398 | Acc: 48.912% (18845/38528)\n",
            "301 391 Loss: 1.397 | Acc: 48.942% (18919/38656)\n",
            "302 391 Loss: 1.396 | Acc: 48.987% (18999/38784)\n",
            "303 391 Loss: 1.396 | Acc: 49.029% (19078/38912)\n",
            "304 391 Loss: 1.395 | Acc: 49.057% (19152/39040)\n",
            "305 391 Loss: 1.394 | Acc: 49.091% (19228/39168)\n",
            "306 391 Loss: 1.393 | Acc: 49.140% (19310/39296)\n",
            "307 391 Loss: 1.391 | Acc: 49.193% (19394/39424)\n",
            "308 391 Loss: 1.390 | Acc: 49.219% (19467/39552)\n",
            "309 391 Loss: 1.389 | Acc: 49.264% (19548/39680)\n",
            "310 391 Loss: 1.388 | Acc: 49.312% (19630/39808)\n",
            "311 391 Loss: 1.387 | Acc: 49.336% (19703/39936)\n",
            "312 391 Loss: 1.386 | Acc: 49.376% (19782/40064)\n",
            "313 391 Loss: 1.385 | Acc: 49.398% (19854/40192)\n",
            "314 391 Loss: 1.384 | Acc: 49.420% (19926/40320)\n",
            "315 391 Loss: 1.383 | Acc: 49.473% (20011/40448)\n",
            "316 391 Loss: 1.381 | Acc: 49.527% (20096/40576)\n",
            "317 391 Loss: 1.380 | Acc: 49.573% (20178/40704)\n",
            "318 391 Loss: 1.379 | Acc: 49.601% (20253/40832)\n",
            "319 391 Loss: 1.379 | Acc: 49.629% (20328/40960)\n",
            "320 391 Loss: 1.378 | Acc: 49.647% (20399/41088)\n",
            "321 391 Loss: 1.377 | Acc: 49.685% (20478/41216)\n",
            "322 391 Loss: 1.376 | Acc: 49.734% (20562/41344)\n",
            "323 391 Loss: 1.375 | Acc: 49.778% (20644/41472)\n",
            "324 391 Loss: 1.374 | Acc: 49.805% (20719/41600)\n",
            "325 391 Loss: 1.372 | Acc: 49.875% (20812/41728)\n",
            "326 391 Loss: 1.371 | Acc: 49.933% (20900/41856)\n",
            "327 391 Loss: 1.370 | Acc: 49.957% (20974/41984)\n",
            "328 391 Loss: 1.369 | Acc: 49.991% (21052/42112)\n",
            "329 391 Loss: 1.368 | Acc: 50.052% (21142/42240)\n",
            "330 391 Loss: 1.367 | Acc: 50.094% (21224/42368)\n",
            "331 391 Loss: 1.366 | Acc: 50.127% (21302/42496)\n",
            "332 391 Loss: 1.365 | Acc: 50.174% (21386/42624)\n",
            "333 391 Loss: 1.364 | Acc: 50.213% (21467/42752)\n",
            "334 391 Loss: 1.363 | Acc: 50.254% (21549/42880)\n",
            "335 391 Loss: 1.361 | Acc: 50.312% (21638/43008)\n",
            "336 391 Loss: 1.361 | Acc: 50.338% (21714/43136)\n",
            "337 391 Loss: 1.360 | Acc: 50.370% (21792/43264)\n",
            "338 391 Loss: 1.359 | Acc: 50.419% (21878/43392)\n",
            "339 391 Loss: 1.358 | Acc: 50.464% (21962/43520)\n",
            "340 391 Loss: 1.357 | Acc: 50.522% (22052/43648)\n",
            "341 391 Loss: 1.355 | Acc: 50.573% (22139/43776)\n",
            "342 391 Loss: 1.355 | Acc: 50.599% (22215/43904)\n",
            "343 391 Loss: 1.354 | Acc: 50.638% (22297/44032)\n",
            "344 391 Loss: 1.353 | Acc: 50.682% (22381/44160)\n",
            "345 391 Loss: 1.353 | Acc: 50.695% (22452/44288)\n",
            "346 391 Loss: 1.351 | Acc: 50.738% (22536/44416)\n",
            "347 391 Loss: 1.351 | Acc: 50.781% (22620/44544)\n",
            "348 391 Loss: 1.349 | Acc: 50.822% (22703/44672)\n",
            "349 391 Loss: 1.348 | Acc: 50.859% (22785/44800)\n",
            "350 391 Loss: 1.348 | Acc: 50.884% (22861/44928)\n",
            "351 391 Loss: 1.347 | Acc: 50.930% (22947/45056)\n",
            "352 391 Loss: 1.346 | Acc: 50.969% (23030/45184)\n",
            "353 391 Loss: 1.345 | Acc: 50.995% (23107/45312)\n",
            "354 391 Loss: 1.345 | Acc: 51.037% (23191/45440)\n",
            "355 391 Loss: 1.344 | Acc: 51.053% (23264/45568)\n",
            "356 391 Loss: 1.343 | Acc: 51.081% (23342/45696)\n",
            "357 391 Loss: 1.343 | Acc: 51.089% (23411/45824)\n",
            "358 391 Loss: 1.342 | Acc: 51.110% (23486/45952)\n",
            "359 391 Loss: 1.341 | Acc: 51.141% (23566/46080)\n",
            "360 391 Loss: 1.340 | Acc: 51.190% (23654/46208)\n",
            "361 391 Loss: 1.339 | Acc: 51.228% (23737/46336)\n",
            "362 391 Loss: 1.338 | Acc: 51.257% (23816/46464)\n",
            "363 391 Loss: 1.338 | Acc: 51.271% (23888/46592)\n",
            "364 391 Loss: 1.337 | Acc: 51.306% (23970/46720)\n",
            "365 391 Loss: 1.336 | Acc: 51.349% (24056/46848)\n",
            "366 391 Loss: 1.335 | Acc: 51.386% (24139/46976)\n",
            "367 391 Loss: 1.334 | Acc: 51.418% (24220/47104)\n",
            "368 391 Loss: 1.334 | Acc: 51.452% (24302/47232)\n",
            "369 391 Loss: 1.333 | Acc: 51.482% (24382/47360)\n",
            "370 391 Loss: 1.333 | Acc: 51.499% (24456/47488)\n",
            "371 391 Loss: 1.332 | Acc: 51.531% (24537/47616)\n",
            "372 391 Loss: 1.331 | Acc: 51.562% (24618/47744)\n",
            "373 391 Loss: 1.331 | Acc: 51.583% (24694/47872)\n",
            "374 391 Loss: 1.330 | Acc: 51.629% (24782/48000)\n",
            "375 391 Loss: 1.329 | Acc: 51.656% (24861/48128)\n",
            "376 391 Loss: 1.328 | Acc: 51.685% (24941/48256)\n",
            "377 391 Loss: 1.327 | Acc: 51.705% (25017/48384)\n",
            "378 391 Loss: 1.326 | Acc: 51.746% (25103/48512)\n",
            "379 391 Loss: 1.326 | Acc: 51.766% (25179/48640)\n",
            "380 391 Loss: 1.325 | Acc: 51.798% (25261/48768)\n",
            "381 391 Loss: 1.324 | Acc: 51.841% (25348/48896)\n",
            "382 391 Loss: 1.323 | Acc: 51.881% (25434/49024)\n",
            "383 391 Loss: 1.322 | Acc: 51.929% (25524/49152)\n",
            "384 391 Loss: 1.321 | Acc: 51.964% (25608/49280)\n",
            "385 391 Loss: 1.320 | Acc: 52.002% (25693/49408)\n",
            "386 391 Loss: 1.319 | Acc: 52.041% (25779/49536)\n",
            "387 391 Loss: 1.318 | Acc: 52.084% (25867/49664)\n",
            "388 391 Loss: 1.317 | Acc: 52.105% (25944/49792)\n",
            "389 391 Loss: 1.317 | Acc: 52.123% (26020/49920)\n",
            "390 391 Loss: 1.316 | Acc: 52.146% (26073/50000)\n"
          ]
        }
      ]
    }
  ]
}