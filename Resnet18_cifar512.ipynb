{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUBaKDMTLMQSCzSMuDiYib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/ProjectR/blob/main/Resnet18_cifar512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox_FE_xeDz6F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 #sto znaci ovaj expansion?\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #dimenzija jezgre odnosno matrice koja se pomice po ulaznoj i stvara mapu znacajki, \n",
        "        #padding nadopunjuje rubove, bias je false jer se koristi BatchNorm, stride je broj koraka(redaka/stupaca) koliko se pomice jezgra\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)#normalizacija pomice vrijednosti u ovisnosti o srednjoj vrij.\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()#kombinira module\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        #u ovaj if se ulazi kod svakog osim prvo bloka\n",
        "        #TODO nadopuniti opis, sto znaci self.expansion? \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> ReLu -> CONV2 -> BN2 = F(X)\n",
        "        # F(x) + shorcut -> ReLu\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):#koliko klasa imamo na kraju\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)#zbog grayscale inpanes je 1, za cifar 3\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)# flattening\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)#listu od dva elementa, prvi i drugi element su strideovi \n",
        "        layers = []\n",
        "        for stride in strides:#svi u layeru imaju stride 1, osim prvog koji ima 2\n",
        "            layers.append(block(self.in_planes, planes, stride))#appenda na listu blok\n",
        "            self.in_planes = planes * block.expansion#pridruzivanje planesa in_planes, mnoezenjem s 1?\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)#out je jezgra, a 4 je stride, odnosno korak\n",
        "        out = out.view(out.size(0), -1)#reshape tensora prije nego ide dalje, -1 znaci da ne znamo broj redaka/stupaca\n",
        "        out = self.linear(out)#flattening prije fully connected layera\n",
        "        return out\n",
        "        # CONV1 -> BN1 -> Layer1(sa dva bloka) -> Layer2(sa dva bloka) -> Layer3(sa dva bloka) -> Layer4(sa dva bloka)\n",
        "        # AVGPOOL -> reshape -> flattening (linear) ili downsample\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])#u svakom sloju koliko je blokova"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQCkD03W_sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caea9d4f-fe43-41dc-db07-0d1414671a32"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "best_acc = 0  \n",
        "start_epoch = 0  \n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([#spaja transformacije zajedno\n",
        "    transforms.RandomCrop(32, padding=4),#slučajno cropa dijelove slike\n",
        "    #transforms.RandomCrop(28, padding=4),#za mnist\n",
        "    transforms.RandomHorizontalFlip(),#ili flipa ili ne\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#prvi tupple su meanovi, \n",
        "    #a drugi stand devijacije, ovo su za cifar10, ima 3 vrijednosti (visina, sirina, boja), za mnist su dvije\n",
        "    #transforms.Normalize((0.1307,), (0.3081,)), ovo je za mnist\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)#skinut cifar i mnist na google drive\n",
        "\n",
        "#trainset = torchvision.datasets.MNIST(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=512, shuffle=True, num_workers=2)\n",
        "#hiperparametri - epohe i batchsize\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "          'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#classes = ('0', '1', '2', '3', '4',\n",
        "#           '5', '6', '7', '8', '9')\n",
        "\n",
        "#Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet18()\n",
        "net = net.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)#prouciti momentum\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)#za smanjivanje learning ratea, zasto cosine\n",
        "\n",
        "start_time = time.time()\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()#postavlja sve vrijednosti na pocetku na 0, da ne kompromitira\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)#računa gubitak uz pomoc negativne log izglednosti\n",
        "        loss.backward()#propagiramo nazad u mrezi\n",
        "        optimizer.step()#natjeramo da iterira po svim parametrira tensora\n",
        "\n",
        "        train_loss += loss.item()#zbraja gubitak\n",
        "        _, predicted = outputs.max(1)#odabiremo neuron s najvecom aktivacijom\n",
        "        total += targets.size(0)#racunamo kolko je tre\n",
        "        correct += predicted.eq(targets).sum().item()#usporeduje s targetima i zbraja koliko je tocnih\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))#redni broj batcha, velicina cijelog dataset, prosjecan gubitak, tocnost,tocno, ukupno \n",
        "     \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "      train(epoch)\n",
        "      scheduler.step()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "Time elapsed: 0.00 min\n",
            "0 98 Loss: 2.421 | Acc: 10.156% (52/512)\n",
            "1 98 Loss: 2.381 | Acc: 10.059% (103/1024)\n",
            "2 98 Loss: 2.342 | Acc: 12.109% (186/1536)\n",
            "3 98 Loss: 2.318 | Acc: 13.232% (271/2048)\n",
            "4 98 Loss: 2.304 | Acc: 13.984% (358/2560)\n",
            "5 98 Loss: 2.294 | Acc: 14.453% (444/3072)\n",
            "6 98 Loss: 2.274 | Acc: 15.430% (553/3584)\n",
            "7 98 Loss: 2.246 | Acc: 16.577% (679/4096)\n",
            "8 98 Loss: 2.219 | Acc: 17.730% (817/4608)\n",
            "9 98 Loss: 2.195 | Acc: 18.438% (944/5120)\n",
            "10 98 Loss: 2.167 | Acc: 19.513% (1099/5632)\n",
            "11 98 Loss: 2.152 | Acc: 20.378% (1252/6144)\n",
            "12 98 Loss: 2.140 | Acc: 20.868% (1389/6656)\n",
            "13 98 Loss: 2.121 | Acc: 21.554% (1545/7168)\n",
            "14 98 Loss: 2.110 | Acc: 21.953% (1686/7680)\n",
            "15 98 Loss: 2.089 | Acc: 22.803% (1868/8192)\n",
            "16 98 Loss: 2.073 | Acc: 23.460% (2042/8704)\n",
            "17 98 Loss: 2.061 | Acc: 23.969% (2209/9216)\n",
            "18 98 Loss: 2.053 | Acc: 24.311% (2365/9728)\n",
            "19 98 Loss: 2.041 | Acc: 24.668% (2526/10240)\n",
            "20 98 Loss: 2.032 | Acc: 25.074% (2696/10752)\n",
            "21 98 Loss: 2.020 | Acc: 25.400% (2861/11264)\n",
            "22 98 Loss: 2.010 | Acc: 25.764% (3034/11776)\n",
            "23 98 Loss: 1.995 | Acc: 26.302% (3232/12288)\n",
            "24 98 Loss: 1.983 | Acc: 26.742% (3423/12800)\n",
            "25 98 Loss: 1.973 | Acc: 27.013% (3596/13312)\n",
            "26 98 Loss: 1.963 | Acc: 27.337% (3779/13824)\n",
            "27 98 Loss: 1.955 | Acc: 27.539% (3948/14336)\n",
            "28 98 Loss: 1.943 | Acc: 28.037% (4163/14848)\n",
            "29 98 Loss: 1.935 | Acc: 28.288% (4345/15360)\n",
            "30 98 Loss: 1.928 | Acc: 28.427% (4512/15872)\n",
            "31 98 Loss: 1.920 | Acc: 28.668% (4697/16384)\n",
            "32 98 Loss: 1.911 | Acc: 28.900% (4883/16896)\n",
            "33 98 Loss: 1.902 | Acc: 29.199% (5083/17408)\n",
            "34 98 Loss: 1.897 | Acc: 29.414% (5271/17920)\n",
            "35 98 Loss: 1.890 | Acc: 29.574% (5451/18432)\n",
            "36 98 Loss: 1.881 | Acc: 29.883% (5661/18944)\n",
            "37 98 Loss: 1.874 | Acc: 30.191% (5874/19456)\n",
            "38 98 Loss: 1.866 | Acc: 30.454% (6081/19968)\n",
            "39 98 Loss: 1.860 | Acc: 30.674% (6282/20480)\n",
            "40 98 Loss: 1.853 | Acc: 30.907% (6488/20992)\n",
            "41 98 Loss: 1.845 | Acc: 31.203% (6710/21504)\n",
            "42 98 Loss: 1.838 | Acc: 31.459% (6926/22016)\n",
            "43 98 Loss: 1.832 | Acc: 31.707% (7143/22528)\n",
            "44 98 Loss: 1.826 | Acc: 31.975% (7367/23040)\n",
            "45 98 Loss: 1.821 | Acc: 32.184% (7580/23552)\n",
            "46 98 Loss: 1.815 | Acc: 32.355% (7786/24064)\n",
            "47 98 Loss: 1.809 | Acc: 32.568% (8004/24576)\n",
            "48 98 Loss: 1.805 | Acc: 32.697% (8203/25088)\n",
            "49 98 Loss: 1.801 | Acc: 32.816% (8401/25600)\n",
            "50 98 Loss: 1.796 | Acc: 32.996% (8616/26112)\n",
            "51 98 Loss: 1.791 | Acc: 33.162% (8829/26624)\n",
            "52 98 Loss: 1.787 | Acc: 33.306% (9038/27136)\n",
            "53 98 Loss: 1.782 | Acc: 33.478% (9256/27648)\n",
            "54 98 Loss: 1.776 | Acc: 33.704% (9491/28160)\n",
            "55 98 Loss: 1.771 | Acc: 33.887% (9716/28672)\n",
            "56 98 Loss: 1.765 | Acc: 34.070% (9943/29184)\n",
            "57 98 Loss: 1.762 | Acc: 34.190% (10153/29696)\n",
            "58 98 Loss: 1.759 | Acc: 34.325% (10369/30208)\n",
            "59 98 Loss: 1.756 | Acc: 34.437% (10579/30720)\n",
            "60 98 Loss: 1.750 | Acc: 34.657% (10824/31232)\n",
            "61 98 Loss: 1.745 | Acc: 34.851% (11063/31744)\n",
            "62 98 Loss: 1.740 | Acc: 35.042% (11303/32256)\n",
            "63 98 Loss: 1.735 | Acc: 35.254% (11552/32768)\n",
            "64 98 Loss: 1.730 | Acc: 35.469% (11804/33280)\n",
            "65 98 Loss: 1.726 | Acc: 35.633% (12041/33792)\n",
            "66 98 Loss: 1.723 | Acc: 35.803% (12282/34304)\n",
            "67 98 Loss: 1.719 | Acc: 35.923% (12507/34816)\n",
            "68 98 Loss: 1.715 | Acc: 36.113% (12758/35328)\n",
            "69 98 Loss: 1.711 | Acc: 36.278% (13002/35840)\n",
            "70 98 Loss: 1.706 | Acc: 36.455% (13252/36352)\n",
            "71 98 Loss: 1.703 | Acc: 36.591% (13489/36864)\n",
            "72 98 Loss: 1.698 | Acc: 36.767% (13742/37376)\n",
            "73 98 Loss: 1.695 | Acc: 36.904% (13982/37888)\n",
            "74 98 Loss: 1.690 | Acc: 37.117% (14253/38400)\n",
            "75 98 Loss: 1.685 | Acc: 37.315% (14520/38912)\n",
            "76 98 Loss: 1.682 | Acc: 37.429% (14756/39424)\n",
            "77 98 Loss: 1.678 | Acc: 37.583% (15009/39936)\n",
            "78 98 Loss: 1.674 | Acc: 37.735% (15263/40448)\n",
            "79 98 Loss: 1.670 | Acc: 37.874% (15513/40960)\n",
            "80 98 Loss: 1.666 | Acc: 38.023% (15769/41472)\n",
            "81 98 Loss: 1.662 | Acc: 38.176% (16028/41984)\n",
            "82 98 Loss: 1.658 | Acc: 38.342% (16294/42496)\n",
            "83 98 Loss: 1.654 | Acc: 38.444% (16534/43008)\n",
            "84 98 Loss: 1.650 | Acc: 38.621% (16808/43520)\n",
            "85 98 Loss: 1.647 | Acc: 38.715% (17047/44032)\n",
            "86 98 Loss: 1.643 | Acc: 38.894% (17325/44544)\n",
            "87 98 Loss: 1.639 | Acc: 39.029% (17585/45056)\n",
            "88 98 Loss: 1.636 | Acc: 39.194% (17860/45568)\n",
            "89 98 Loss: 1.632 | Acc: 39.347% (18131/46080)\n",
            "90 98 Loss: 1.628 | Acc: 39.498% (18403/46592)\n",
            "91 98 Loss: 1.624 | Acc: 39.625% (18665/47104)\n",
            "92 98 Loss: 1.620 | Acc: 39.802% (18952/47616)\n",
            "93 98 Loss: 1.616 | Acc: 39.956% (19230/48128)\n",
            "94 98 Loss: 1.612 | Acc: 40.101% (19505/48640)\n",
            "95 98 Loss: 1.609 | Acc: 40.236% (19777/49152)\n",
            "96 98 Loss: 1.604 | Acc: 40.406% (20067/49664)\n",
            "97 98 Loss: 1.600 | Acc: 40.480% (20240/50000)\n",
            "\n",
            "Epoch: 1\n",
            "Time elapsed: 2.57 min\n",
            "0 98 Loss: 1.283 | Acc: 51.562% (264/512)\n",
            "1 98 Loss: 1.270 | Acc: 53.906% (552/1024)\n",
            "2 98 Loss: 1.255 | Acc: 54.818% (842/1536)\n",
            "3 98 Loss: 1.239 | Acc: 55.176% (1130/2048)\n",
            "4 98 Loss: 1.244 | Acc: 55.000% (1408/2560)\n",
            "5 98 Loss: 1.243 | Acc: 55.469% (1704/3072)\n",
            "6 98 Loss: 1.245 | Acc: 55.357% (1984/3584)\n",
            "7 98 Loss: 1.250 | Acc: 54.907% (2249/4096)\n",
            "8 98 Loss: 1.238 | Acc: 55.360% (2551/4608)\n",
            "9 98 Loss: 1.238 | Acc: 55.625% (2848/5120)\n",
            "10 98 Loss: 1.233 | Acc: 55.948% (3151/5632)\n",
            "11 98 Loss: 1.226 | Acc: 56.283% (3458/6144)\n",
            "12 98 Loss: 1.229 | Acc: 55.995% (3727/6656)\n",
            "13 98 Loss: 1.222 | Acc: 56.334% (4038/7168)\n",
            "14 98 Loss: 1.220 | Acc: 56.484% (4338/7680)\n",
            "15 98 Loss: 1.223 | Acc: 56.262% (4609/8192)\n",
            "16 98 Loss: 1.219 | Acc: 56.342% (4904/8704)\n",
            "17 98 Loss: 1.219 | Acc: 56.261% (5185/9216)\n",
            "18 98 Loss: 1.218 | Acc: 56.199% (5467/9728)\n",
            "19 98 Loss: 1.212 | Acc: 56.445% (5780/10240)\n",
            "20 98 Loss: 1.210 | Acc: 56.492% (6074/10752)\n",
            "21 98 Loss: 1.209 | Acc: 56.392% (6352/11264)\n",
            "22 98 Loss: 1.206 | Acc: 56.386% (6640/11776)\n",
            "23 98 Loss: 1.207 | Acc: 56.429% (6934/12288)\n",
            "24 98 Loss: 1.205 | Acc: 56.555% (7239/12800)\n",
            "25 98 Loss: 1.199 | Acc: 56.686% (7546/13312)\n",
            "26 98 Loss: 1.197 | Acc: 56.771% (7848/13824)\n",
            "27 98 Loss: 1.195 | Acc: 56.766% (8138/14336)\n",
            "28 98 Loss: 1.191 | Acc: 56.863% (8443/14848)\n",
            "29 98 Loss: 1.189 | Acc: 57.005% (8756/15360)\n",
            "30 98 Loss: 1.189 | Acc: 57.075% (9059/15872)\n",
            "31 98 Loss: 1.186 | Acc: 57.117% (9358/16384)\n",
            "32 98 Loss: 1.184 | Acc: 57.244% (9672/16896)\n",
            "33 98 Loss: 1.184 | Acc: 57.273% (9970/17408)\n",
            "34 98 Loss: 1.181 | Acc: 57.472% (10299/17920)\n",
            "35 98 Loss: 1.178 | Acc: 57.568% (10611/18432)\n",
            "36 98 Loss: 1.177 | Acc: 57.612% (10914/18944)\n",
            "37 98 Loss: 1.175 | Acc: 57.669% (11220/19456)\n",
            "38 98 Loss: 1.174 | Acc: 57.732% (11528/19968)\n",
            "39 98 Loss: 1.172 | Acc: 57.759% (11829/20480)\n",
            "40 98 Loss: 1.172 | Acc: 57.784% (12130/20992)\n",
            "41 98 Loss: 1.171 | Acc: 57.794% (12428/21504)\n",
            "42 98 Loss: 1.170 | Acc: 57.831% (12732/22016)\n",
            "43 98 Loss: 1.169 | Acc: 57.866% (13036/22528)\n",
            "44 98 Loss: 1.171 | Acc: 57.839% (13326/23040)\n",
            "45 98 Loss: 1.170 | Acc: 57.842% (13623/23552)\n",
            "46 98 Loss: 1.170 | Acc: 57.862% (13924/24064)\n",
            "47 98 Loss: 1.168 | Acc: 57.943% (14240/24576)\n",
            "48 98 Loss: 1.167 | Acc: 57.980% (14546/25088)\n",
            "49 98 Loss: 1.167 | Acc: 58.020% (14853/25600)\n",
            "50 98 Loss: 1.164 | Acc: 58.146% (15183/26112)\n",
            "51 98 Loss: 1.162 | Acc: 58.214% (15499/26624)\n",
            "52 98 Loss: 1.161 | Acc: 58.236% (15803/27136)\n",
            "53 98 Loss: 1.160 | Acc: 58.283% (16114/27648)\n",
            "54 98 Loss: 1.159 | Acc: 58.320% (16423/28160)\n",
            "55 98 Loss: 1.157 | Acc: 58.388% (16741/28672)\n",
            "56 98 Loss: 1.155 | Acc: 58.477% (17066/29184)\n",
            "57 98 Loss: 1.155 | Acc: 58.459% (17360/29696)\n",
            "58 98 Loss: 1.154 | Acc: 58.491% (17669/30208)\n",
            "59 98 Loss: 1.153 | Acc: 58.522% (17978/30720)\n",
            "60 98 Loss: 1.151 | Acc: 58.520% (18277/31232)\n",
            "61 98 Loss: 1.149 | Acc: 58.578% (18595/31744)\n",
            "62 98 Loss: 1.149 | Acc: 58.600% (18902/32256)\n",
            "63 98 Loss: 1.147 | Acc: 58.719% (19241/32768)\n",
            "64 98 Loss: 1.145 | Acc: 58.786% (19564/33280)\n",
            "65 98 Loss: 1.145 | Acc: 58.813% (19874/33792)\n",
            "66 98 Loss: 1.143 | Acc: 58.877% (20197/34304)\n",
            "67 98 Loss: 1.142 | Acc: 58.950% (20524/34816)\n",
            "68 98 Loss: 1.142 | Acc: 58.933% (20820/35328)\n",
            "69 98 Loss: 1.141 | Acc: 58.979% (21138/35840)\n",
            "70 98 Loss: 1.140 | Acc: 59.050% (21466/36352)\n",
            "71 98 Loss: 1.139 | Acc: 59.085% (21781/36864)\n",
            "72 98 Loss: 1.137 | Acc: 59.145% (22106/37376)\n",
            "73 98 Loss: 1.136 | Acc: 59.166% (22417/37888)\n",
            "74 98 Loss: 1.134 | Acc: 59.297% (22770/38400)\n",
            "75 98 Loss: 1.131 | Acc: 59.388% (23109/38912)\n",
            "76 98 Loss: 1.130 | Acc: 59.446% (23436/39424)\n",
            "77 98 Loss: 1.129 | Acc: 59.483% (23755/39936)\n",
            "78 98 Loss: 1.127 | Acc: 59.526% (24077/40448)\n",
            "79 98 Loss: 1.125 | Acc: 59.585% (24406/40960)\n",
            "80 98 Loss: 1.125 | Acc: 59.594% (24715/41472)\n",
            "81 98 Loss: 1.123 | Acc: 59.649% (25043/41984)\n",
            "82 98 Loss: 1.122 | Acc: 59.664% (25355/42496)\n",
            "83 98 Loss: 1.121 | Acc: 59.726% (25687/43008)\n",
            "84 98 Loss: 1.120 | Acc: 59.747% (26002/43520)\n",
            "85 98 Loss: 1.118 | Acc: 59.816% (26338/44032)\n",
            "86 98 Loss: 1.115 | Acc: 59.912% (26687/44544)\n",
            "87 98 Loss: 1.114 | Acc: 59.956% (27014/45056)\n",
            "88 98 Loss: 1.112 | Acc: 60.014% (27347/45568)\n",
            "89 98 Loss: 1.110 | Acc: 60.091% (27690/46080)\n",
            "90 98 Loss: 1.109 | Acc: 60.143% (28022/46592)\n",
            "91 98 Loss: 1.108 | Acc: 60.220% (28366/47104)\n",
            "92 98 Loss: 1.105 | Acc: 60.297% (28711/47616)\n",
            "93 98 Loss: 1.104 | Acc: 60.354% (29047/48128)\n",
            "94 98 Loss: 1.102 | Acc: 60.448% (29402/48640)\n",
            "95 98 Loss: 1.100 | Acc: 60.482% (29728/49152)\n",
            "96 98 Loss: 1.099 | Acc: 60.519% (30056/49664)\n",
            "97 98 Loss: 1.099 | Acc: 60.538% (30269/50000)\n",
            "\n",
            "Epoch: 2\n",
            "Time elapsed: 5.15 min\n",
            "0 98 Loss: 0.911 | Acc: 65.625% (336/512)\n",
            "1 98 Loss: 0.902 | Acc: 66.504% (681/1024)\n",
            "2 98 Loss: 0.910 | Acc: 66.862% (1027/1536)\n",
            "3 98 Loss: 0.924 | Acc: 67.041% (1373/2048)\n",
            "4 98 Loss: 0.932 | Acc: 66.602% (1705/2560)\n",
            "5 98 Loss: 0.954 | Acc: 66.081% (2030/3072)\n",
            "6 98 Loss: 0.960 | Acc: 65.848% (2360/3584)\n",
            "7 98 Loss: 0.951 | Acc: 66.089% (2707/4096)\n",
            "8 98 Loss: 0.951 | Acc: 66.319% (3056/4608)\n",
            "9 98 Loss: 0.954 | Acc: 66.133% (3386/5120)\n",
            "10 98 Loss: 0.956 | Acc: 65.927% (3713/5632)\n",
            "11 98 Loss: 0.955 | Acc: 65.934% (4051/6144)\n",
            "12 98 Loss: 0.949 | Acc: 66.331% (4415/6656)\n",
            "13 98 Loss: 0.955 | Acc: 66.099% (4738/7168)\n",
            "14 98 Loss: 0.955 | Acc: 66.237% (5087/7680)\n",
            "15 98 Loss: 0.950 | Acc: 66.345% (5435/8192)\n",
            "16 98 Loss: 0.952 | Acc: 66.360% (5776/8704)\n",
            "17 98 Loss: 0.951 | Acc: 66.471% (6126/9216)\n",
            "18 98 Loss: 0.955 | Acc: 66.375% (6457/9728)\n",
            "19 98 Loss: 0.955 | Acc: 66.357% (6795/10240)\n",
            "20 98 Loss: 0.956 | Acc: 66.332% (7132/10752)\n",
            "21 98 Loss: 0.954 | Acc: 66.388% (7478/11264)\n",
            "22 98 Loss: 0.954 | Acc: 66.321% (7810/11776)\n",
            "23 98 Loss: 0.954 | Acc: 66.366% (8155/12288)\n",
            "24 98 Loss: 0.950 | Acc: 66.562% (8520/12800)\n",
            "25 98 Loss: 0.948 | Acc: 66.647% (8872/13312)\n",
            "26 98 Loss: 0.945 | Acc: 66.659% (9215/13824)\n",
            "27 98 Loss: 0.939 | Acc: 66.832% (9581/14336)\n",
            "28 98 Loss: 0.938 | Acc: 66.837% (9924/14848)\n",
            "29 98 Loss: 0.938 | Acc: 66.842% (10267/15360)\n",
            "30 98 Loss: 0.936 | Acc: 66.885% (10616/15872)\n",
            "31 98 Loss: 0.936 | Acc: 66.840% (10951/16384)\n",
            "32 98 Loss: 0.936 | Acc: 66.838% (11293/16896)\n",
            "33 98 Loss: 0.936 | Acc: 66.837% (11635/17408)\n",
            "34 98 Loss: 0.935 | Acc: 66.914% (11991/17920)\n",
            "35 98 Loss: 0.931 | Acc: 67.025% (12354/18432)\n",
            "36 98 Loss: 0.929 | Acc: 67.092% (12710/18944)\n",
            "37 98 Loss: 0.929 | Acc: 67.162% (13067/19456)\n",
            "38 98 Loss: 0.929 | Acc: 67.167% (13412/19968)\n",
            "39 98 Loss: 0.929 | Acc: 67.168% (13756/20480)\n",
            "40 98 Loss: 0.929 | Acc: 67.140% (14094/20992)\n",
            "41 98 Loss: 0.930 | Acc: 67.062% (14421/21504)\n",
            "42 98 Loss: 0.930 | Acc: 67.083% (14769/22016)\n",
            "43 98 Loss: 0.929 | Acc: 67.125% (15122/22528)\n",
            "44 98 Loss: 0.929 | Acc: 67.144% (15470/23040)\n",
            "45 98 Loss: 0.929 | Acc: 67.166% (15819/23552)\n",
            "46 98 Loss: 0.929 | Acc: 67.133% (16155/24064)\n",
            "47 98 Loss: 0.927 | Acc: 67.208% (16517/24576)\n",
            "48 98 Loss: 0.926 | Acc: 67.239% (16869/25088)\n",
            "49 98 Loss: 0.923 | Acc: 67.371% (17247/25600)\n",
            "50 98 Loss: 0.923 | Acc: 67.345% (17585/26112)\n",
            "51 98 Loss: 0.923 | Acc: 67.330% (17926/26624)\n",
            "52 98 Loss: 0.924 | Acc: 67.309% (18265/27136)\n",
            "53 98 Loss: 0.922 | Acc: 67.365% (18625/27648)\n",
            "54 98 Loss: 0.920 | Acc: 67.454% (18995/28160)\n",
            "55 98 Loss: 0.919 | Acc: 67.470% (19345/28672)\n",
            "56 98 Loss: 0.919 | Acc: 67.503% (19700/29184)\n",
            "57 98 Loss: 0.918 | Acc: 67.561% (20063/29696)\n",
            "58 98 Loss: 0.916 | Acc: 67.628% (20429/30208)\n",
            "59 98 Loss: 0.915 | Acc: 67.660% (20785/30720)\n",
            "60 98 Loss: 0.914 | Acc: 67.697% (21143/31232)\n",
            "61 98 Loss: 0.914 | Acc: 67.720% (21497/31744)\n",
            "62 98 Loss: 0.912 | Acc: 67.773% (21861/32256)\n",
            "63 98 Loss: 0.913 | Acc: 67.755% (22202/32768)\n",
            "64 98 Loss: 0.912 | Acc: 67.776% (22556/33280)\n",
            "65 98 Loss: 0.912 | Acc: 67.750% (22894/33792)\n",
            "66 98 Loss: 0.912 | Acc: 67.736% (23236/34304)\n",
            "67 98 Loss: 0.910 | Acc: 67.791% (23602/34816)\n",
            "68 98 Loss: 0.908 | Acc: 67.827% (23962/35328)\n",
            "69 98 Loss: 0.907 | Acc: 67.885% (24330/35840)\n",
            "70 98 Loss: 0.906 | Acc: 67.916% (24689/36352)\n",
            "71 98 Loss: 0.905 | Acc: 67.931% (25042/36864)\n",
            "72 98 Loss: 0.905 | Acc: 67.913% (25383/37376)\n",
            "73 98 Loss: 0.904 | Acc: 67.924% (25735/37888)\n",
            "74 98 Loss: 0.904 | Acc: 67.927% (26084/38400)\n",
            "75 98 Loss: 0.903 | Acc: 67.987% (26455/38912)\n",
            "76 98 Loss: 0.900 | Acc: 68.073% (26837/39424)\n",
            "77 98 Loss: 0.899 | Acc: 68.094% (27194/39936)\n",
            "78 98 Loss: 0.897 | Acc: 68.132% (27558/40448)\n",
            "79 98 Loss: 0.895 | Acc: 68.201% (27935/40960)\n",
            "80 98 Loss: 0.894 | Acc: 68.280% (28317/41472)\n",
            "81 98 Loss: 0.893 | Acc: 68.314% (28681/41984)\n",
            "82 98 Loss: 0.892 | Acc: 68.305% (29027/42496)\n",
            "83 98 Loss: 0.890 | Acc: 68.371% (29405/43008)\n",
            "84 98 Loss: 0.890 | Acc: 68.394% (29765/43520)\n",
            "85 98 Loss: 0.889 | Acc: 68.439% (30135/44032)\n",
            "86 98 Loss: 0.888 | Acc: 68.438% (30485/44544)\n",
            "87 98 Loss: 0.887 | Acc: 68.484% (30856/45056)\n",
            "88 98 Loss: 0.886 | Acc: 68.467% (31199/45568)\n",
            "89 98 Loss: 0.885 | Acc: 68.509% (31569/46080)\n",
            "90 98 Loss: 0.884 | Acc: 68.604% (31964/46592)\n",
            "91 98 Loss: 0.883 | Acc: 68.642% (32333/47104)\n",
            "92 98 Loss: 0.882 | Acc: 68.662% (32694/47616)\n",
            "93 98 Loss: 0.882 | Acc: 68.690% (33059/48128)\n",
            "94 98 Loss: 0.880 | Acc: 68.734% (33432/48640)\n",
            "95 98 Loss: 0.881 | Acc: 68.734% (33784/49152)\n",
            "96 98 Loss: 0.880 | Acc: 68.762% (34150/49664)\n",
            "97 98 Loss: 0.878 | Acc: 68.808% (34404/50000)\n",
            "\n",
            "Epoch: 3\n",
            "Time elapsed: 7.73 min\n",
            "0 98 Loss: 0.756 | Acc: 72.656% (372/512)\n",
            "1 98 Loss: 0.753 | Acc: 72.266% (740/1024)\n",
            "2 98 Loss: 0.753 | Acc: 72.852% (1119/1536)\n",
            "3 98 Loss: 0.764 | Acc: 72.949% (1494/2048)\n",
            "4 98 Loss: 0.771 | Acc: 72.773% (1863/2560)\n",
            "5 98 Loss: 0.771 | Acc: 72.786% (2236/3072)\n",
            "6 98 Loss: 0.774 | Acc: 72.656% (2604/3584)\n",
            "7 98 Loss: 0.770 | Acc: 72.729% (2979/4096)\n",
            "8 98 Loss: 0.771 | Acc: 72.830% (3356/4608)\n",
            "9 98 Loss: 0.767 | Acc: 72.852% (3730/5120)\n",
            "10 98 Loss: 0.760 | Acc: 73.082% (4116/5632)\n",
            "11 98 Loss: 0.764 | Acc: 72.868% (4477/6144)\n",
            "12 98 Loss: 0.762 | Acc: 73.047% (4862/6656)\n",
            "13 98 Loss: 0.759 | Acc: 73.075% (5238/7168)\n",
            "14 98 Loss: 0.762 | Acc: 72.995% (5606/7680)\n",
            "15 98 Loss: 0.760 | Acc: 73.108% (5989/8192)\n",
            "16 98 Loss: 0.759 | Acc: 73.208% (6372/8704)\n",
            "17 98 Loss: 0.759 | Acc: 73.220% (6748/9216)\n",
            "18 98 Loss: 0.760 | Acc: 73.211% (7122/9728)\n",
            "19 98 Loss: 0.760 | Acc: 73.154% (7491/10240)\n",
            "20 98 Loss: 0.760 | Acc: 73.121% (7862/10752)\n",
            "21 98 Loss: 0.764 | Acc: 72.994% (8222/11264)\n",
            "22 98 Loss: 0.762 | Acc: 73.030% (8600/11776)\n",
            "23 98 Loss: 0.761 | Acc: 73.088% (8981/12288)\n",
            "24 98 Loss: 0.761 | Acc: 73.062% (9352/12800)\n",
            "25 98 Loss: 0.759 | Acc: 73.205% (9745/13312)\n",
            "26 98 Loss: 0.758 | Acc: 73.184% (10117/13824)\n",
            "27 98 Loss: 0.759 | Acc: 73.054% (10473/14336)\n",
            "28 98 Loss: 0.760 | Acc: 73.027% (10843/14848)\n",
            "29 98 Loss: 0.757 | Acc: 73.105% (11229/15360)\n",
            "30 98 Loss: 0.757 | Acc: 73.129% (11607/15872)\n",
            "31 98 Loss: 0.758 | Acc: 73.120% (11980/16384)\n",
            "32 98 Loss: 0.761 | Acc: 73.047% (12342/16896)\n",
            "33 98 Loss: 0.759 | Acc: 73.133% (12731/17408)\n",
            "34 98 Loss: 0.758 | Acc: 73.097% (13099/17920)\n",
            "35 98 Loss: 0.758 | Acc: 73.139% (13481/18432)\n",
            "36 98 Loss: 0.759 | Acc: 73.142% (13856/18944)\n",
            "37 98 Loss: 0.760 | Acc: 73.083% (14219/19456)\n",
            "38 98 Loss: 0.759 | Acc: 73.087% (14594/19968)\n",
            "39 98 Loss: 0.759 | Acc: 73.105% (14972/20480)\n",
            "40 98 Loss: 0.758 | Acc: 73.142% (15354/20992)\n",
            "41 98 Loss: 0.758 | Acc: 73.135% (15727/21504)\n",
            "42 98 Loss: 0.760 | Acc: 73.083% (16090/22016)\n",
            "43 98 Loss: 0.759 | Acc: 73.113% (16471/22528)\n",
            "44 98 Loss: 0.758 | Acc: 73.134% (16850/23040)\n",
            "45 98 Loss: 0.757 | Acc: 73.212% (17243/23552)\n",
            "46 98 Loss: 0.757 | Acc: 73.196% (17614/24064)\n",
            "47 98 Loss: 0.755 | Acc: 73.271% (18007/24576)\n",
            "48 98 Loss: 0.755 | Acc: 73.274% (18383/25088)\n",
            "49 98 Loss: 0.755 | Acc: 73.258% (18754/25600)\n",
            "50 98 Loss: 0.755 | Acc: 73.284% (19136/26112)\n",
            "51 98 Loss: 0.754 | Acc: 73.250% (19502/26624)\n",
            "52 98 Loss: 0.754 | Acc: 73.235% (19873/27136)\n",
            "53 98 Loss: 0.753 | Acc: 73.264% (20256/27648)\n",
            "54 98 Loss: 0.752 | Acc: 73.342% (20653/28160)\n",
            "55 98 Loss: 0.751 | Acc: 73.361% (21034/28672)\n",
            "56 98 Loss: 0.750 | Acc: 73.379% (21415/29184)\n",
            "57 98 Loss: 0.751 | Acc: 73.347% (21781/29696)\n",
            "58 98 Loss: 0.750 | Acc: 73.375% (22165/30208)\n",
            "59 98 Loss: 0.749 | Acc: 73.421% (22555/30720)\n",
            "60 98 Loss: 0.749 | Acc: 73.447% (22939/31232)\n",
            "61 98 Loss: 0.747 | Acc: 73.541% (23345/31744)\n",
            "62 98 Loss: 0.747 | Acc: 73.571% (23731/32256)\n",
            "63 98 Loss: 0.746 | Acc: 73.563% (24105/32768)\n",
            "64 98 Loss: 0.744 | Acc: 73.627% (24503/33280)\n",
            "65 98 Loss: 0.744 | Acc: 73.624% (24879/33792)\n",
            "66 98 Loss: 0.744 | Acc: 73.642% (25262/34304)\n",
            "67 98 Loss: 0.743 | Acc: 73.662% (25646/34816)\n",
            "68 98 Loss: 0.742 | Acc: 73.695% (26035/35328)\n",
            "69 98 Loss: 0.741 | Acc: 73.722% (26422/35840)\n",
            "70 98 Loss: 0.741 | Acc: 73.759% (26813/36352)\n",
            "71 98 Loss: 0.740 | Acc: 73.755% (27189/36864)\n",
            "72 98 Loss: 0.740 | Acc: 73.799% (27583/37376)\n",
            "73 98 Loss: 0.739 | Acc: 73.839% (27976/37888)\n",
            "74 98 Loss: 0.737 | Acc: 73.901% (28378/38400)\n",
            "75 98 Loss: 0.738 | Acc: 73.877% (28747/38912)\n",
            "76 98 Loss: 0.736 | Acc: 73.927% (29145/39424)\n",
            "77 98 Loss: 0.736 | Acc: 73.948% (29532/39936)\n",
            "78 98 Loss: 0.735 | Acc: 73.964% (29917/40448)\n",
            "79 98 Loss: 0.734 | Acc: 73.999% (30310/40960)\n",
            "80 98 Loss: 0.734 | Acc: 74.019% (30697/41472)\n",
            "81 98 Loss: 0.732 | Acc: 74.085% (31104/41984)\n",
            "82 98 Loss: 0.732 | Acc: 74.122% (31499/42496)\n",
            "83 98 Loss: 0.731 | Acc: 74.151% (31891/43008)\n",
            "84 98 Loss: 0.730 | Acc: 74.177% (32282/43520)\n",
            "85 98 Loss: 0.729 | Acc: 74.230% (32685/44032)\n",
            "86 98 Loss: 0.728 | Acc: 74.277% (33086/44544)\n",
            "87 98 Loss: 0.728 | Acc: 74.279% (33467/45056)\n",
            "88 98 Loss: 0.726 | Acc: 74.318% (33865/45568)\n",
            "89 98 Loss: 0.726 | Acc: 74.334% (34253/46080)\n",
            "90 98 Loss: 0.727 | Acc: 74.328% (34631/46592)\n",
            "91 98 Loss: 0.727 | Acc: 74.312% (35004/47104)\n",
            "92 98 Loss: 0.727 | Acc: 74.326% (35391/47616)\n",
            "93 98 Loss: 0.726 | Acc: 74.377% (35796/48128)\n",
            "94 98 Loss: 0.726 | Acc: 74.356% (36167/48640)\n",
            "95 98 Loss: 0.726 | Acc: 74.359% (36549/49152)\n",
            "96 98 Loss: 0.725 | Acc: 74.390% (36945/49664)\n",
            "97 98 Loss: 0.724 | Acc: 74.432% (37216/50000)\n",
            "\n",
            "Epoch: 4\n",
            "Time elapsed: 10.31 min\n",
            "0 98 Loss: 0.615 | Acc: 76.953% (394/512)\n",
            "1 98 Loss: 0.596 | Acc: 78.418% (803/1024)\n",
            "2 98 Loss: 0.595 | Acc: 78.906% (1212/1536)\n",
            "3 98 Loss: 0.633 | Acc: 77.734% (1592/2048)\n",
            "4 98 Loss: 0.624 | Acc: 78.008% (1997/2560)\n",
            "5 98 Loss: 0.611 | Acc: 78.646% (2416/3072)\n",
            "6 98 Loss: 0.626 | Acc: 78.209% (2803/3584)\n",
            "7 98 Loss: 0.634 | Acc: 77.808% (3187/4096)\n",
            "8 98 Loss: 0.633 | Acc: 77.951% (3592/4608)\n",
            "9 98 Loss: 0.627 | Acc: 78.262% (4007/5120)\n",
            "10 98 Loss: 0.624 | Acc: 78.356% (4413/5632)\n",
            "11 98 Loss: 0.632 | Acc: 78.109% (4799/6144)\n",
            "12 98 Loss: 0.633 | Acc: 78.035% (5194/6656)\n",
            "13 98 Loss: 0.640 | Acc: 77.609% (5563/7168)\n",
            "14 98 Loss: 0.642 | Acc: 77.578% (5958/7680)\n",
            "15 98 Loss: 0.650 | Acc: 77.283% (6331/8192)\n",
            "16 98 Loss: 0.649 | Acc: 77.229% (6722/8704)\n",
            "17 98 Loss: 0.648 | Acc: 77.224% (7117/9216)\n",
            "18 98 Loss: 0.646 | Acc: 77.262% (7516/9728)\n",
            "19 98 Loss: 0.646 | Acc: 77.256% (7911/10240)\n",
            "20 98 Loss: 0.643 | Acc: 77.427% (8325/10752)\n",
            "21 98 Loss: 0.640 | Acc: 77.495% (8729/11264)\n",
            "22 98 Loss: 0.643 | Acc: 77.454% (9121/11776)\n",
            "23 98 Loss: 0.642 | Acc: 77.531% (9527/12288)\n",
            "24 98 Loss: 0.638 | Acc: 77.742% (9951/12800)\n",
            "25 98 Loss: 0.637 | Acc: 77.802% (10357/13312)\n",
            "26 98 Loss: 0.638 | Acc: 77.749% (10748/13824)\n",
            "27 98 Loss: 0.638 | Acc: 77.741% (11145/14336)\n",
            "28 98 Loss: 0.637 | Acc: 77.802% (11552/14848)\n",
            "29 98 Loss: 0.635 | Acc: 77.904% (11966/15360)\n",
            "30 98 Loss: 0.634 | Acc: 77.917% (12367/15872)\n",
            "31 98 Loss: 0.636 | Acc: 77.844% (12754/16384)\n",
            "32 98 Loss: 0.638 | Acc: 77.728% (13133/16896)\n",
            "33 98 Loss: 0.638 | Acc: 77.757% (13536/17408)\n",
            "34 98 Loss: 0.637 | Acc: 77.796% (13941/17920)\n",
            "35 98 Loss: 0.640 | Acc: 77.686% (14319/18432)\n",
            "36 98 Loss: 0.637 | Acc: 77.819% (14742/18944)\n",
            "37 98 Loss: 0.636 | Acc: 77.827% (15142/19456)\n",
            "38 98 Loss: 0.634 | Acc: 77.870% (15549/19968)\n",
            "39 98 Loss: 0.632 | Acc: 77.935% (15961/20480)\n",
            "40 98 Loss: 0.634 | Acc: 77.892% (16351/20992)\n",
            "41 98 Loss: 0.634 | Acc: 77.883% (16748/21504)\n",
            "42 98 Loss: 0.634 | Acc: 77.848% (17139/22016)\n",
            "43 98 Loss: 0.634 | Acc: 77.805% (17528/22528)\n",
            "44 98 Loss: 0.635 | Acc: 77.773% (17919/23040)\n",
            "45 98 Loss: 0.636 | Acc: 77.734% (18308/23552)\n",
            "46 98 Loss: 0.635 | Acc: 77.776% (18716/24064)\n",
            "47 98 Loss: 0.635 | Acc: 77.751% (19108/24576)\n",
            "48 98 Loss: 0.634 | Acc: 77.742% (19504/25088)\n",
            "49 98 Loss: 0.634 | Acc: 77.770% (19909/25600)\n",
            "50 98 Loss: 0.634 | Acc: 77.803% (20316/26112)\n",
            "51 98 Loss: 0.635 | Acc: 77.727% (20694/26624)\n",
            "52 98 Loss: 0.634 | Acc: 77.790% (21109/27136)\n",
            "53 98 Loss: 0.634 | Acc: 77.774% (21503/27648)\n",
            "54 98 Loss: 0.634 | Acc: 77.820% (21914/28160)\n",
            "55 98 Loss: 0.635 | Acc: 77.822% (22313/28672)\n",
            "56 98 Loss: 0.637 | Acc: 77.775% (22698/29184)\n",
            "57 98 Loss: 0.636 | Acc: 77.792% (23101/29696)\n",
            "58 98 Loss: 0.635 | Acc: 77.787% (23498/30208)\n",
            "59 98 Loss: 0.636 | Acc: 77.786% (23896/30720)\n",
            "60 98 Loss: 0.635 | Acc: 77.811% (24302/31232)\n",
            "61 98 Loss: 0.635 | Acc: 77.794% (24695/31744)\n",
            "62 98 Loss: 0.635 | Acc: 77.824% (25103/32256)\n",
            "63 98 Loss: 0.635 | Acc: 77.805% (25495/32768)\n",
            "64 98 Loss: 0.635 | Acc: 77.822% (25899/33280)\n",
            "65 98 Loss: 0.634 | Acc: 77.838% (26303/33792)\n",
            "66 98 Loss: 0.633 | Acc: 77.868% (26712/34304)\n",
            "67 98 Loss: 0.633 | Acc: 77.861% (27108/34816)\n",
            "68 98 Loss: 0.632 | Acc: 77.873% (27511/35328)\n",
            "69 98 Loss: 0.633 | Acc: 77.877% (27911/35840)\n",
            "70 98 Loss: 0.632 | Acc: 77.869% (28307/36352)\n",
            "71 98 Loss: 0.631 | Acc: 77.894% (28715/36864)\n",
            "72 98 Loss: 0.632 | Acc: 77.865% (29103/37376)\n",
            "73 98 Loss: 0.632 | Acc: 77.821% (29485/37888)\n",
            "74 98 Loss: 0.632 | Acc: 77.839% (29890/38400)\n",
            "75 98 Loss: 0.631 | Acc: 77.860% (30297/38912)\n",
            "76 98 Loss: 0.630 | Acc: 77.874% (30701/39424)\n",
            "77 98 Loss: 0.629 | Acc: 77.895% (31108/39936)\n",
            "78 98 Loss: 0.629 | Acc: 77.902% (31510/40448)\n",
            "79 98 Loss: 0.629 | Acc: 77.896% (31906/40960)\n",
            "80 98 Loss: 0.629 | Acc: 77.920% (32315/41472)\n",
            "81 98 Loss: 0.628 | Acc: 77.942% (32723/41984)\n",
            "82 98 Loss: 0.628 | Acc: 77.956% (33128/42496)\n",
            "83 98 Loss: 0.628 | Acc: 77.932% (33517/43008)\n",
            "84 98 Loss: 0.629 | Acc: 77.925% (33913/43520)\n",
            "85 98 Loss: 0.628 | Acc: 77.932% (34315/44032)\n",
            "86 98 Loss: 0.628 | Acc: 77.957% (34725/44544)\n",
            "87 98 Loss: 0.627 | Acc: 77.985% (35137/45056)\n",
            "88 98 Loss: 0.628 | Acc: 77.932% (35512/45568)\n",
            "89 98 Loss: 0.628 | Acc: 77.930% (35910/46080)\n",
            "90 98 Loss: 0.627 | Acc: 77.951% (36319/46592)\n",
            "91 98 Loss: 0.626 | Acc: 77.964% (36724/47104)\n",
            "92 98 Loss: 0.626 | Acc: 77.988% (37135/47616)\n",
            "93 98 Loss: 0.626 | Acc: 78.019% (37549/48128)\n",
            "94 98 Loss: 0.625 | Acc: 78.047% (37962/48640)\n",
            "95 98 Loss: 0.625 | Acc: 78.033% (38355/49152)\n",
            "96 98 Loss: 0.624 | Acc: 78.081% (38778/49664)\n",
            "97 98 Loss: 0.623 | Acc: 78.104% (39052/50000)\n",
            "\n",
            "Epoch: 5\n",
            "Time elapsed: 12.87 min\n",
            "0 98 Loss: 0.626 | Acc: 79.297% (406/512)\n",
            "1 98 Loss: 0.575 | Acc: 80.957% (829/1024)\n",
            "2 98 Loss: 0.563 | Acc: 81.576% (1253/1536)\n",
            "3 98 Loss: 0.565 | Acc: 81.201% (1663/2048)\n",
            "4 98 Loss: 0.560 | Acc: 81.172% (2078/2560)\n",
            "5 98 Loss: 0.552 | Acc: 81.152% (2493/3072)\n",
            "6 98 Loss: 0.549 | Acc: 81.250% (2912/3584)\n",
            "7 98 Loss: 0.549 | Acc: 81.201% (3326/4096)\n",
            "8 98 Loss: 0.550 | Acc: 80.990% (3732/4608)\n",
            "9 98 Loss: 0.553 | Acc: 80.762% (4135/5120)\n",
            "10 98 Loss: 0.558 | Acc: 80.593% (4539/5632)\n",
            "11 98 Loss: 0.560 | Acc: 80.550% (4949/6144)\n",
            "12 98 Loss: 0.557 | Acc: 80.589% (5364/6656)\n",
            "13 98 Loss: 0.552 | Acc: 80.706% (5785/7168)\n",
            "14 98 Loss: 0.552 | Acc: 80.716% (6199/7680)\n",
            "15 98 Loss: 0.553 | Acc: 80.652% (6607/8192)\n",
            "16 98 Loss: 0.553 | Acc: 80.722% (7026/8704)\n",
            "17 98 Loss: 0.548 | Acc: 80.914% (7457/9216)\n",
            "18 98 Loss: 0.547 | Acc: 80.983% (7878/9728)\n",
            "19 98 Loss: 0.546 | Acc: 81.025% (8297/10240)\n",
            "20 98 Loss: 0.546 | Acc: 81.101% (8720/10752)\n",
            "21 98 Loss: 0.543 | Acc: 81.064% (9131/11264)\n",
            "22 98 Loss: 0.544 | Acc: 81.131% (9554/11776)\n",
            "23 98 Loss: 0.542 | Acc: 81.209% (9979/12288)\n",
            "24 98 Loss: 0.541 | Acc: 81.219% (10396/12800)\n",
            "25 98 Loss: 0.543 | Acc: 81.122% (10799/13312)\n",
            "26 98 Loss: 0.542 | Acc: 81.127% (11215/13824)\n",
            "27 98 Loss: 0.542 | Acc: 81.104% (11627/14336)\n",
            "28 98 Loss: 0.540 | Acc: 81.210% (12058/14848)\n",
            "29 98 Loss: 0.539 | Acc: 81.198% (12472/15360)\n",
            "30 98 Loss: 0.539 | Acc: 81.155% (12881/15872)\n",
            "31 98 Loss: 0.540 | Acc: 81.097% (13287/16384)\n",
            "32 98 Loss: 0.539 | Acc: 81.084% (13700/16896)\n",
            "33 98 Loss: 0.539 | Acc: 81.089% (14116/17408)\n",
            "34 98 Loss: 0.542 | Acc: 80.971% (14510/17920)\n",
            "35 98 Loss: 0.542 | Acc: 80.995% (14929/18432)\n",
            "36 98 Loss: 0.541 | Acc: 80.997% (15344/18944)\n",
            "37 98 Loss: 0.542 | Acc: 81.003% (15760/19456)\n",
            "38 98 Loss: 0.542 | Acc: 81.030% (16180/19968)\n",
            "39 98 Loss: 0.542 | Acc: 81.011% (16591/20480)\n",
            "40 98 Loss: 0.544 | Acc: 80.988% (17001/20992)\n",
            "41 98 Loss: 0.545 | Acc: 80.934% (17404/21504)\n",
            "42 98 Loss: 0.543 | Acc: 80.996% (17832/22016)\n",
            "43 98 Loss: 0.542 | Acc: 81.050% (18259/22528)\n",
            "44 98 Loss: 0.542 | Acc: 81.089% (18683/23040)\n",
            "45 98 Loss: 0.542 | Acc: 81.004% (19078/23552)\n",
            "46 98 Loss: 0.542 | Acc: 81.042% (19502/24064)\n",
            "47 98 Loss: 0.542 | Acc: 81.042% (19917/24576)\n",
            "48 98 Loss: 0.541 | Acc: 81.091% (20344/25088)\n",
            "49 98 Loss: 0.542 | Acc: 81.055% (20750/25600)\n",
            "50 98 Loss: 0.543 | Acc: 81.036% (21160/26112)\n",
            "51 98 Loss: 0.543 | Acc: 81.028% (21573/26624)\n",
            "52 98 Loss: 0.543 | Acc: 80.996% (21979/27136)\n",
            "53 98 Loss: 0.543 | Acc: 80.997% (22394/27648)\n",
            "54 98 Loss: 0.542 | Acc: 81.069% (22829/28160)\n",
            "55 98 Loss: 0.542 | Acc: 81.058% (23241/28672)\n",
            "56 98 Loss: 0.542 | Acc: 81.065% (23658/29184)\n",
            "57 98 Loss: 0.542 | Acc: 81.048% (24068/29696)\n",
            "58 98 Loss: 0.543 | Acc: 81.012% (24472/30208)\n",
            "59 98 Loss: 0.543 | Acc: 81.012% (24887/30720)\n",
            "60 98 Loss: 0.543 | Acc: 81.048% (25313/31232)\n",
            "61 98 Loss: 0.542 | Acc: 81.089% (25741/31744)\n",
            "62 98 Loss: 0.541 | Acc: 81.114% (26164/32256)\n",
            "63 98 Loss: 0.541 | Acc: 81.088% (26571/32768)\n",
            "64 98 Loss: 0.541 | Acc: 81.088% (26986/33280)\n",
            "65 98 Loss: 0.541 | Acc: 81.099% (27405/33792)\n",
            "66 98 Loss: 0.542 | Acc: 81.063% (27808/34304)\n",
            "67 98 Loss: 0.541 | Acc: 81.078% (28228/34816)\n",
            "68 98 Loss: 0.541 | Acc: 81.080% (28644/35328)\n",
            "69 98 Loss: 0.540 | Acc: 81.063% (29053/35840)\n",
            "70 98 Loss: 0.541 | Acc: 81.060% (29467/36352)\n",
            "71 98 Loss: 0.540 | Acc: 81.071% (29886/36864)\n",
            "72 98 Loss: 0.539 | Acc: 81.065% (30299/37376)\n",
            "73 98 Loss: 0.540 | Acc: 81.057% (30711/37888)\n",
            "74 98 Loss: 0.540 | Acc: 81.036% (31118/38400)\n",
            "75 98 Loss: 0.540 | Acc: 81.052% (31539/38912)\n",
            "76 98 Loss: 0.540 | Acc: 81.027% (31944/39424)\n",
            "77 98 Loss: 0.540 | Acc: 81.037% (32363/39936)\n",
            "78 98 Loss: 0.540 | Acc: 81.042% (32780/40448)\n",
            "79 98 Loss: 0.540 | Acc: 81.052% (33199/40960)\n",
            "80 98 Loss: 0.540 | Acc: 81.038% (33608/41472)\n",
            "81 98 Loss: 0.539 | Acc: 81.079% (34040/41984)\n",
            "82 98 Loss: 0.539 | Acc: 81.088% (34459/42496)\n",
            "83 98 Loss: 0.539 | Acc: 81.069% (34866/43008)\n",
            "84 98 Loss: 0.540 | Acc: 81.036% (35267/43520)\n",
            "85 98 Loss: 0.539 | Acc: 81.062% (35693/44032)\n",
            "86 98 Loss: 0.539 | Acc: 81.050% (36103/44544)\n",
            "87 98 Loss: 0.540 | Acc: 81.050% (36518/45056)\n",
            "88 98 Loss: 0.540 | Acc: 81.024% (36921/45568)\n",
            "89 98 Loss: 0.541 | Acc: 81.007% (37328/46080)\n",
            "90 98 Loss: 0.541 | Acc: 81.016% (37747/46592)\n",
            "91 98 Loss: 0.540 | Acc: 81.055% (38180/47104)\n",
            "92 98 Loss: 0.539 | Acc: 81.109% (38621/47616)\n",
            "93 98 Loss: 0.538 | Acc: 81.115% (39039/48128)\n",
            "94 98 Loss: 0.538 | Acc: 81.123% (39458/48640)\n",
            "95 98 Loss: 0.538 | Acc: 81.171% (39897/49152)\n",
            "96 98 Loss: 0.538 | Acc: 81.178% (40316/49664)\n",
            "97 98 Loss: 0.539 | Acc: 81.150% (40575/50000)\n",
            "\n",
            "Epoch: 6\n",
            "Time elapsed: 15.45 min\n",
            "0 98 Loss: 0.522 | Acc: 83.789% (429/512)\n",
            "1 98 Loss: 0.503 | Acc: 83.398% (854/1024)\n",
            "2 98 Loss: 0.506 | Acc: 83.268% (1279/1536)\n",
            "3 98 Loss: 0.536 | Acc: 82.324% (1686/2048)\n",
            "4 98 Loss: 0.524 | Acc: 82.656% (2116/2560)\n",
            "5 98 Loss: 0.518 | Acc: 82.747% (2542/3072)\n",
            "6 98 Loss: 0.513 | Acc: 82.896% (2971/3584)\n",
            "7 98 Loss: 0.518 | Acc: 82.690% (3387/4096)\n",
            "8 98 Loss: 0.517 | Acc: 82.552% (3804/4608)\n",
            "9 98 Loss: 0.514 | Acc: 82.441% (4221/5120)\n",
            "10 98 Loss: 0.509 | Acc: 82.475% (4645/5632)\n",
            "11 98 Loss: 0.505 | Acc: 82.454% (5066/6144)\n",
            "12 98 Loss: 0.506 | Acc: 82.347% (5481/6656)\n",
            "13 98 Loss: 0.505 | Acc: 82.310% (5900/7168)\n",
            "14 98 Loss: 0.509 | Acc: 82.188% (6312/7680)\n",
            "15 98 Loss: 0.508 | Acc: 82.153% (6730/8192)\n",
            "16 98 Loss: 0.510 | Acc: 82.261% (7160/8704)\n",
            "17 98 Loss: 0.512 | Acc: 82.107% (7567/9216)\n",
            "18 98 Loss: 0.512 | Acc: 82.103% (7987/9728)\n",
            "19 98 Loss: 0.512 | Acc: 82.129% (8410/10240)\n",
            "20 98 Loss: 0.507 | Acc: 82.301% (8849/10752)\n",
            "21 98 Loss: 0.506 | Acc: 82.227% (9262/11264)\n",
            "22 98 Loss: 0.506 | Acc: 82.235% (9684/11776)\n",
            "23 98 Loss: 0.504 | Acc: 82.349% (10119/12288)\n",
            "24 98 Loss: 0.503 | Acc: 82.359% (10542/12800)\n",
            "25 98 Loss: 0.504 | Acc: 82.294% (10955/13312)\n",
            "26 98 Loss: 0.505 | Acc: 82.277% (11374/13824)\n",
            "27 98 Loss: 0.506 | Acc: 82.206% (11785/14336)\n",
            "28 98 Loss: 0.504 | Acc: 82.233% (12210/14848)\n",
            "29 98 Loss: 0.507 | Acc: 82.181% (12623/15360)\n",
            "30 98 Loss: 0.507 | Acc: 82.164% (13041/15872)\n",
            "31 98 Loss: 0.506 | Acc: 82.172% (13463/16384)\n",
            "32 98 Loss: 0.506 | Acc: 82.167% (13883/16896)\n",
            "33 98 Loss: 0.504 | Acc: 82.273% (14322/17408)\n",
            "34 98 Loss: 0.504 | Acc: 82.277% (14744/17920)\n",
            "35 98 Loss: 0.505 | Acc: 82.237% (15158/18432)\n",
            "36 98 Loss: 0.504 | Acc: 82.300% (15591/18944)\n",
            "37 98 Loss: 0.504 | Acc: 82.293% (16011/19456)\n",
            "38 98 Loss: 0.505 | Acc: 82.267% (16427/19968)\n",
            "39 98 Loss: 0.505 | Acc: 82.324% (16860/20480)\n",
            "40 98 Loss: 0.506 | Acc: 82.274% (17271/20992)\n",
            "41 98 Loss: 0.504 | Acc: 82.320% (17702/21504)\n",
            "42 98 Loss: 0.503 | Acc: 82.363% (18133/22016)\n",
            "43 98 Loss: 0.502 | Acc: 82.413% (18566/22528)\n",
            "44 98 Loss: 0.502 | Acc: 82.435% (18993/23040)\n",
            "45 98 Loss: 0.500 | Acc: 82.515% (19434/23552)\n",
            "46 98 Loss: 0.499 | Acc: 82.497% (19852/24064)\n",
            "47 98 Loss: 0.500 | Acc: 82.483% (20271/24576)\n",
            "48 98 Loss: 0.499 | Acc: 82.486% (20694/25088)\n",
            "49 98 Loss: 0.498 | Acc: 82.555% (21134/25600)\n",
            "50 98 Loss: 0.497 | Acc: 82.613% (21572/26112)\n",
            "51 98 Loss: 0.495 | Acc: 82.685% (22014/26624)\n",
            "52 98 Loss: 0.495 | Acc: 82.687% (22438/27136)\n",
            "53 98 Loss: 0.495 | Acc: 82.690% (22862/27648)\n",
            "54 98 Loss: 0.495 | Acc: 82.692% (23286/28160)\n",
            "55 98 Loss: 0.496 | Acc: 82.649% (23697/28672)\n",
            "56 98 Loss: 0.496 | Acc: 82.645% (24119/29184)\n",
            "57 98 Loss: 0.496 | Acc: 82.658% (24546/29696)\n",
            "58 98 Loss: 0.495 | Acc: 82.723% (24989/30208)\n",
            "59 98 Loss: 0.494 | Acc: 82.738% (25417/30720)\n",
            "60 98 Loss: 0.494 | Acc: 82.774% (25852/31232)\n",
            "61 98 Loss: 0.494 | Acc: 82.759% (26271/31744)\n",
            "62 98 Loss: 0.494 | Acc: 82.769% (26698/32256)\n",
            "63 98 Loss: 0.494 | Acc: 82.733% (27110/32768)\n",
            "64 98 Loss: 0.493 | Acc: 82.764% (27544/33280)\n",
            "65 98 Loss: 0.495 | Acc: 82.733% (27957/33792)\n",
            "66 98 Loss: 0.494 | Acc: 82.740% (28383/34304)\n",
            "67 98 Loss: 0.494 | Acc: 82.726% (28802/34816)\n",
            "68 98 Loss: 0.495 | Acc: 82.696% (29215/35328)\n",
            "69 98 Loss: 0.494 | Acc: 82.734% (29652/35840)\n",
            "70 98 Loss: 0.494 | Acc: 82.746% (30080/36352)\n",
            "71 98 Loss: 0.494 | Acc: 82.772% (30513/36864)\n",
            "72 98 Loss: 0.494 | Acc: 82.767% (30935/37376)\n",
            "73 98 Loss: 0.494 | Acc: 82.781% (31364/37888)\n",
            "74 98 Loss: 0.493 | Acc: 82.786% (31790/38400)\n",
            "75 98 Loss: 0.492 | Acc: 82.843% (32236/38912)\n",
            "76 98 Loss: 0.492 | Acc: 82.825% (32653/39424)\n",
            "77 98 Loss: 0.492 | Acc: 82.828% (33078/39936)\n",
            "78 98 Loss: 0.493 | Acc: 82.793% (33488/40448)\n",
            "79 98 Loss: 0.492 | Acc: 82.803% (33916/40960)\n",
            "80 98 Loss: 0.492 | Acc: 82.808% (34342/41472)\n",
            "81 98 Loss: 0.491 | Acc: 82.848% (34783/41984)\n",
            "82 98 Loss: 0.492 | Acc: 82.822% (35196/42496)\n",
            "83 98 Loss: 0.492 | Acc: 82.838% (35627/43008)\n",
            "84 98 Loss: 0.492 | Acc: 82.852% (36057/43520)\n",
            "85 98 Loss: 0.491 | Acc: 82.903% (36504/44032)\n",
            "86 98 Loss: 0.491 | Acc: 82.914% (36933/44544)\n",
            "87 98 Loss: 0.491 | Acc: 82.912% (37357/45056)\n",
            "88 98 Loss: 0.491 | Acc: 82.911% (37781/45568)\n",
            "89 98 Loss: 0.492 | Acc: 82.893% (38197/46080)\n",
            "90 98 Loss: 0.491 | Acc: 82.913% (38631/46592)\n",
            "91 98 Loss: 0.491 | Acc: 82.893% (39046/47104)\n",
            "92 98 Loss: 0.490 | Acc: 82.932% (39489/47616)\n",
            "93 98 Loss: 0.490 | Acc: 82.945% (39920/48128)\n",
            "94 98 Loss: 0.490 | Acc: 82.942% (40343/48640)\n",
            "95 98 Loss: 0.490 | Acc: 82.969% (40781/49152)\n",
            "96 98 Loss: 0.490 | Acc: 82.984% (41213/49664)\n",
            "97 98 Loss: 0.490 | Acc: 82.978% (41489/50000)\n",
            "\n",
            "Epoch: 7\n",
            "Time elapsed: 18.03 min\n",
            "0 98 Loss: 0.469 | Acc: 84.375% (432/512)\n",
            "1 98 Loss: 0.478 | Acc: 83.105% (851/1024)\n",
            "2 98 Loss: 0.462 | Acc: 83.464% (1282/1536)\n",
            "3 98 Loss: 0.465 | Acc: 83.789% (1716/2048)\n",
            "4 98 Loss: 0.471 | Acc: 83.789% (2145/2560)\n",
            "5 98 Loss: 0.475 | Acc: 83.854% (2576/3072)\n",
            "6 98 Loss: 0.466 | Acc: 84.068% (3013/3584)\n",
            "7 98 Loss: 0.460 | Acc: 84.302% (3453/4096)\n",
            "8 98 Loss: 0.450 | Acc: 84.614% (3899/4608)\n",
            "9 98 Loss: 0.451 | Acc: 84.434% (4323/5120)\n",
            "10 98 Loss: 0.447 | Acc: 84.375% (4752/5632)\n",
            "11 98 Loss: 0.449 | Acc: 84.359% (5183/6144)\n",
            "12 98 Loss: 0.449 | Acc: 84.315% (5612/6656)\n",
            "13 98 Loss: 0.447 | Acc: 84.417% (6051/7168)\n",
            "14 98 Loss: 0.446 | Acc: 84.492% (6489/7680)\n",
            "15 98 Loss: 0.445 | Acc: 84.534% (6925/8192)\n",
            "16 98 Loss: 0.445 | Acc: 84.467% (7352/8704)\n",
            "17 98 Loss: 0.445 | Acc: 84.494% (7787/9216)\n",
            "18 98 Loss: 0.448 | Acc: 84.313% (8202/9728)\n",
            "19 98 Loss: 0.448 | Acc: 84.326% (8635/10240)\n",
            "20 98 Loss: 0.449 | Acc: 84.273% (9061/10752)\n",
            "21 98 Loss: 0.450 | Acc: 84.304% (9496/11264)\n",
            "22 98 Loss: 0.447 | Acc: 84.409% (9940/11776)\n",
            "23 98 Loss: 0.445 | Acc: 84.432% (10375/12288)\n",
            "24 98 Loss: 0.444 | Acc: 84.438% (10808/12800)\n",
            "25 98 Loss: 0.443 | Acc: 84.480% (11246/13312)\n",
            "26 98 Loss: 0.445 | Acc: 84.361% (11662/13824)\n",
            "27 98 Loss: 0.444 | Acc: 84.368% (12095/14336)\n",
            "28 98 Loss: 0.447 | Acc: 84.294% (12516/14848)\n",
            "29 98 Loss: 0.447 | Acc: 84.232% (12938/15360)\n",
            "30 98 Loss: 0.446 | Acc: 84.255% (13373/15872)\n",
            "31 98 Loss: 0.444 | Acc: 84.375% (13824/16384)\n",
            "32 98 Loss: 0.445 | Acc: 84.280% (14240/16896)\n",
            "33 98 Loss: 0.443 | Acc: 84.312% (14677/17408)\n",
            "34 98 Loss: 0.443 | Acc: 84.286% (15104/17920)\n",
            "35 98 Loss: 0.444 | Acc: 84.261% (15531/18432)\n",
            "36 98 Loss: 0.442 | Acc: 84.306% (15971/18944)\n",
            "37 98 Loss: 0.442 | Acc: 84.293% (16400/19456)\n",
            "38 98 Loss: 0.442 | Acc: 84.290% (16831/19968)\n",
            "39 98 Loss: 0.442 | Acc: 84.287% (17262/20480)\n",
            "40 98 Loss: 0.441 | Acc: 84.313% (17699/20992)\n",
            "41 98 Loss: 0.439 | Acc: 84.398% (18149/21504)\n",
            "42 98 Loss: 0.440 | Acc: 84.425% (18587/22016)\n",
            "43 98 Loss: 0.441 | Acc: 84.388% (19011/22528)\n",
            "44 98 Loss: 0.441 | Acc: 84.392% (19444/23040)\n",
            "45 98 Loss: 0.441 | Acc: 84.409% (19880/23552)\n",
            "46 98 Loss: 0.442 | Acc: 84.383% (20306/24064)\n",
            "47 98 Loss: 0.441 | Acc: 84.395% (20741/24576)\n",
            "48 98 Loss: 0.442 | Acc: 84.435% (21183/25088)\n",
            "49 98 Loss: 0.442 | Acc: 84.406% (21608/25600)\n",
            "50 98 Loss: 0.442 | Acc: 84.417% (22043/26112)\n",
            "51 98 Loss: 0.441 | Acc: 84.454% (22485/26624)\n",
            "52 98 Loss: 0.442 | Acc: 84.456% (22918/27136)\n",
            "53 98 Loss: 0.442 | Acc: 84.469% (23354/27648)\n",
            "54 98 Loss: 0.442 | Acc: 84.482% (23790/28160)\n",
            "55 98 Loss: 0.441 | Acc: 84.466% (24218/28672)\n",
            "56 98 Loss: 0.441 | Acc: 84.450% (24646/29184)\n",
            "57 98 Loss: 0.441 | Acc: 84.500% (25093/29696)\n",
            "58 98 Loss: 0.442 | Acc: 84.481% (25520/30208)\n",
            "59 98 Loss: 0.442 | Acc: 84.495% (25957/30720)\n",
            "60 98 Loss: 0.442 | Acc: 84.497% (26390/31232)\n",
            "61 98 Loss: 0.441 | Acc: 84.517% (26829/31744)\n",
            "62 98 Loss: 0.442 | Acc: 84.477% (27249/32256)\n",
            "63 98 Loss: 0.442 | Acc: 84.479% (27682/32768)\n",
            "64 98 Loss: 0.442 | Acc: 84.468% (28111/33280)\n",
            "65 98 Loss: 0.442 | Acc: 84.487% (28550/33792)\n",
            "66 98 Loss: 0.443 | Acc: 84.474% (28978/34304)\n",
            "67 98 Loss: 0.443 | Acc: 84.464% (29407/34816)\n",
            "68 98 Loss: 0.444 | Acc: 84.417% (29823/35328)\n",
            "69 98 Loss: 0.444 | Acc: 84.422% (30257/35840)\n",
            "70 98 Loss: 0.445 | Acc: 84.425% (30690/36352)\n",
            "71 98 Loss: 0.445 | Acc: 84.432% (31125/36864)\n",
            "72 98 Loss: 0.445 | Acc: 84.423% (31554/37376)\n",
            "73 98 Loss: 0.444 | Acc: 84.441% (31993/37888)\n",
            "74 98 Loss: 0.444 | Acc: 84.419% (32417/38400)\n",
            "75 98 Loss: 0.445 | Acc: 84.390% (32838/38912)\n",
            "76 98 Loss: 0.445 | Acc: 84.388% (33269/39424)\n",
            "77 98 Loss: 0.445 | Acc: 84.398% (33705/39936)\n",
            "78 98 Loss: 0.445 | Acc: 84.424% (34148/40448)\n",
            "79 98 Loss: 0.445 | Acc: 84.407% (34573/40960)\n",
            "80 98 Loss: 0.445 | Acc: 84.404% (35004/41472)\n",
            "81 98 Loss: 0.445 | Acc: 84.396% (35433/41984)\n",
            "82 98 Loss: 0.445 | Acc: 84.396% (35865/42496)\n",
            "83 98 Loss: 0.445 | Acc: 84.380% (36290/43008)\n",
            "84 98 Loss: 0.445 | Acc: 84.380% (36722/43520)\n",
            "85 98 Loss: 0.446 | Acc: 84.380% (37154/44032)\n",
            "86 98 Loss: 0.445 | Acc: 84.391% (37591/44544)\n",
            "87 98 Loss: 0.446 | Acc: 84.377% (38017/45056)\n",
            "88 98 Loss: 0.446 | Acc: 84.388% (38454/45568)\n",
            "89 98 Loss: 0.445 | Acc: 84.408% (38895/46080)\n",
            "90 98 Loss: 0.445 | Acc: 84.403% (39325/46592)\n",
            "91 98 Loss: 0.445 | Acc: 84.411% (39761/47104)\n",
            "92 98 Loss: 0.444 | Acc: 84.449% (40211/47616)\n",
            "93 98 Loss: 0.444 | Acc: 84.454% (40646/48128)\n",
            "94 98 Loss: 0.445 | Acc: 84.439% (41071/48640)\n",
            "95 98 Loss: 0.444 | Acc: 84.446% (41507/49152)\n",
            "96 98 Loss: 0.444 | Acc: 84.468% (41950/49664)\n",
            "97 98 Loss: 0.444 | Acc: 84.474% (42237/50000)\n",
            "\n",
            "Epoch: 8\n",
            "Time elapsed: 20.60 min\n",
            "0 98 Loss: 0.298 | Acc: 88.867% (455/512)\n",
            "1 98 Loss: 0.350 | Acc: 88.086% (902/1024)\n",
            "2 98 Loss: 0.370 | Acc: 87.500% (1344/1536)\n",
            "3 98 Loss: 0.378 | Acc: 87.402% (1790/2048)\n",
            "4 98 Loss: 0.380 | Acc: 87.070% (2229/2560)\n",
            "5 98 Loss: 0.386 | Acc: 86.816% (2667/3072)\n",
            "6 98 Loss: 0.393 | Acc: 86.802% (3111/3584)\n",
            "7 98 Loss: 0.399 | Acc: 86.426% (3540/4096)\n",
            "8 98 Loss: 0.404 | Acc: 86.220% (3973/4608)\n",
            "9 98 Loss: 0.404 | Acc: 86.211% (4414/5120)\n",
            "10 98 Loss: 0.404 | Acc: 86.346% (4863/5632)\n",
            "11 98 Loss: 0.405 | Acc: 86.100% (5290/6144)\n",
            "12 98 Loss: 0.402 | Acc: 86.253% (5741/6656)\n",
            "13 98 Loss: 0.398 | Acc: 86.370% (6191/7168)\n",
            "14 98 Loss: 0.399 | Acc: 86.341% (6631/7680)\n",
            "15 98 Loss: 0.397 | Acc: 86.462% (7083/8192)\n",
            "16 98 Loss: 0.398 | Acc: 86.420% (7522/8704)\n",
            "17 98 Loss: 0.398 | Acc: 86.426% (7965/9216)\n",
            "18 98 Loss: 0.399 | Acc: 86.410% (8406/9728)\n",
            "19 98 Loss: 0.400 | Acc: 86.328% (8840/10240)\n",
            "20 98 Loss: 0.402 | Acc: 86.226% (9271/10752)\n",
            "21 98 Loss: 0.402 | Acc: 86.266% (9717/11264)\n",
            "22 98 Loss: 0.401 | Acc: 86.337% (10167/11776)\n",
            "23 98 Loss: 0.401 | Acc: 86.312% (10606/12288)\n",
            "24 98 Loss: 0.403 | Acc: 86.250% (11040/12800)\n",
            "25 98 Loss: 0.406 | Acc: 86.200% (11475/13312)\n",
            "26 98 Loss: 0.405 | Acc: 86.249% (11923/13824)\n",
            "27 98 Loss: 0.407 | Acc: 86.203% (12358/14336)\n",
            "28 98 Loss: 0.407 | Acc: 86.180% (12796/14848)\n",
            "29 98 Loss: 0.406 | Acc: 86.204% (13241/15360)\n",
            "30 98 Loss: 0.407 | Acc: 86.183% (13679/15872)\n",
            "31 98 Loss: 0.406 | Acc: 86.218% (14126/16384)\n",
            "32 98 Loss: 0.408 | Acc: 86.097% (14547/16896)\n",
            "33 98 Loss: 0.407 | Acc: 86.150% (14997/17408)\n",
            "34 98 Loss: 0.406 | Acc: 86.144% (15437/17920)\n",
            "35 98 Loss: 0.407 | Acc: 86.155% (15880/18432)\n",
            "36 98 Loss: 0.406 | Acc: 86.170% (16324/18944)\n",
            "37 98 Loss: 0.406 | Acc: 86.143% (16760/19456)\n",
            "38 98 Loss: 0.405 | Acc: 86.158% (17204/19968)\n",
            "39 98 Loss: 0.406 | Acc: 86.147% (17643/20480)\n",
            "40 98 Loss: 0.406 | Acc: 86.133% (18081/20992)\n",
            "41 98 Loss: 0.407 | Acc: 86.086% (18512/21504)\n",
            "42 98 Loss: 0.405 | Acc: 86.133% (18963/22016)\n",
            "43 98 Loss: 0.406 | Acc: 86.115% (19400/22528)\n",
            "44 98 Loss: 0.406 | Acc: 86.146% (19848/23040)\n",
            "45 98 Loss: 0.406 | Acc: 86.137% (20287/23552)\n",
            "46 98 Loss: 0.406 | Acc: 86.137% (20728/24064)\n",
            "47 98 Loss: 0.405 | Acc: 86.169% (21177/24576)\n",
            "48 98 Loss: 0.405 | Acc: 86.153% (21614/25088)\n",
            "49 98 Loss: 0.404 | Acc: 86.168% (22059/25600)\n",
            "50 98 Loss: 0.405 | Acc: 86.133% (22491/26112)\n",
            "51 98 Loss: 0.406 | Acc: 86.091% (22921/26624)\n",
            "52 98 Loss: 0.406 | Acc: 86.085% (23360/27136)\n",
            "53 98 Loss: 0.405 | Acc: 86.115% (23809/27648)\n",
            "54 98 Loss: 0.405 | Acc: 86.083% (24241/28160)\n",
            "55 98 Loss: 0.405 | Acc: 86.070% (24678/28672)\n",
            "56 98 Loss: 0.405 | Acc: 86.075% (25120/29184)\n",
            "57 98 Loss: 0.404 | Acc: 86.096% (25567/29696)\n",
            "58 98 Loss: 0.405 | Acc: 86.063% (25998/30208)\n",
            "59 98 Loss: 0.406 | Acc: 86.051% (26435/30720)\n",
            "60 98 Loss: 0.406 | Acc: 86.050% (26875/31232)\n",
            "61 98 Loss: 0.407 | Acc: 86.029% (27309/31744)\n",
            "62 98 Loss: 0.408 | Acc: 85.987% (27736/32256)\n",
            "63 98 Loss: 0.407 | Acc: 86.032% (28191/32768)\n",
            "64 98 Loss: 0.407 | Acc: 86.055% (28639/33280)\n",
            "65 98 Loss: 0.406 | Acc: 86.059% (29081/33792)\n",
            "66 98 Loss: 0.406 | Acc: 86.072% (29526/34304)\n",
            "67 98 Loss: 0.406 | Acc: 86.070% (29966/34816)\n",
            "68 98 Loss: 0.406 | Acc: 86.073% (30408/35328)\n",
            "69 98 Loss: 0.406 | Acc: 86.094% (30856/35840)\n",
            "70 98 Loss: 0.405 | Acc: 86.092% (31296/36352)\n",
            "71 98 Loss: 0.406 | Acc: 86.046% (31720/36864)\n",
            "72 98 Loss: 0.405 | Acc: 86.082% (32174/37376)\n",
            "73 98 Loss: 0.405 | Acc: 86.101% (32622/37888)\n",
            "74 98 Loss: 0.406 | Acc: 86.089% (33058/38400)\n",
            "75 98 Loss: 0.407 | Acc: 86.071% (33492/38912)\n",
            "76 98 Loss: 0.407 | Acc: 86.034% (33918/39424)\n",
            "77 98 Loss: 0.407 | Acc: 86.035% (34359/39936)\n",
            "78 98 Loss: 0.406 | Acc: 86.054% (34807/40448)\n",
            "79 98 Loss: 0.406 | Acc: 86.055% (35248/40960)\n",
            "80 98 Loss: 0.407 | Acc: 86.046% (35685/41472)\n",
            "81 98 Loss: 0.407 | Acc: 86.038% (36122/41984)\n",
            "82 98 Loss: 0.407 | Acc: 86.013% (36552/42496)\n",
            "83 98 Loss: 0.407 | Acc: 86.017% (36994/43008)\n",
            "84 98 Loss: 0.407 | Acc: 86.029% (37440/43520)\n",
            "85 98 Loss: 0.407 | Acc: 86.035% (37883/44032)\n",
            "86 98 Loss: 0.407 | Acc: 86.030% (38321/44544)\n",
            "87 98 Loss: 0.408 | Acc: 86.002% (38749/45056)\n",
            "88 98 Loss: 0.409 | Acc: 85.957% (39169/45568)\n",
            "89 98 Loss: 0.409 | Acc: 85.955% (39608/46080)\n",
            "90 98 Loss: 0.409 | Acc: 85.963% (40052/46592)\n",
            "91 98 Loss: 0.409 | Acc: 85.997% (40508/47104)\n",
            "92 98 Loss: 0.409 | Acc: 85.984% (40942/47616)\n",
            "93 98 Loss: 0.409 | Acc: 85.962% (41372/48128)\n",
            "94 98 Loss: 0.408 | Acc: 85.964% (41813/48640)\n",
            "95 98 Loss: 0.408 | Acc: 85.960% (42251/49152)\n",
            "96 98 Loss: 0.408 | Acc: 85.962% (42692/49664)\n",
            "97 98 Loss: 0.408 | Acc: 85.968% (42984/50000)\n",
            "\n",
            "Epoch: 9\n",
            "Time elapsed: 23.17 min\n",
            "0 98 Loss: 0.413 | Acc: 85.938% (440/512)\n",
            "1 98 Loss: 0.429 | Acc: 84.375% (864/1024)\n",
            "2 98 Loss: 0.385 | Acc: 86.263% (1325/1536)\n",
            "3 98 Loss: 0.376 | Acc: 86.377% (1769/2048)\n",
            "4 98 Loss: 0.380 | Acc: 86.172% (2206/2560)\n",
            "5 98 Loss: 0.389 | Acc: 86.035% (2643/3072)\n",
            "6 98 Loss: 0.394 | Acc: 86.189% (3089/3584)\n",
            "7 98 Loss: 0.395 | Acc: 86.108% (3527/4096)\n",
            "8 98 Loss: 0.397 | Acc: 86.220% (3973/4608)\n",
            "9 98 Loss: 0.393 | Acc: 86.309% (4419/5120)\n",
            "10 98 Loss: 0.395 | Acc: 86.222% (4856/5632)\n",
            "11 98 Loss: 0.394 | Acc: 86.247% (5299/6144)\n",
            "12 98 Loss: 0.391 | Acc: 86.268% (5742/6656)\n",
            "13 98 Loss: 0.383 | Acc: 86.426% (6195/7168)\n",
            "14 98 Loss: 0.384 | Acc: 86.354% (6632/7680)\n",
            "15 98 Loss: 0.383 | Acc: 86.365% (7075/8192)\n",
            "16 98 Loss: 0.381 | Acc: 86.523% (7531/8704)\n",
            "17 98 Loss: 0.384 | Acc: 86.480% (7970/9216)\n",
            "18 98 Loss: 0.381 | Acc: 86.544% (8419/9728)\n",
            "19 98 Loss: 0.382 | Acc: 86.553% (8863/10240)\n",
            "20 98 Loss: 0.379 | Acc: 86.719% (9324/10752)\n",
            "21 98 Loss: 0.379 | Acc: 86.603% (9755/11264)\n",
            "22 98 Loss: 0.378 | Acc: 86.583% (10196/11776)\n",
            "23 98 Loss: 0.379 | Acc: 86.654% (10648/12288)\n",
            "24 98 Loss: 0.380 | Acc: 86.648% (11091/12800)\n",
            "25 98 Loss: 0.379 | Acc: 86.614% (11530/13312)\n",
            "26 98 Loss: 0.379 | Acc: 86.697% (11985/13824)\n",
            "27 98 Loss: 0.378 | Acc: 86.782% (12441/14336)\n",
            "28 98 Loss: 0.378 | Acc: 86.773% (12884/14848)\n",
            "29 98 Loss: 0.378 | Acc: 86.771% (13328/15360)\n",
            "30 98 Loss: 0.377 | Acc: 86.794% (13776/15872)\n",
            "31 98 Loss: 0.378 | Acc: 86.755% (14214/16384)\n",
            "32 98 Loss: 0.376 | Acc: 86.813% (14668/16896)\n",
            "33 98 Loss: 0.375 | Acc: 86.857% (15120/17408)\n",
            "34 98 Loss: 0.375 | Acc: 86.830% (15560/17920)\n",
            "35 98 Loss: 0.376 | Acc: 86.854% (16009/18432)\n",
            "36 98 Loss: 0.376 | Acc: 86.830% (16449/18944)\n",
            "37 98 Loss: 0.376 | Acc: 86.842% (16896/19456)\n",
            "38 98 Loss: 0.376 | Acc: 86.854% (17343/19968)\n",
            "39 98 Loss: 0.377 | Acc: 86.865% (17790/20480)\n",
            "40 98 Loss: 0.378 | Acc: 86.800% (18221/20992)\n",
            "41 98 Loss: 0.377 | Acc: 86.812% (18668/21504)\n",
            "42 98 Loss: 0.377 | Acc: 86.846% (19120/22016)\n",
            "43 98 Loss: 0.377 | Acc: 86.879% (19572/22528)\n",
            "44 98 Loss: 0.377 | Acc: 86.910% (20024/23040)\n",
            "45 98 Loss: 0.378 | Acc: 86.867% (20459/23552)\n",
            "46 98 Loss: 0.378 | Acc: 86.864% (20903/24064)\n",
            "47 98 Loss: 0.378 | Acc: 86.853% (21345/24576)\n",
            "48 98 Loss: 0.378 | Acc: 86.850% (21789/25088)\n",
            "49 98 Loss: 0.377 | Acc: 86.867% (22238/25600)\n",
            "50 98 Loss: 0.377 | Acc: 86.872% (22684/26112)\n",
            "51 98 Loss: 0.376 | Acc: 86.907% (23138/26624)\n",
            "52 98 Loss: 0.376 | Acc: 86.918% (23586/27136)\n",
            "53 98 Loss: 0.375 | Acc: 86.932% (24035/27648)\n",
            "54 98 Loss: 0.375 | Acc: 86.911% (24474/28160)\n",
            "55 98 Loss: 0.375 | Acc: 86.883% (24911/28672)\n",
            "56 98 Loss: 0.375 | Acc: 86.890% (25358/29184)\n",
            "57 98 Loss: 0.374 | Acc: 86.931% (25815/29696)\n",
            "58 98 Loss: 0.375 | Acc: 86.924% (26258/30208)\n",
            "59 98 Loss: 0.375 | Acc: 86.914% (26700/30720)\n",
            "60 98 Loss: 0.374 | Acc: 86.952% (27157/31232)\n",
            "61 98 Loss: 0.375 | Acc: 86.905% (27587/31744)\n",
            "62 98 Loss: 0.375 | Acc: 86.905% (28032/32256)\n",
            "63 98 Loss: 0.376 | Acc: 86.877% (28468/32768)\n",
            "64 98 Loss: 0.377 | Acc: 86.866% (28909/33280)\n",
            "65 98 Loss: 0.377 | Acc: 86.873% (29356/33792)\n",
            "66 98 Loss: 0.377 | Acc: 86.838% (29789/34304)\n",
            "67 98 Loss: 0.378 | Acc: 86.831% (30231/34816)\n",
            "68 98 Loss: 0.378 | Acc: 86.880% (30693/35328)\n",
            "69 98 Loss: 0.378 | Acc: 86.875% (31136/35840)\n",
            "70 98 Loss: 0.377 | Acc: 86.884% (31584/36352)\n",
            "71 98 Loss: 0.378 | Acc: 86.871% (32024/36864)\n",
            "72 98 Loss: 0.377 | Acc: 86.877% (32471/37376)\n",
            "73 98 Loss: 0.378 | Acc: 86.874% (32915/37888)\n",
            "74 98 Loss: 0.379 | Acc: 86.859% (33354/38400)\n",
            "75 98 Loss: 0.379 | Acc: 86.845% (33793/38912)\n",
            "76 98 Loss: 0.378 | Acc: 86.866% (34246/39424)\n",
            "77 98 Loss: 0.378 | Acc: 86.874% (34694/39936)\n",
            "78 98 Loss: 0.378 | Acc: 86.894% (35147/40448)\n",
            "79 98 Loss: 0.378 | Acc: 86.892% (35591/40960)\n",
            "80 98 Loss: 0.378 | Acc: 86.907% (36042/41472)\n",
            "81 98 Loss: 0.378 | Acc: 86.907% (36487/41984)\n",
            "82 98 Loss: 0.378 | Acc: 86.891% (36925/42496)\n",
            "83 98 Loss: 0.379 | Acc: 86.877% (37364/43008)\n",
            "84 98 Loss: 0.379 | Acc: 86.886% (37813/43520)\n",
            "85 98 Loss: 0.379 | Acc: 86.860% (38246/44032)\n",
            "86 98 Loss: 0.381 | Acc: 86.822% (38674/44544)\n",
            "87 98 Loss: 0.380 | Acc: 86.816% (39116/45056)\n",
            "88 98 Loss: 0.380 | Acc: 86.811% (39558/45568)\n",
            "89 98 Loss: 0.380 | Acc: 86.801% (39998/46080)\n",
            "90 98 Loss: 0.381 | Acc: 86.790% (40437/46592)\n",
            "91 98 Loss: 0.381 | Acc: 86.782% (40878/47104)\n",
            "92 98 Loss: 0.380 | Acc: 86.803% (41332/47616)\n",
            "93 98 Loss: 0.380 | Acc: 86.806% (41778/48128)\n",
            "94 98 Loss: 0.380 | Acc: 86.778% (42209/48640)\n",
            "95 98 Loss: 0.380 | Acc: 86.792% (42660/49152)\n",
            "96 98 Loss: 0.380 | Acc: 86.785% (43101/49664)\n",
            "97 98 Loss: 0.381 | Acc: 86.788% (43394/50000)\n"
          ]
        }
      ]
    }
  ]
}